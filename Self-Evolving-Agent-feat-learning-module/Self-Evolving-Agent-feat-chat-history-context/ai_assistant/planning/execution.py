# Code for task execution.
import traceback
from typing import List, Dict, Any, Optional, Tuple
import ai_assistant.tools.tool_system as ts_module_type
import re
import asyncio
from ..core.reflection import global_reflection_log, analyze_last_failure
from ai_assistant.memory.awareness import record_tool_goal_association
from ai_assistant.memory.event_logger import log_event
from ai_assistant.learning.learning import LearningAgent # Import LearningAgent
from ai_assistant.planning.planning import PlannerAgent # Added for re-planning
from ..core.task_manager import TaskManager # Added for TaskManager
from ..core.notification_manager import NotificationManager # Added for NotificationManager

class ExecutionAgent:
    """
    Responsible for executing a plan (a sequence of tool invocations)
    generated by the PlannerAgent.
    """
    MAX_RETRIES_PER_STEP = 1 # Results in 1 initial attempt + 1 retry = 2 total attempts
    MAX_REPLAN_ATTEMPTS = 1  # Max number of times to attempt re-planning

    async def execute_plan(
        self, 
        goal_description: str, 
        initial_plan: List[Dict[str, Any]], 
        tool_system: Any,
        planner_agent: PlannerAgent, # Added for re-planning
        learning_agent: LearningAgent, # Added LearningAgent
        task_manager: Optional[TaskManager] = None,
        notification_manager: Optional[NotificationManager] = None, # New parameter
        ollama_model_name: Optional[str] = None
    ) -> Tuple[List[Dict[str, Any]], List[Any]]: # Returns final_plan, results
        """
        Executes each step in the provided plan with retries and logs the execution.
        Includes logic for re-planning if critical failures occur.
        """
        current_plan = list(initial_plan) # Make a mutable copy
        replan_attempts = 0

        while replan_attempts <= self.MAX_REPLAN_ATTEMPTS:
            if not current_plan:
                print("ExecutionAgent: Plan is empty. Nothing to execute.")
                if replan_attempts == 0: # Only log empty plan if it was the initial plan
                    reflection_entry_obj_empty = global_reflection_log.log_execution(
                        goal_description=goal_description, plan=current_plan, execution_results=[],
                        overall_success=False, notes="Initial plan was empty."
                    )
                    if learning_agent:
                        learning_agent.process_reflection_entry(reflection_entry_obj_empty) # type: ignore
                return [], [] # MODIFIED: Return empty plan and results

            print(f"\nExecutionAgent: Starting execution of {'re-plan' if replan_attempts > 0 else 'plan'} for goal '{goal_description}' (Attempt {replan_attempts + 1}) with {len(current_plan)} steps.")
            
            # Reset per-plan state for each new plan (or re-plan)
            plan_results: List[Any] = []
            plan_step_notes: List[str] = []
            # plan_step_errors_details: List[Dict[str, Any]] = [] # Not directly used for re-planning logic, but for logging
            
            first_critical_error_details: Dict[str, Optional[str]] = {
                "error_type": None, "error_message": None, "traceback_snippet": None
            }
            
            plan_failed_critically = False

            for i, step in enumerate(current_plan):
                tool_name = step.get("tool_name")
                args = step.get("args", ())
                kwargs = step.get("kwargs", {})

                step_result: Any = None
                current_step_error_details: Dict[str, Any] = {}
                step_attempt_note: str = ""

                if not tool_name:
                    err_msg = f"Step {i+1} is missing 'tool_name'. Skipping."
                    print(f"ExecutionAgent: {err_msg}")
                    step_result = RuntimeError(err_msg)
                    current_step_error_details = {'error_type': type(step_result).__name__, 'error_message': err_msg, 'traceback_snippet': None}
                    step_attempt_note = "Skipped due to missing tool name."
                else:
                    if not isinstance(args, tuple): args = tuple(args) if isinstance(args, list) else (args,)
                    if not isinstance(kwargs, dict): kwargs = {}

                    processed_args = list(args)
                    for i_arg, arg_val in enumerate(processed_args):
                        if isinstance(arg_val, str):
                            match = re.fullmatch(r"\[\[step_(\d+)_output\]\]", arg_val)
                            if match:
                                ref_step_num = int(match.group(1))
                                if 1 <= ref_step_num <= len(plan_results):
                                    plan_result_to_sub = plan_results[ref_step_num - 1]
                                    if not isinstance(plan_result_to_sub, str):
                                        print(f"Warning: Step {ref_step_num} result is not a string ('{type(plan_result_to_sub).__name__}'). Using its string representation for placeholder '{arg_val}'.")
                                        processed_args[i_arg] = str(plan_result_to_sub)
                                    else:
                                        processed_args[i_arg] = plan_result_to_sub
                                else:
                                    print(f"ExecutionAgent: Warning - Invalid step reference {arg_val} for step {i+1}. Placeholder passed as is.")
                    final_args_for_tool = tuple(processed_args)

                    processed_kwargs = kwargs.copy()
                    for kw_key, kw_val in processed_kwargs.items():
                        if isinstance(kw_val, str):
                            match = re.fullmatch(r"\[\[step_(\d+)_output\]\]", kw_val)
                            if match:
                                ref_step_num = int(match.group(1))
                                if 1 <= ref_step_num <= len(plan_results):
                                    kw_plan_result_to_sub = plan_results[ref_step_num-1]
                                    if not isinstance(kw_plan_result_to_sub, str):
                                        print(f"Warning: Step {ref_step_num} result for kwarg '{kw_key}' is not str. Using str().")
                                        processed_kwargs[kw_key] = str(kw_plan_result_to_sub)
                                    else:
                                        processed_kwargs[kw_key] = kw_plan_result_to_sub
                                else:
                                    print(f"ExecutionAgent: Warning - Invalid step reference {kw_val} for kwarg '{kw_key}' in step {i+1}. Placeholder passed as is.")
                    final_kwargs_for_tool = processed_kwargs

                    for attempt in range(self.MAX_RETRIES_PER_STEP + 1):
                        try:
                            print(f"ExecutionAgent: Executing step {i+1}/{len(current_plan)} - Tool: {tool_name} (Args: {final_args_for_tool}, Kwargs: {final_kwargs_for_tool}), Attempt: {attempt+1}/{self.MAX_RETRIES_PER_STEP + 1}")
                            step_result = await tool_system.execute_tool(
                                tool_name,
                                args=final_args_for_tool,
                                kwargs=final_kwargs_for_tool,
                                task_manager=task_manager,
                                notification_manager=notification_manager # Pass notification_manager
                            )
                            current_step_error_details = {} 
                            if attempt > 0:
                                step_attempt_note = f"Succeeded on retry (attempt {attempt+1})."
                            print(f"ExecutionAgent: Step {i+1} completed. Result: {str(step_result)[:200] + '...' if len(str(step_result)) > 200 else step_result}")
                            break 
                        except Exception as e:
                            step_result = e 
                            tb_snippet = traceback.format_exc(limit=3)
                            current_step_error_details = {'error_type': type(e).__name__, 'error_message': str(e), 'traceback_snippet': tb_snippet}
                            
                            if attempt < self.MAX_RETRIES_PER_STEP:
                                print(f"ExecutionAgent: Tool '{tool_name}' failed (Attempt {attempt+1}). Error: {str(e)}. Retrying...")
                            else: 
                                step_attempt_note = f"Failed after {self.MAX_RETRIES_PER_STEP + 1} attempt(s). Last error: {str(e)}"
                                print(f"ExecutionAgent: Tool '{tool_name}' also failed on last retry (Attempt {attempt+1}). Error: {str(e)}")

                plan_results.append(step_result)
                plan_step_notes.append(step_attempt_note)
                # plan_step_errors_details.append(current_step_error_details) # For detailed step-by-step logging if needed

                # Check for failure: either an exception or a dictionary indicating failure
                step_failed = False
                if isinstance(step_result, Exception):
                    step_failed = True
                elif isinstance(step_result, dict):
                    # Check for common failure indicators in dictionary results
                    if step_result.get("ran_successfully") is False or step_result.get("error") is not None:
                        step_failed = True
                        # Populate current_step_error_details if it's a dict-reported error and not already set by an exception
                        if not current_step_error_details:
                            current_step_error_details = {'error_type': 'ToolReportedError', 'error_message': step_result.get("error", str(step_result.get("stderr","Unknown tool error"))), 'traceback_snippet': None}

                if step_failed:
                    if first_critical_error_details["error_type"] is None: # Capture first critical error of this plan attempt
                        first_critical_error_details = current_step_error_details
                    
                    # Log this specific plan attempt's failure before trying to re-plan
                    reflection_entry_obj_fail = global_reflection_log.log_execution(
                        goal_description=goal_description,
                        plan=current_plan, # Log the plan that just failed
                        execution_results=plan_results, 
                        overall_success=False, # This specific plan attempt failed
                        notes=f"Plan attempt {replan_attempts + 1} failed at step {i+1} ({tool_name}). {step_attempt_note}",
                        first_error_type=first_critical_error_details["error_type"],
                        first_error_message=first_critical_error_details["error_message"],
                        first_traceback_snippet=first_critical_error_details["traceback_snippet"]
                    )
                    if learning_agent:
                        learning_agent.process_reflection_entry(reflection_entry_obj_fail)
                    plan_failed_critically = True # Mark that this plan attempt had a critical failure

                    if replan_attempts < self.MAX_REPLAN_ATTEMPTS:
                        print(f"ExecutionAgent: Critical failure in plan attempt {replan_attempts + 1}. Attempting to analyze failure and re-plan...")
                        tool_registry = tool_system.list_tools()
                        failure_analysis = analyze_last_failure(tool_registry, ollama_model_name=ollama_model_name)

                        if failure_analysis and failure_analysis.strip():
                            print(f"ExecutionAgent: Failure analysis obtained:\n{failure_analysis}")
                            new_plan = await planner_agent.replan_after_failure(
                                original_goal=goal_description,
                                failure_analysis=failure_analysis,
                                available_tools=tool_registry,
                                ollama_model_name=ollama_model_name
                            )
                            if new_plan:
                                print(f"ExecutionAgent: Successfully re-planned. New plan has {len(new_plan)} steps. Resetting and retrying.")
                                current_plan = new_plan
                                replan_attempts += 1 # Increment before breaking to restart loop
                                break # Break from step loop to restart with new plan in the outer while loop
                            else:
                                print("ExecutionAgent: Re-planning attempt failed to produce a new plan. Proceeding with original failure.")
                                # Fall through to normal failure handling outside the step loop as plan_failed_critically is True
                        else:
                            print("ExecutionAgent: Failure analysis did not yield significant results. Proceeding with original failure.")
                            # Fall through
                    else:
                        print(f"ExecutionAgent: Maximum re-plan attempts ({self.MAX_REPLAN_ATTEMPTS}) reached. Plan execution failed.")
                        # Fall through
                    
                    break # Break from step loop (current plan execution stops due to critical error)

            # After iterating through all steps of the current_plan or breaking due to critical failure
            if not plan_failed_critically: # Plan completed all steps without critical error
                print(f"ExecutionAgent: Plan attempt {replan_attempts + 1} completed successfully.")
                # Log this successful plan attempt
                reflection_entry_obj_success = global_reflection_log.log_execution(
                    goal_description=goal_description,
                    plan=current_plan,
                    execution_results=plan_results,
                    overall_success=True, # This specific plan attempt was successful
                    notes=f"Plan attempt {replan_attempts + 1} succeeded. " + ". ".join(filter(None, plan_step_notes)),
                    first_error_type=None, first_error_message=None, first_traceback_snippet=None
                )
                if learning_agent:
                    learning_agent.process_reflection_entry(reflection_entry_obj_success) # type: ignore
                # Record tool-goal associations only if this final plan was successful
                for step in current_plan:
                    tool_name = step.get("tool_name")
                    if tool_name: record_tool_goal_association(tool_name, goal_description)
                
                # Log GOAL_EXECUTION_COMPLETED for overall success
                tools_used_in_final_plan = list(set(step.get("tool_name") for step in current_plan if step.get("tool_name")))
                log_event(
                    event_type="GOAL_EXECUTION_COMPLETED",
                    description=f"Goal execution completed successfully for: {goal_description} (possibly after re-planning).",
                    source="ExecutionAgent.execute_plan",
                    metadata={
                        "goal_description": goal_description,
                        "final_plan_summary": [{"tool": step.get("tool_name"), "args_preview": str(step.get("args",()))[:50]} for step in current_plan],
                        "overall_success": True,
                        "num_steps_in_final_plan": len(current_plan),
                        "tools_used_in_final_plan": tools_used_in_final_plan,
                        "replan_attempts_made": replan_attempts,
                    }
                )
                return current_plan, plan_results # MODIFIED: Return successful plan and its results

            # If plan_failed_critically is True and we are here, it means either re-planning didn't happen,
            # or re-planning failed to produce a new plan, or re-plan limit was reached.
            if replan_attempts >= self.MAX_REPLAN_ATTEMPTS or not new_plan: # Check if we should stop trying
                if plan_failed_critically : # ensure this is only if the last attempt also failed.
                    print(f"ExecutionAgent: Plan execution failed for goal '{goal_description}' after {replan_attempts} re-plan attempt(s).")
                    # The final failure was already logged by global_reflection_log inside the loop.
                    # Log GOAL_EXECUTION_COMPLETED for overall failure
                    tools_used_in_last_plan = list(set(step.get("tool_name") for step in current_plan if step.get("tool_name"))) # current_plan is the one that failed last
                    log_event(
                        event_type="GOAL_EXECUTION_COMPLETED",
                        description=f"Goal execution failed for: {goal_description}",
                        source="ExecutionAgent.execute_plan",
                        metadata={
                            "goal_description": goal_description,
                            "last_attempted_plan_summary": [{"tool": step.get("tool_name"), "args_preview": str(step.get("args",()))[:50]} for step in current_plan],
                            "overall_success": False,
                            "num_steps_in_last_plan": len(current_plan),
                            "tools_used_in_last_plan": tools_used_in_last_plan,
                            "replan_attempts_made": replan_attempts,
                            "first_error_type_in_last_plan": first_critical_error_details.get("error_type"),
                            "first_error_message_in_last_plan": first_critical_error_details.get("error_message")
                        }
                    )
                    return current_plan, plan_results # MODIFIED: Return last attempted plan and its (failed) results
            # If we are here and plan_failed_critically is true, but replan_attempts < MAX_REPLAN_ATTEMPTS and new_plan was generated,
            # the outer while loop will continue with the new_plan.
        
        # Should ideally be covered by returns inside the loop.
        # This path implies MAX_REPLAN_ATTEMPTS was 0 and the first plan failed, or some other edge case.
        print(f"ExecutionAgent: Exiting execute_plan for goal '{goal_description}' after exhausting plan attempts.")
        return current_plan, plan_results # MODIFIED: Return the last plan and its results, even if loop exhausted


if __name__ == '__main__':
    # Example Usage and Test (assuming global_reflection_log is available for testing its effect)
    # Needs to be updated to include PlannerAgent and ollama_model_name for full testing.

    async def main_test():
        class MockToolSystemForExecutionTest:
            def __init__(self, succeed_after_n_failures=0):
                self.tools_called = []
                self.call_counts = {}
                self.succeed_after_n_failures = succeed_after_n_failures

            def list_tools(self): # Mock for analyze_last_failure
                return {
                    "greet_user": "Greets the user. Args: name (str)",
                    "add_numbers": "Adds two numbers. Args: a (str), b (str)",
                    "faulty_tool": "A tool that always fails initially.",
                    "sometimes_faulty_tool": "A tool that fails N times then succeeds.",
                    "none_tool": "A tool that returns None.",
                    "tool_not_found_in_mock": "This tool does not exist here."
                }

            async def execute_tool(self, name, args=(), kwargs=None):
                if kwargs is None: kwargs = {}
                call_info = {'name': name, 'args': args, 'kwargs': kwargs}
                self.tools_called.append(call_info)
                self.call_counts[name] = self.call_counts.get(name, 0) + 1
                
                print(f"MockToolSystem: Executing {name} (Call #{self.call_counts[name]}) with {args} and {kwargs}")
                
                if name == "greet_user":
                    return f"Hello, {args[0]}!" if args else "Hello, World!"
                elif name == "add_numbers":
                    # Ensure args are actual numbers for mock, not strings like "5"
                    actual_args = []
                    for arg in args:
                        try: actual_args.append(int(arg)) # Simple int conversion for test
                        except ValueError: return f"Error: add_numbers requires parsable integer arguments. Got '{arg}'."
                    
                    if not all(isinstance(arg, int) for arg in actual_args): # Redundant due to above, but good check
                        return "Error: add_numbers requires integer arguments."
                    return actual_args[0] + actual_args[1]
                elif name == "faulty_tool":
                    raise ValueError("Intentionally faulty tool error on every call.")
                elif name == "sometimes_faulty_tool":
                    if self.call_counts.get(name, 0) <= self.succeed_after_n_failures:
                        raise ValueError(f"Sometimes_faulty_tool error (call #{self.call_counts[name]})")
                    return f"Success from sometimes_faulty_tool after {self.succeed_after_n_failures} failures."
                elif name == "none_tool":
                    return None
                elif name == "tool_not_found_in_mock":
                    raise ts_module_type.ToolNotFoundError(f"Mock: Tool '{name}' not found.")
                await asyncio.sleep(0) 
                return f"Mock success for {name}"

        # Mock PlannerAgent for testing ExecutionAgent's re-plan logic
        class MockPlannerAgentForExecutionTest(PlannerAgent):
            def __init__(self, new_plan_on_replan=None, fail_replan=False):
                super().__init__() # Not strictly needed if only overriding replan
                self.new_plan_on_replan = new_plan_on_replan if new_plan_on_replan else []
                self.fail_replan = fail_replan
                self.replan_calls = 0

            async def replan_after_failure(self, original_goal: str, failure_analysis: str, available_tools: Dict[str, str], ollama_model_name: Optional[str] = None) -> List[Dict[str, Any]]:
                self.replan_calls += 1
                print(f"MockPlannerAgent: replan_after_failure called for goal '{original_goal}'. Analysis: '{failure_analysis[:50]}...'")
                if self.fail_replan:
                    print("MockPlannerAgent: Simulating re-plan failure (returning empty list).")
                    return []
                print(f"MockPlannerAgent: Returning new plan: {self.new_plan_on_replan}")
                return self.new_plan_on_replan

        # Mock LearningAgent for testing ExecutionAgent's calls
        class MockLearningAgent(LearningAgent): # Inherit from LearningAgent
            def process_reflection_entry(self, entry):
                print(f"MockLearningAgent: process_reflection_entry called for entry ID {entry.entry_id}, Goal: '{entry.goal_description[:30]}...'")

        mock_learning_agent_instance = MockLearningAgent()

        executor_test = ExecutionAgent()
        mock_ollama_model = "mock_model_for_testing" # Passed but not used by mocks here

        print("--- ExecutionAgent Tests with Re-planning ---")
        initial_log_count = len(global_reflection_log.get_entries(limit=1000))

        # Test 1: Simple successful plan (no re-planning needed)
        print("\n--- Test 1: Successful Plan (No Re-plan) ---")
        mock_ts_test1 = MockToolSystemForExecutionTest()
        mock_planner_test1 = MockPlannerAgentForExecutionTest() # Won't be called
        plan1 = [{"tool_name": "greet_user", "args": ("Alice",), "kwargs": {}}, {"tool_name": "add_numbers", "args": ("5", "3"), "kwargs": {}}]
        results1 = await executor_test.execute_plan(
            "Test Goal 1", 
            plan1, 
            mock_ts_test1, 
            mock_planner_test1, 
            mock_learning_agent_instance, # Pass mock learning agent
            ollama_model_name=mock_ollama_model)
        print(f"Test 1 Results: {results1}")
        assert mock_planner_test1.replan_calls == 0
        assert any(isinstance(r, Exception) for r in results1) == False # Should be all success
        mock_ts_test1.tools_called = []; mock_ts_test1.call_counts = {}

        # Test 2: Plan fails, re-planning provides a successful new plan
        print("\n--- Test 2: Failure, Re-plan to Success ---")
        mock_ts_test2 = MockToolSystemForExecutionTest(succeed_after_n_failures=0) # faulty_tool will always fail
                                                                                # sometimes_faulty_tool will succeed on first call if used in new plan
        successful_replan = [{"tool_name": "greet_user", "args": ("Replanned User",), "kwargs": {}}, {"tool_name": "add_numbers", "args": ("10", "10"), "kwargs": {}}]
        mock_planner_test2 = MockPlannerAgentForExecutionTest(new_plan_on_replan=successful_replan)
        
        plan2_initial = [{"tool_name": "faulty_tool", "args": (), "kwargs": {}}] # This will fail
        results2 = await executor_test.execute_plan(
            "Test Goal 2", 
            plan2_initial, 
            mock_ts_test2, 
            mock_planner_test2, 
            mock_learning_agent_instance, 
            ollama_model_name=mock_ollama_model)
        print(f"Test 2 Results: {results2}")
        assert mock_planner_test2.replan_calls == 1
        # Check if final results correspond to the successful_replan
        assert "Hello, Replanned User!" in results2
        assert 20 in results2 # 10 + 10
        assert any("Re-planning attempt 1 initiated" in str(r) for r in results2)
        mock_ts_test2.tools_called = []; mock_ts_test2.call_counts = {}

        # Test 3: Plan fails, re-planning also provides a failing plan (or empty) -> overall failure
        print("\n--- Test 3: Failure, Re-plan also Fails ---")
        mock_ts_test3 = MockToolSystemForExecutionTest() # faulty_tool will always fail
        failing_replan = [{"tool_name": "faulty_tool", "args": ("still_fails",), "kwargs": {}}] # This will also fail
        mock_planner_test3 = MockPlannerAgentForExecutionTest(new_plan_on_replan=failing_replan)
        
        plan3_initial = [{"tool_name": "faulty_tool", "args": (), "kwargs": {}}]
        results3 = await executor_test.execute_plan(
            "Test Goal 3", 
            plan3_initial, 
            mock_ts_test3, 
            mock_planner_test3, 
            mock_learning_agent_instance, 
            ollama_model_name=mock_ollama_model)
        print(f"Test 3 Results: {results3}")
        assert mock_planner_test3.replan_calls == 1 # Called once for the first failure
        # ExecutionAgent.MAX_REPLAN_ATTEMPTS is 1, so after the re-plan fails, it stops.
        # The results should contain results from the first failed plan, the re-plan note, and then the second failed plan.
        assert any(isinstance(r, ValueError) and "Intentionally faulty tool error" in str(r) for r in results3)
        assert any("Re-planning attempt 1 initiated" in str(r) for r in results3)
        mock_ts_test3.tools_called = []; mock_ts_test3.call_counts = {}

        # Test 4: Plan fails, re-planning fails to produce any plan (returns empty list)
        print("\n--- Test 4: Failure, Re-plan returns Empty Plan ---")
        mock_ts_test4 = MockToolSystemForExecutionTest()
        mock_planner_test4 = MockPlannerAgentForExecutionTest(fail_replan=True) # Simulate planner returning []
        
        plan4_initial = [{"tool_name": "faulty_tool", "args": (), "kwargs": {}}]
        results4 = await executor_test.execute_plan(
            "Test Goal 4", 
            plan4_initial, 
            mock_ts_test4, 
            mock_planner_test4, 
            mock_learning_agent_instance, 
            ollama_model_name=mock_ollama_model)
        print(f"Test 4 Results: {results4}")
        assert mock_planner_test4.replan_calls == 1
        assert any(isinstance(r, ValueError) and "Intentionally faulty tool error" in str(r) for r in results4)
        # Should not contain "Re-planning attempt" note if planner returns empty, but the fact that it failed is logged.
        # The overall_plan_execution_results will contain the error from the first plan.
        mock_ts_test4.tools_called = []; mock_ts_test4.call_counts = {}


        # Test 5: Initial plan is empty
        print("\n--- Test 5: Initial Empty Plan ---")
        mock_ts_test5 = MockToolSystemForExecutionTest()
        mock_planner_test5 = MockPlannerAgentForExecutionTest()
        plan5_empty = []
        results5 = await executor_test.execute_plan(
            "Test Goal 5", 
            plan5_empty, 
            mock_ts_test5, 
            mock_planner_test5, 
            mock_learning_agent_instance, 
            ollama_model_name=mock_ollama_model)
        print(f"Test 5 Results: {results5}")
        assert results5 == []
        assert mock_planner_test5.replan_calls == 0

        # Test 6: Max re-plan attempts reached
        print("\n--- Test 6: Max Re-plan Attempts Reached ---")
        ExecutionAgent.MAX_REPLAN_ATTEMPTS = 1 # Ensure it's 1 for this test
        mock_ts_test6 = MockToolSystemForExecutionTest() # faulty_tool always fails
        failing_replan_again = [{"tool_name": "faulty_tool", "args": ("attempt_2_fail",), "kwargs": {}}]
        # Planner will keep providing a failing plan
        mock_planner_test6 = MockPlannerAgentForExecutionTest(new_plan_on_replan=failing_replan_again)
        
        plan6_initial = [{"tool_name": "faulty_tool", "args": ("attempt_1_fail",), "kwargs": {}}]
        results6 = await executor_test.execute_plan(
            "Test Goal 6", 
            plan6_initial, 
            mock_ts_test6, 
            mock_planner_test6, 
            mock_learning_agent_instance, 
            ollama_model_name=mock_ollama_model)
        print(f"Test 6 Results: {results6}")
        assert mock_planner_test6.replan_calls == ExecutionAgent.MAX_REPLAN_ATTEMPTS 
        # Results should show two sets of failures + re-plan note
        assert len([r for r in results6 if isinstance(r, ValueError)]) == 2 # Two faulty_tool errors
        assert any("Re-planning attempt 1 initiated" in str(r) for r in results6)
        mock_ts_test6.tools_called = []; mock_ts_test6.call_counts = {}
        ExecutionAgent.MAX_REPLAN_ATTEMPTS = 1 # Reset for other tests if any

        final_log_count = len(global_reflection_log.get_entries(limit=1000))
        print(f"\nNumber of entries added to global_reflection_log during these tests: {final_log_count - initial_log_count}")
        print("\n--- ExecutionAgent Tests with Re-planning Finished ---")

    if __name__ == '__main__':
        asyncio.run(main_test())
