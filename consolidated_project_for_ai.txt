# === CONSOLIDATED PROJECT VIEW FOR AI ASSISTANT ===
# This file combines multiple Python files from a project into a single text file.
# Its purpose is to provide full contextual understanding to an AI.
#
# HOW TO READ THIS FILE (for AI):
# 1. PROJECT STRUCTURE: The section below outlines the original folder and file layout.
#    [D] denotes a directory, [F] denotes a file.
# 2. CODE BLOCKS: Each original file's content is enclosed by:
#    ### START FILE: path/to/your/file.py ###
#    (content of file.py)
#    ### END FILE: path/to/your/file.py ###
#    Use these markers to understand the original separation and context of the code.
# 3. EXECUTION: This consolidated file is NOT meant to be executed directly.
#    Imports and relative paths will likely be broken.
# =================================================

# --- PROJECT FILE STRUCTURE ---
Project Root Scanned: c:\Users\Owner\Desktop\Self-Evolving-Agent-feat-learning-module\Self-Evolving-Agent-feat-learning-module\Self-Evolving-Agent-feat-chat-history-context
Structure:
  ├── [F] .gitignore
├── [D] ai_assistant
  ├── [F] __init__.py
  ├── [D] ai_generated_projects
    ├── [D] filefinder
      ├── [F] _ai_project_manifest.json
      ├── [F] README.md
      ├── [D] src
        ├── [F] file_system_operations.py
        ├── [F] main.py
        ├── [F] pattern_matching.py
        ├── [F] utils.py
      ├── [D] tests
    ├── [D] hangmangame
      ├── [F] _ai_project_manifest.json
      ├── [F] README.md
      ├── [D] src
        ├── [F] game_logic.py
        ├── [F] main.py
        ├── [F] test_game_logic.py
        ├── [F] ui.py
        ├── [F] word_list.txt
      ├── [D] tests
    ├── [D] meaningfulprojectname
      ├── [F] _ai_project_manifest.json
      ├── [F] README.md
      ├── [D] src
        ├── [F] api_handler.py
        ├── [F] config.py
        ├── [F] data_processor.py
        ├── [F] main.py
        ├── [F] utils.py
      ├── [D] tests
    ├── [D] myhangmangame
      ├── [F] _ai_project_manifest.json
      ├── [F] README.md
      ├── [D] src
        ├── [F] game.py
        ├── [F] graphics.py
        ├── [F] user_input.py
      ├── [D] tests
    ├── [D] mywebapp
      ├── [F] _ai_project_manifest.json
      ├── [F] README.md
      ├── [D] src
        ├── [F] data_storage.py
        ├── [F] main.py
        ├── [F] to_do_manager.py
        ├── [F] ui.py
      ├── [D] tests
    ├── [D] project_name
    ├── [D] sanitized_project_name
    ├── [D] snakegame
      ├── [F] _ai_project_manifest.json
      ├── [F] README.md
      ├── [D] src
        ├── [F] display.py
        ├── [F] food.py
        ├── [F] main.py
        ├── [F] snake.py
        ├── [F] utils.py
      ├── [D] tests
  ├── [D] code_services
    ├── [F] __init__.py
    ├── [F] service.py
  ├── [D] code_synthesis
    ├── [F] __init__.py
    ├── [F] data_structures.py
    ├── [F] service.py
  ├── [D] communication
    ├── [F] __init__.py
    ├── [F] cli.py
  ├── [F] config.py
  ├── [D] core
    ├── [F] __init__.py
    ├── [F] autonomous_reflection.py
    ├── [F] background_service.py
    ├── [F] conversation_intelligence.py
    ├── [D] data
      ├── [F] actionable_insights.json
      ├── [F] event_log.json
      ├── [F] reflection_log.json
      ├── [F] suggestions.json
      ├── [F] tool_registry.json
    ├── [F] fs_utils.py
    ├── [F] orchestrator.py
    ├── [F] project_manager.py
    ├── [F] refinement.py
    ├── [F] reflection.py
    ├── [F] reviewer.py
    ├── [F] self_modification.py
    ├── [F] status_reporting.py
    ├── [F] suggestion_manager.py
    ├── [F] system_executor.py
    ├── [F] tool_creator.py
    ├── [F] tool_designer.py
    ├── [F] tool_executor.py
  ├── [D] custom_tools
    ├── [F] __init__.py
    ├── [F] awareness_tools.py
    ├── [F] code_execution_tools.py
    ├── [F] config_management_tools.py
    ├── [F] conversational_tools.py
    ├── [F] file_system_tools.py
    ├── [D] generated
      ├── [F] __init__.py
    ├── [F] git_tools.py
    ├── [F] knowledge_tools.py
    ├── [F] meta_programming_tools.py
    ├── [F] my_extra_tools.py
    ├── [F] project_execution_tools.py
    ├── [F] project_management_tools.py
    ├── [F] search_tools.py
  ├── [D] data
  ├── [D] debugging
    ├── [F] __init__.py
    ├── [F] logger.py
    ├── [F] resilience.py
  ├── [D] execution
    ├── [F] __init__.py
    ├── [F] action_executor.py
  ├── [D] expansion
    ├── [F] __init__.py
    ├── [F] modularity.py
    ├── [F] plugins.py
  ├── [D] goals
    ├── [F] __init__.py
    ├── [F] goal_management.py
  ├── [D] learning
    ├── [F] __init__.py
    ├── [F] autonomous_learning.py
    ├── [F] evolution.py
    ├── [F] learning.py
  ├── [D] llm_interface
    ├── [F] __init__.py
    ├── [F] ollama_client.py
  ├── [F] main.py
  ├── [D] memory
    ├── [F] __init__.py
    ├── [F] awareness.py
    ├── [F] event_logger.py
    ├── [F] persistent_memory.py
  ├── [D] planning
    ├── [F] __init__.py
    ├── [F] execution.py
    ├── [F] llm_argument_parser.py
    ├── [F] planning.py
  ├── [D] project_management
    ├── [F] manifest_schema.py
  ├── [D] tools
    ├── [F] __init__.py
    ├── [F] tool_management_tools.py
    ├── [F] tool_system.py
  ├── [D] utils
    ├── [F] __init__.py
    ├── [F] display_utils.py
├── [D] docs
  ├── [F] code_writing_system_design.md
  ├── [F] unified_code_writing_system.md
  ├── [F] README.md
  ├── [F] requirements.txt
├── [D] tests
  ├── [F] __init__.py
  ├── [F] test_action_executor.py
  ├── [F] test_autonomous_reflection.py
  ├── [F] test_cli.py
  ├── [F] test_code_service.py
  ├── [F] test_code_synthesis_service.py
  ├── [F] test_learning.py
  ├── [F] test_placeholder.py
  ├── [F] test_planning.py
  ├── [F] test_reflection.py
# --- END OF PROJECT FILE STRUCTURE ---


# --- CONSOLIDATED CODE CONTENT (from: c:\Users\Owner\Desktop\Self-Evolving-Agent-feat-learning-module\Self-Evolving-Agent-feat-learning-module\Self-Evolving-Agent-feat-chat-history-context) ---
# Generated by script: combine_program_for_context.py
# Output file name: consolidated_project_for_ai.txt

# ### START FILE: ai_assistant/__init__.py ###
# This file marks ai_assistant as a package.

# ### END FILE: ai_assistant/__init__.py ###

# ### START FILE: ai_assistant/ai_generated_projects/filefinder/src/file_system_operations.py ###
import os

def list_files_in_directory(directory_path):
    """
    Lists all files in a directory.

    Args:
        directory_path (str): The path to the directory.

    Returns:
        list: A list of filenames within the directory.
              Returns an empty list if the directory is empty or if an error occurs.
    """
    try:
        filenames = os.listdir(directory_path)
        return filenames
    except FileNotFoundError:
        return []
    except Exception as e:
        print(f"An error occurred: {e}")
        return []

def check_file_exists(file_path):
    """
    Checks if a file exists.

    Args:
        file_path (str): The path to the file.

    Returns:
        bool: True if the file exists, False otherwise.
    """
    return os.path.exists(file_path)
# ### END FILE: ai_assistant/ai_generated_projects/filefinder/src/file_system_operations.py ###

# ### START FILE: ai_assistant/ai_generated_projects/filefinder/src/main.py ###
def main():
    try:
        directory_path = get_user_input("Enter the directory path: ")
        search_pattern = get_user_input("Enter the search pattern: ")
        
        results = call_search_function(directory_path, search_pattern)
        
        if results:
            print_results(results)
        else:
            print("No files found matching the search pattern.")
    except Exception as e:
        print(f"An error occurred: {e}")

if __name__ == "__main__":
    main()
# ### END FILE: ai_assistant/ai_generated_projects/filefinder/src/main.py ###

# ### START FILE: ai_assistant/ai_generated_projects/filefinder/src/pattern_matching.py ###
import re

def match_pattern(filename, pattern):
    """
    Matches a filename against a given search pattern.

    Args:
        filename (str): The filename to search against.
        pattern (str): The search pattern.

    Returns:
        list: A list of filenames that match the pattern.
              Returns an empty list if no files match.
    """
    try:
        regex = re.compile(pattern)
        matches = []
        if isinstance(filename, list):
            for file in filename:
                if regex.search(file):
                    matches.append(file)
        else:
            if regex.search(filename):
                matches.append(filename)
            
        return matches
    except re.error:
        print("Invalid regular expression pattern.")
        return []
# ### END FILE: ai_assistant/ai_generated_projects/filefinder/src/pattern_matching.py ###

# ### START FILE: ai_assistant/ai_generated_projects/filefinder/src/utils.py ###
import logging

def handle_file_not_found_error(filename):
    """
    Handles the FileNotFoundError exception.
    """
    try:
        raise FileNotFoundError(f"File not found: {filename}")
    except Exception as e:
        logging.error(f"Error handling file not found: {e}")
        raise

def log_search_results(results):
    """
    Logs the search results.
    """
    log_message = f"Search Results: {results}"
    logging.info(log_message)
# ### END FILE: ai_assistant/ai_generated_projects/filefinder/src/utils.py ###

# ### START FILE: ai_assistant/ai_generated_projects/hangmangame/src/game_logic.py ###
import random

def select_word(word_list):
    """Selects a random word from the word list."""
    if not word_list:
        return None  # Handle empty word list
    return random.choice(word_list)

def check_guess(word, guess, displayed_word, incorrect_guesses):
    """Checks if the user's guess is correct and updates the game state."""
    if guess in word:
        displayed_word = displayed_word.replace(guess, guess, 1)
        return displayed_word, incorrect_guesses
    else:
        incorrect_guesses += 1
        return displayed_word, incorrect_guesses

def update_game_state(word, displayed_word, incorrect_guesses, guessed_letters):
    """Updates the game state, displaying the word and showing guessed letters."""
    if incorrect_guesses >= 6:
        return "You lost!"
    if "_" not in displayed_word:
        return "You won!"
    return displayed_word, incorrect_guesses, guessed_letters

def determine_winner(word, displayed_word, incorrect_guesses):
    """Checks if the user has guessed the word correctly or if they've run out of attempts."""
    if "_" not in displayed_word:
        return "You won!"
    if incorrect_guesses >= 6:
        return "You lost!"
    return None
# ### END FILE: ai_assistant/ai_generated_projects/hangmangame/src/game_logic.py ###

# ### START FILE: ai_assistant/ai_generated_projects/hangmangame/src/main.py ###
def main():
    # Initialize the game
    # game_logic.py handles the setup_game function
    # Assuming setup_game returns the hidden word and the number of attempts remaining
    # For example: hidden_word, attempts_remaining = setup_game()

    # Game loop
    attempts_remaining = 6  # Initial number of attempts
    while attempts_remaining > 0:
        # Display the current game state
        # ui.py handles display_game_state(hidden_word, attempts_remaining)
        
        # Get user input
        guess = handle_input() # Assuming handle_input() returns a single letter

        # Validate user input
        if not guess.isalpha() or len(guess) != 1:
            print("Invalid input. Please enter a single letter.")
            continue

        # Update the game state based on the input
        # game_logic.py handles the logic for checking if the guess is correct
        # and updating the hidden word and attempts_remaining
        # For example: hidden_word, attempts_remaining = game_logic.check_guess(guess, hidden_word, attempts_remaining)

        # Display the updated game state
        # ui.py handles display_game_state(hidden_word, attempts_remaining)

        if hidden_word == " ":
            print("You won! The word was:", hidden_word)
            break
        if attempts_remaining == 0:
            print("You lost! The word was:", hidden_word)
            break

    # End of game loop
    
def handle_input():
    # Placeholder for user input handling.  This should call ui.py
    # to get the user's guess.
    return input("Guess a letter: ")

if __name__ == "__main__":
    main()
# ### END FILE: ai_assistant/ai_generated_projects/hangmangame/src/main.py ###

# ### START FILE: ai_assistant/ai_generated_projects/hangmangame/src/test_game_logic.py ###
import unittest
from game_logic import select_word, check_guess, determine_winner

class TestGameLogic(unittest.TestCase):

    def test_select_word(self):
        word_list = ["apple", "banana", "cherry"]
        word = select_word(word_list)
        self.assertIsInstance(word, str)
        self.assertTrue(len(word) > 0)
        self.assertTrue(word in word_list)

    def test_check_guess(self):
        word = "apple"
        guess = "a"
        result = check_guess(word, guess)
        self.assertEqual(result, True)

        guess = "z"
        result = check_guess(word, guess)
        self.assertEqual(result, False)

        word = "banana"
        guess = "n"
        result = check_guess(word, guess)
        self.assertEqual(result, True)

    def test_determine_winner(self):
        word = "apple"
        guessed_letters = ["a", "p", "p", "l", "e"]
        winner = determine_winner(word, guessed_letters)
        self.assertTrue(winner)

        word = "banana"
        guessed_letters = ["b", "a", "n", "a", "n", "a"]
        winner = determine_winner(word, guessed_letters)
        self.assertTrue(winner)

        word = "cherry"
        guessed_letters = ["c", "h", "e", "r", "r", "y"]
        winner = determine_winner(word, guessed_letters)
        self.assertTrue(winner)

if __name__ == '__main__':
    unittest.main()
# ### END FILE: ai_assistant/ai_generated_projects/hangmangame/src/test_game_logic.py ###

# ### START FILE: ai_assistant/ai_generated_projects/hangmangame/src/ui.py ###
# ui.py
# Handles the user interface for the Hangman game.
# Assumes game_logic.py exists and provides the correct word when the game is over.

def render_word(hidden_word, guessed_letters):
    """
    Displays the hidden word with underscores for unguessed letters.
    """
    displayed_word = ""
    for letter in hidden_word:
        if letter in guessed_letters:
            displayed_word += letter + " "
        else:
            displayed_word += "_ "
    return displayed_word.strip()

def display_guesses(guessed_letters):
    """
    Displays the list of letters the user has correctly guessed.
    """
    if not guessed_letters:
        return "No letters guessed yet."
    else:
        return ", ".join(guessed_letters)

def update_ui(hidden_word, guessed_letters, game_over):
    """
    Orchestrates the updates to the UI based on the game logic's output.
    """
    if game_over:
        print("Game Over!")
        print(render_word(hidden_word, guessed_letters))
    else:
        print(render_word(hidden_word, guessed_letters))
        print("Guessed letters:", display_guesses(guessed_letters))

def handle_display_updates(game_logic_output):
    """
    Placeholder for future enhancements to handle UI updates.
    """
    pass
# ### END FILE: ai_assistant/ai_generated_projects/hangmangame/src/ui.py ###

# ### START FILE: ai_assistant/ai_generated_projects/meaningfulprojectname/src/api_handler.py ###
import utils
from main import app

def route_dispatcher(request):
    routes = {
        '/api/data': handle_data_request,
        '/api/user': handle_user_request,
        '/api/validate': handle_validation_request
    }
    url = request.get('url')
    method = request.get('method')
    handler = routes.get((url, method))
    if handler:
        return handler(request)
    else:
        return response_formatter({'error': 'Not Found'})

def request_validator(request):
    is_valid, message = utils.validate_request(request)
    if not is_valid:
        return False, message
    return True, None

def response_formatter(data, error=None):
    if error:
        return {'error': error}
    return {'data': data}

def handle_data_request(request):
    is_valid, message = request_validator(request)
    if not is_valid:
        return response_formatter({}, message)
    return response_formatter({'message': 'Data request handled'})

def handle_user_request(request):
    is_valid, message = request_validator(request)
    if not is_valid:
        return response_formatter({}, message)
    return response_formatter({'message': 'User request handled'})

def handle_validation_request(request):
    is_valid, message = request_validator(request)
    if not is_valid:
        return response_formatter({}, message)
    return response_formatter({'message': 'Validation request handled'})

@app.route('/api/*', methods=['GET', 'POST', 'PUT', 'DELETE'])
def api_route():
    request = {'url': request.path, 'method': request.method}
    response = route_dispatcher(request)
    return response_formatter(response)
# ### END FILE: ai_assistant/ai_generated_projects/meaningfulprojectname/src/api_handler.py ###

# ### START FILE: ai_assistant/ai_generated_projects/meaningfulprojectname/src/config.py ###
import os

DATABASE_URL = os.environ.get('DATABASE_URL', 'sqlite:///app.db')
api_keys_str = os.environ.get('API_KEYS', '')
API_KEYS = api_keys_str.split(',') if api_keys_str else []
LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO')

def load_config():
    return {
        'DATABASE_URL': DATABASE_URL,
        'API_KEYS': API_KEYS,
        'LOG_LEVEL': LOG_LEVEL
    }
# ### END FILE: ai_assistant/ai_generated_projects/meaningfulprojectname/src/config.py ###

# ### START FILE: ai_assistant/ai_generated_projects/meaningfulprojectname/src/data_processor.py ###
import pandas as pd
import numpy as np
from sqlalchemy import create_engine
from config import DATA_SOURCE_PATH, DATABASE_URI, TABLE_NAME
from utils import clean_data, validate_data

def ingest_data():
    """Ingest data from a CSV file specified in the configuration."""
    try:
        data = pd.read_csv(DATA_SOURCE_PATH)
        return data
    except Exception as e:
        raise ValueError(f"Error ingesting data: {str(e)}")

def transform_data(data):
    """Transform data using utility functions and basic preprocessing."""
    try:
        # Validate data integrity
        validate_data(data)
        
        # Clean and preprocess data
        data = clean_data(data)
        
        # Additional transformation steps
        data['timestamp'] = pd.to_datetime(data['timestamp'])
        data = data.dropna()
        
        return data
    except Exception as e:
        raise ValueError(f"Error transforming data: {str(e)}")

def store_data(data):
    """Store processed data in a SQL database using the configured URI."""
    try:
        engine = create_engine(DATABASE_URI)
        data.to_sql(TABLE_NAME, engine, if_exists='replace', index=False)
        return True
    except Exception as e:
        raise ValueError(f"Error storing data: {str(e)}")
# ### END FILE: ai_assistant/ai_generated_projects/meaningfulprojectname/src/data_processor.py ###

# ### START FILE: ai_assistant/ai_generated_projects/meaningfulprojectname/src/main.py ###
import config
import utils

def main():
    try:
        config.load_config()
        utils.initialize()
        register_event_listeners()
        while True:
            utils.process_events()
            if utils.check_shutdown():
                break
    except Exception as e:
        utils.log_error(f"Critical error: {str(e)}")
        utils.cleanup()

if __name__ == "__main__":
    main()
# ### END FILE: ai_assistant/ai_generated_projects/meaningfulprojectname/src/main.py ###

# ### START FILE: ai_assistant/ai_generated_projects/meaningfulprojectname/src/utils.py ###
import logging
import datetime
import config

def validate_integer(value, min_val=None, max_val=None):
    if not isinstance(value, int):
        raise ValueError(f"Expected integer, got {type(value)}")
    if min_val is not None and value < min_val:
        raise ValueError(f"Value {value} is below minimum {min_val}")
    if max_val is not None and value > max_val:
        raise ValueError(f"Value {value} exceeds maximum {max_val}")

def validate_string(value, min_length=None, max_length=None):
    if not isinstance(value, str):
        raise ValueError(f"Expected string, got {type(value)}")
    if min_length is not None and len(value) < min_length:
        raise ValueError(f"String length {len(value)} is below minimum {min_length}")
    if max_length is not None and len(value) > max_length:
        raise ValueError(f"String length {len(value)} exceeds maximum {max_length}")

def log_error(message, error=None):
    logging.error(message)
    if error:
        logging.error("Error details: %s", error)

def get_current_time(format_str="%Y-%m-%d %H:%M:%S"):
    return datetime.datetime.now().strftime(format_str)
# ### END FILE: ai_assistant/ai_generated_projects/meaningfulprojectname/src/utils.py ###

# ### START FILE: ai_assistant/ai_generated_projects/myhangmangame/src/game.py ###
import random
import os

word_list = ['apple', 'banana', 'cherry', 'date', 'elderberry']

def handle_user_input(user_input):
    if len(user_input) == 1:
        return user_input.lower()
    elif len(user_input) <= len(word_list[0]):
        return ''.join([user_input])
    else:
        print('Invalid input. Please enter a single letter or a whole word.')
        return None

def update_game_state(word_to_guess, guessed_letters, incorrect_guesses):
    for letter in word_to_guess:
        if letter not in guessed_letters:
            incorrect_guesses.append(letter)
    return word_to_guess, guessed_letters, incorrect_guesses

def display_hangman(incorrect_guesses):
    stages = [  # final state: head, torso, both arms, and both legs
                """
                   --------
                   |      |
                   |      O
                   |     \\|/
                   |      |
                   |     / \\
                   -
                """,
                # head, torso, both arms, and one leg
                """
                   --------
                   |      |
                   |      O
                   |     \\|/
                   |      |
                   |     / 
                   -
                """,
                # head, torso, and both arms
                """
                   --------
                   |      |
                   |      O
                   |     \\|/
                   |      |
                   |      
                   -
                """,
                # head, torso, and one arm
                """
                   --------
                   |      |
                   |      O
                   |     \\|
                   |      |
                   |     
                   -
                """,
                # head and torso
                """
                   --------
                   |      |
                   |      O
                   |      |
                   |      |
                   |     
                   -
                """,
                # head
                """
                   --------
                   |      |
                   |      O
                   |    
                   |      
                   |     
                   -
                """,
                # initial empty state
                """
                   --------
                   |      |
                   |      
                   |    
                   |      
                   |     
                   -
                """
    ]
    return stages[len(incorrect_guesses)-1]

def main():
    word_to_guess = random.choice(word_list)
    guessed_letters = []
    incorrect_guesses = []

    while len(incorrect_guesses) < 6:
        user_input = input("Guess a letter or a whole word: ")
        user_input = handle_user_input(user_input)

        if user_input is None:
            continue

        word_to_guess, guessed_letters, incorrect_guesses = update_game_state(word_to_guess, guessed_letters, incorrect_guesses)
        print(' '.join([letter if letter in guessed_letters else display_hangman(incorrect_guesses)[7+i] for i, letter in enumerate(word_to_guess)]))

    if len(incorrect_guesses) < 6:
        print(f'Congratulations! You won. The word was {word_to_guess}.')
    else:
        print('Game over. The word was ' + word_to_guess)

if __name__ == "__main__":
    main()
# ### END FILE: ai_assistant/ai_generated_projects/myhangmangame/src/game.py ###

# ### START FILE: ai_assistant/ai_generated_projects/myhangmangame/src/graphics.py ###
from PIL import Image, ImageDraw

def generate_hangman_image(incorrect_guesses):
    # Create a new image with white background
    img = Image.new('RGB', (200, 200), color=(255, 255, 255))
    
    # Define the hangman parts and their corresponding incorrect guesses
    hangman_parts = {
        'head': 1,
        'body': 2,
        'left_arm': 3,
        'right_arm': 4,
        'left_leg': 5,
        'right_leg': 6
    }
    
    # Draw each part of the hangman graphic based on the number of incorrect guesses
    for i, (part, guess_count) in enumerate(hangman_parts.items()):
        if guess_count <= incorrect_guesses:
            draw = ImageDraw.Draw(img)
            if part == 'head':
                x, y = 50, 50
                radius = 20
                draw.ellipse([(x - radius, y - radius), (x + radius, y + radius)], fill=(0, 0, 0))
            elif part == 'body':
                x, y = 75, 100
                width, height = 10, 10
                draw.rectangle([(x - width/2, y - height/2), (x + width/2, y + height/2)], fill=(0, 0, 0))
            elif part == 'left_arm':
                x, y = 50, 150
                width, height = 5, 20
                draw.line([(x, y - height), (x - width/2, y + height)], fill=(0, 0, 0))
            elif part == 'right_arm':
                x, y = 75, 150
                width, height = 5, 20
                draw.line([(x, y - height), (x + width/2, y + height)], fill=(0, 0, 0))
            elif part == 'left_leg':
                x, y = 50, 200
                width, height = 5, 10
                draw.line([(x, y - height), (x - width/2, y + height)], fill=(0, 0, 0))
            elif part == 'right_leg':
                x, y = 75, 200
                width, height = 5, 10
                draw.line([(x, y - height), (x + width/2, y + height)], fill=(0, 0, 0))
    return img

def draw_hangman_on_screen(img):
    # Display the generated image on the screen using Pygame
    from pygame import display
    display.set_mode((800, 600))
    display.set_caption('Hangman Game')
    screen = display.set_mode((800, 600))
    screen.blit(img, (0, 0))
    display.update()

# Example usage:
incorrect_guesses = 5
generated_img = generate_hangman_image(incorrect_guesses)
draw_hangman_on_screen(generated_img)
# ### END FILE: ai_assistant/ai_generated_projects/myhangmangame/src/graphics.py ###

# ### START FILE: ai_assistant/ai_generated_projects/myhangmangame/src/user_input.py ###
def handle_letter_input(letter):
    if len(letter) != 1:
        return False
    elif not letter.isalpha():
        return False
    else:
        return True

def check_for_win_conditions(word, guessed_letters):
    for letter in word:
        if letter not in guessed_letters:
            return False
    return True

def generate_error_message(error_type):
    error_messages = {
        'invalid_input': "Please enter a single alphabetic character.",
        'already_guessed': "You have already guessed this letter. Please try again.",
        'win_condition_met': " Congratulations, you won!"
    }
    return error_messages.get(error_type, "An unknown error occurred.")

def get_user_letter():
    while True:
        user_input = input("Please enter a single alphabetic character: ")
        if handle_letter_input(user_input):
            return user_input
        else:
            print(generate_error_message('invalid_input'))

def update_game_state(word, guessed_letters, correct_guesses):
    for letter in word:
        if letter not in guessed_letters:
            correct_guesses.append(letter)
    return guessed_letters, correct_guesses

def check_win_condition(game_state, word):
    return check_for_win_conditions(word, game_state[0])

def display_error_message(error_type):
    error_messages = {
        'invalid_input': "Please enter a single alphabetic character.",
        'already_guessed': "You have already guessed this letter. Please try again.",
        'win_condition_met': " Congratulations, you won!"
    }
    print(error_messages.get(error_type, "An unknown error occurred."))

def main():
    word = input("Please enter the word to guess: ")
    guessed_letters = []
    correct_guesses = []
    game_state = update_game_state(word, guessed_letters, correct_guesses)
    while True:
        user_letter = get_user_letter()
        if check_win_condition(game_state, word):
            display_error_message('win_condition_met')
            break
        else:
            updated_game_state = update_game_state(word, guessed_letters, correct_guesses)
            game_state = updated_game_state
# ### END FILE: ai_assistant/ai_generated_projects/myhangmangame/src/user_input.py ###

# ### START FILE: ai_assistant/ai_generated_projects/mywebapp/src/data_storage.py ###
def save_to_file(items, filename="todo.txt"):
    """Saves a list of to-do items to a file."""
    try:
        with open(filename, "w") as f:
            for item in items:
                f.write(item + "\n")
    except Exception as e:
        print(f"Error saving to file: {e}")


def load_from_file(filename="todo.txt"):
    """Loads to-do items from a file."""
    try:
        with open(filename, "r") as f:
            items = [line.strip() for line in f]
        return items
    except FileNotFoundError:
        return []
    except Exception as e:
        print(f"Error loading from file: {e}")
        return []
# ### END FILE: ai_assistant/ai_generated_projects/mywebapp/src/data_storage.py ###

# ### START FILE: ai_assistant/ai_generated_projects/mywebapp/src/main.py ###
import to_do_manager

def main():
    while True:
        print("\nTo-Do List Application")
        print("1. Add task")
        print("2. List tasks")
        print("3. Mark task as complete")
        print("4. Exit")

        choice = input("Enter your choice: ")

        if choice == '1':
            task = input("Enter task description: ")
            to_do_manager.add_task(task)
        elif choice == '2':
            tasks = to_do_manager.get_all_tasks()
            if tasks:
                for i, task in enumerate(tasks):
                    print(f"{i+1}. {task}")
            else:
                print("No tasks in the to-do list.")
        elif choice == '3':
            try:
                task_number = int(input("Enter the number of the task to mark as complete: ")) - 1
                if 0 <= task_number < len(to_do_manager.get_all_tasks()):
                    to_do_manager.mark_task_complete(task_number)
                else:
                    print("Invalid task number.")
            except ValueError:
                print("Invalid input. Please enter a number.")
        elif choice == '4':
            print("Exiting application.")
            break
        else:
            print("Invalid choice. Please try again.")

if __name__ == "__main__":
    main()
# ### END FILE: ai_assistant/ai_generated_projects/mywebapp/src/main.py ###

# ### START FILE: ai_assistant/ai_generated_projects/mywebapp/src/to_do_manager.py ###
def add_item(to_do_list, task):
    """Adds a new to-do item to the list."""
    to_do_list.append({"task": task, "complete": False})
    print(f"Added item: {task}")

def delete_item(to_do_list, task):
    """Removes a to-do item from the list."""
    for i, item in enumerate(to_do_list):
        if item["task"] == task:
            del to_do_list[i]
            print(f"Deleted item: {task}")
            return
    print(f"Item not found: {task}")

def mark_complete(to_do_list, task):
    """Marks an existing to-do item as complete."""
    for item in to_do_list:
        if item["task"] == task:
            item["complete"] = True
            print(f"Marked '{task}' as complete.")
            return
    print(f"Item not found: {task}")

def list_items(to_do_list):
    """Displays all the to-do items in the list."""
    if not to_do_list:
        print("No items in the to-do list.")
        return

    for i, item in enumerate(to_do_list):
        status = "[X]" if item["complete"] else "[ ]"
        print(f"{i+1}. {status} {item['task']}")

if __name__ == '__main__':
    to_do_list = []

    while True:
        print("\nTo-Do List Manager")
        print("1. Add item")
        print("2. Delete item")
        print("3. Mark complete")
        print("4. List items")
        print("5. Exit")

        choice = input("Enter your choice: ")

        if choice == '1':
            task = input("Enter task description: ")
            add_item(to_do_list, task)
        elif choice == '2':
            task = input("Enter task to delete: ")
            delete_item(to_do_list, task)
        elif choice == '3':
            task = input("Enter task to mark complete: ")
            mark_complete(to_do_list, task)
        elif choice == '4':
            list_items(to_do_list)
        elif choice == '5':
            break
        else:
            print("Invalid choice. Please try again.")
# ### END FILE: ai_assistant/ai_generated_projects/mywebapp/src/to_do_manager.py ###

# ### START FILE: ai_assistant/ai_generated_projects/mywebapp/src/ui.py ###
def display_menu():
    print("\nTo-Do List Manager")
    print("1. Add Task")
    print("2. View Tasks")
    print("3. Delete Task")
    print("4. Quit")

def get_user_input():
    try:
        return int(input("Enter your choice: "))
    except ValueError:
        print("Invalid input. Please enter a number.")
        return None

def process_command(choice):
    if choice == 1:
        print("Adding task. (Not implemented in this example)")
    elif choice == 2:
        print("Viewing tasks. (Not implemented in this example)")
    elif choice == 3:
        print("Deleting task. (Not implemented in this example)")
    elif choice == 4:
        print("Exiting...")
        return False  # Signal to exit the main loop
    else:
        print("Invalid choice.")
    return True #Signal to continue

def main():
    running = True
    while running:
        display_menu()
        choice = get_user_input()
        if choice is None:
            continue
        running = process_command(choice)

if __name__ == "__main__":
    main()
# ### END FILE: ai_assistant/ai_generated_projects/mywebapp/src/ui.py ###

# ### START FILE: ai_assistant/ai_generated_projects/snakegame/src/display.py ###
import pygame

def initialize_display(width, height):
    pygame.init()
    screen = pygame.display.set_mode((width, height))
    pygame.display.set_caption("Snake")
    return screen

def draw_snake(screen, snake_coordinates):
    for x, y in snake_coordinates:
        pygame.draw.rect(screen, (0, 0, 20, 20), pygame.Rect(x, y, 20, 20))

def draw_food(screen, food_x, food_y):
    pygame.draw.rect(screen, (255, 0, 0), pygame.Rect(food_x, food_y, 20, 20))

def draw_boundaries(screen):
    pygame.draw.rect(screen, (0, 0, 20, 20), pygame.Rect(0, 0, 20, 20))
    pygame.draw.rect(screen, (0, 0, 20, 20), pygame.Rect(width - 20, 0, 20, 20))
    pygame.draw.rect(screen, (0, 0, 20, 20), pygame.Rect(0, height - 20, 20, 20))
    pygame.draw.rect(screen, (0, 0, 20, 20), pygame.Rect(width - 20, height - 20, 20, 20))

def update_display(screen, snake_coordinates, food_x, food_y):
    screen.fill((255, 255, 255))  # White background
    draw_snake(screen, snake_coordinates)
    draw_food(screen, food_x, food_y)
    draw_boundaries(screen)
    pygame.display.flip()
# ### END FILE: ai_assistant/ai_generated_projects/snakegame/src/display.py ###

# ### START FILE: ai_assistant/ai_generated_projects/snakegame/src/food.py ###
import random

class Food:
    def __init__(self, grid_width, grid_height):
        self.x = 0
        self.y = 0
        self.grid_width = grid_width
        self.grid_height = grid_height
        self.generate_food()

    def generate_food(self):
        self.x = random.randint(0, self.grid_width - 1)
        self.y = random.randint(0, self.grid_height - 1)

    def move_food(self):
        # Placeholder - This method will be called in the snake.py file
        pass

    def reset(self):
        self.generate_food()
# ### END FILE: ai_assistant/ai_generated_projects/snakegame/src/food.py ###

# ### START FILE: ai_assistant/ai_generated_projects/snakegame/src/main.py ###
import snake
import food
import display
import utils

def main():
    # Initialize game state
    game_state = utils.initialize_game_state()

    # Run the game loop
    while not game_state['game_over']:
        # Handle input
        event = utils.get_event()
        if event:
            snake.move(game_state['snake'], event)

        # Update game state
        snake.update(game_state)
        food.update(game_state)

        # Render the game
        display.render(game_state)

        # Check for game over
        if game_state['game_over']:
            break

    print("Game Over!")

if __name__ == "__main__":
    main()
# ### END FILE: ai_assistant/ai_generated_projects/snakegame/src/main.py ###

# ### START FILE: ai_assistant/ai_generated_projects/snakegame/src/snake.py ###
class Snake:
    """
    Represents the snake in the Snake game.
    Handles movement, growth, and collision detection.
    """

    def __init__(self, start_position, direction="right"):
        """
        Initializes the snake with a starting position and direction.

        Args:
            start_position (tuple): The initial (x, y) coordinates of the snake's head.
            direction (str): The initial direction of the snake ("up", "down", "left", "right").
        """
        self.body = [start_position]
        self.direction = direction
        self.head = start_position

    def move(self):
        """
        Updates the snake's position based on its current direction.
        """
        if self.direction == "up":
            self.head = (self.head[0], self.head[1] - 1)
        elif self.direction == "down":
            self.head = (self.head[0], self.head[1] + 1)
        elif self.direction == "left":
            self.head = (self.head[0] - 1, self.head[1])
        elif self.direction == "right":
            self.head = (self.head[0] + 1, self.head[1])

        self.body.insert(0, self.head)  # Add new head to the beginning of the body
        if len(self.body) > 1:
            self.body.pop()  # Remove the tail segment

    def grow(self):
        """
        Increases the length of the snake by adding a new segment to its tail.
        """
        self.body.insert(0, self.body[-1])

    def check_collision(self):
        """
        Checks if the snake has collided with the walls or itself.

        Returns:
            bool: True if a collision has occurred, False otherwise.
        """
        if (
            self.head[0] < 0
            or self.head[0] >= 10  # Assuming a 10x10 grid
            or self.head[1] < 0
            or self.head[1] >= 10
        ):
            return True

        for i in range(len(self.body)):
            if self.body[i] == self.head:
                return True

        return False

    def reset(self):
        """
        Resets the snake's position and length to their initial values.
        """
        self.body = [self.head]
        self.direction = "right"
# ### END FILE: ai_assistant/ai_generated_projects/snakegame/src/snake.py ###

# ### START FILE: ai_assistant/ai_generated_projects/snakegame/src/utils.py ###
def handle_input():
    """
    Handles user input to update the snake's direction.
    """
    global direction
    if direction != "right" and key_press == "right":
        direction = "right"
    elif direction != "left" and key_press == "left":
        direction = "left"
    elif direction != "up" and key_press == "up":
        direction = "up"
    elif direction != "down" and key_press == "down":
        direction = "down"

def check_collision(snake_body, food_position):
    """
    Checks if the snake has collided with the walls or itself.
    """
    x, y = snake_body[0]
    width = game_width
    height = game_height

    if x < 0 or x >= width or y < 0 or y >= height:
        return True

    for i in range(1, len(snake_body)):
        if snake_body[i] == snake_body[0]:
            return True

    return False

def get_key_press():
    """
    Simulates key press detection for testing purposes.
    """
    global key_press
    key_press = None
# ### END FILE: ai_assistant/ai_generated_projects/snakegame/src/utils.py ###

# ### START FILE: ai_assistant/code_services/__init__.py ###
# Initializes the code_services package.

# ### END FILE: ai_assistant/code_services/__init__.py ###

# ### START FILE: ai_assistant/code_services/service.py ###
# ai_assistant/code_services/service.py
import logging
from typing import Dict, Any, Optional
import re
import json # For parsing metadata

# Assuming these imports are relative to the ai_assistant package root
from ..config import get_model_for_task, is_debug_mode
from ..core.fs_utils import write_to_file

logger = logging.getLogger(__name__)
if not logger.handlers: # pragma: no cover
    if not logging.getLogger().handlers:
         logger.addHandler(logging.StreamHandler())
         logger.setLevel(logging.INFO)

LLM_NEW_TOOL_PROMPT_TEMPLATE = """Based on the following high-level description of a desired tool, your task is to generate a single Python function and associated metadata.

Tool Description: "{description}"

Instructions:
1.  **Metadata Line (First Line of Response):** At the very beginning of your response, include a line starting with '# METADATA: ' followed by a JSON string. This JSON string *MUST* contain:
    - 'suggested_function_name': A Pythonic function name (snake_case) for the generated function.
    - 'suggested_tool_name': A short, user-friendly name for tool registration (camelCase or snake_case is acceptable).
    - 'suggested_description': A concise description (max 1-2 sentences) of what the tool does, suitable for a tool registry.
    Example of the first line of the response:
    # METADATA: {{"suggested_function_name": "calculate_circle_area", "suggested_tool_name": "calculateCircleArea", "suggested_description": "Calculates the area of a circle given its radius."}}

2.  **Python Function Code (Following Metadata):** After the metadata line, provide the raw Python code for the function.
    - The function should be self-contained if possible, or use common Python standard libraries.
    - Include type hints for all parameters and the return value.
    - Include a comprehensive docstring explaining what the function does, its arguments (name, type, description), and what it returns.
    - Implement basic error handling using try-except blocks where appropriate (e.g., for type conversions if arguments might not be of the expected type, or for file operations).

Constraints:
- Respond ONLY with the metadata line followed by the raw Python code.
- Do not include any other explanations, comments outside the function's docstring (except the metadata line), or markdown formatting like ```python.

Response Structure:
# METADATA: {{"suggested_function_name": "...", "suggested_tool_name": "...", "suggested_description": "..."}}
def generated_function_name(param1: type, ...) -> return_type:
    \"\"\"Docstring for the function.\"\"\"
    # Function implementation
    ...

Now, generate the metadata and Python function based on the Tool Description provided above.
"""

LLM_CODE_FIX_PROMPT_TEMPLATE = """
The following Python function (from module '{module_path}', function name '{function_name}') has an issue.
Original Problem Description / Goal for Fix:
{problem_description}

Original Function Code:
```python
{original_code}
```

Your task is to provide a corrected version of this Python function.
- Only output the complete, raw Python code for the corrected function.
- Do NOT include any explanations, markdown formatting (like ```python), or any text other than the function code itself.
- Ensure the function signature (name, parameters, type hints) remains the same unless the problem description explicitly requires changing it.
- If you cannot determine a fix or the original code is not a single function, return only the text: "// NO_CODE_SUGGESTION_POSSIBLE"

Corrected Python function code:
"""

# Prompt for generating unit test scaffolds
LLM_UNIT_TEST_SCAFFOLD_PROMPT_TEMPLATE = """You are an expert Python testing assistant.
Given the following Python code, generate a basic unit test scaffold using the 'unittest' framework.

The scaffold should include:
1.  Necessary imports (e.g., `unittest`, and the module containing the code to be tested if it's implied to be in a separate file). Assume the code to be tested is available in a module that can be imported as '{module_name_hint}'.
2.  A test class that inherits from `unittest.TestCase`.
3.  A `setUp` method if it seems beneficial (e.g., if the input code is a class that needs instantiation).
4.  Placeholder test methods (e.g., `test_function_name_basic_case`, `test_function_name_edge_case`) for each public function or method in the provided code.
    - Each placeholder test method should include `self.fail("Test not yet implemented")` or a simple `pass`.
5.  An `if __name__ == '__main__': unittest.main()` block.

Do NOT generate actual test logic or assertions within the placeholder methods. Only generate the structural scaffold.

Python code to generate a unit test scaffold for:
```python
{code_to_test}
```

Unit test scaffold:
"""

# Prompt for generating a structured outline for hierarchical code generation
LLM_HIERARCHICAL_OUTLINE_PROMPT_TEMPLATE = """
You are a senior software architect. Based on the following high-level requirement, generate a structural outline of the Python code needed.
The outline must be a single JSON object.
The JSON object should describe the main module, any classes, and functions/methods.
For each component (module, class, function, method), include:
- "type": e.g., "module", "class", "function", "method"
- "name": The Pythonic name.
- "description": A brief explanation of its purpose.
- (For functions/methods) "signature": e.g., "(self, arg1: str, arg2: int) -> bool"
- (For functions/methods) "body_placeholder": A short comment or note indicating what the implementation should achieve.
- (For classes) "attributes": A list of attribute definitions (name, type, description).
- (For modules) "imports": A list of necessary Python modules to import.

High-Level Requirement:
{high_level_description}

JSON Outline:
"""

# Prompt for generating component details based on an outline
LLM_COMPONENT_DETAIL_PROMPT_TEMPLATE = """You are an expert Python programmer. Your task is to implement the body of a specific Python function or method based on its definition and the overall context of its containing module or class.

Overall Module/Class Context:
<context_summary>
{overall_context_summary}
</context_summary>

Component to Implement:
- Type: {component_type}
- Name: {component_name}
- Signature: `{component_signature}`
- Description/Purpose: {component_description}
- Body Placeholder (Initial thought from outline): {component_body_placeholder}

Required Module-Level Imports (available for use, do not redeclare unless shadowing):
{module_imports}

Instructions for Implementation:
1.  Implement *only* the Python code for the body of the function/method `{component_name}`.
2.  Adhere strictly to the provided signature: `{component_signature}`.
3.  Ensure your code fulfills the component's described purpose: "{component_description}" and expands on the placeholder: "{component_body_placeholder}".
4.  Use the provided module-level imports if needed. Do not add new module-level imports unless absolutely necessary and clearly justified by a specific library for the task. Local imports within the function are acceptable if scoped appropriately.
5.  If the component is a class method, you can assume it has access to `self` and any attributes defined in the `Overall Module/Class Context` (if provided for a class).
6.  Focus on clear, correct, and efficient Python code. Include comments for complex logic.
7.  For simplicity and consistency, always generate the full component code including signature, i.e., `def function_name(...):\n    body...`. The assembly step can handle placing it correctly.
8.  If the task is impossible or the description is too ambiguous to implement, return only the comment: `# IMPLEMENTATION_ERROR: Ambiguous instruction or impossible task.`

Python code for `{component_name}`:
"""

class CodeService:
    def __init__(self, llm_provider: Optional[Any] = None, self_modification_service: Optional[Any] = None):
        self.llm_provider = llm_provider
        self.self_modification_service = self_modification_service
        logger.info("CodeService initialized.")
        if is_debug_mode():
            print("[DEBUG] CodeService initialized with llm_provider:", llm_provider, "self_modification_service:", self_modification_service)
        if not self.llm_provider: # pragma: no cover
            logger.warning("CodeService initialized without an LLM provider. Code generation capabilities will be limited.")
        if not self.self_modification_service: # pragma: no cover
            logger.warning("CodeService initialized without a self-modification service. File operations will be limited.")

    async def generate_code(
        self,
        context: str,
        prompt_or_description: str, # For "NEW_TOOL", this is tool desc. For "SCAFFOLD", this is the code to test.
        language: str = "python",
        target_path: Optional[str] = None,
        llm_config: Optional[Dict[str, Any]] = None,
        additional_context: Optional[Dict[str, Any]] = None # Use for module_name_hint
    ) -> Dict[str, Any]:
        logger.info(f"CodeService.generate_code called with context='{context}', description='{prompt_or_description[:50]}...'")

        if not self.llm_provider: # Check if provider is configured
            logger.error("LLM provider not configured for CodeService.") # pragma: no cover
            return {"status": "ERROR_LLM_PROVIDER_MISSING", "code_string": None, "metadata": None, "logs": ["LLM provider not configured."], "error": "LLM provider missing."}

        if language != "python": # pragma: no cover
            return {
                "status": "ERROR_UNSUPPORTED_LANGUAGE", "code_string": None, "metadata": None,
                "logs": [f"Language '{language}' not supported for {context} yet."],
                "error": "Unsupported language."
            }

        # Default LLM parameters
        model_to_use = get_model_for_task("code_generation") # Can be specialized later
        temperature = 0.3 # Scaffolds should be somewhat deterministic
        max_tokens_to_use = 2048 # Allow for larger scaffolds

        if llm_config: # pragma: no cover
            model_to_use = llm_config.get("model_name", model_to_use)
            temperature = llm_config.get("temperature", temperature)
            max_tokens_to_use = llm_config.get("max_tokens", max_tokens_to_use)

        if context == "NEW_TOOL":
            formatted_prompt = LLM_NEW_TOOL_PROMPT_TEMPLATE.format(description=prompt_or_description)
            logs = [f"Using NEW_TOOL context. Prompt description: {prompt_or_description[:50]}..."]

            raw_llm_output = await self.llm_provider.invoke_ollama_model_async(
                formatted_prompt, model_name=model_to_use, temperature=temperature, max_tokens=max_tokens_to_use
            )

            if not raw_llm_output or not raw_llm_output.strip():
                logger.warning("LLM returned empty response for NEW_TOOL.")
                logs.append("LLM returned empty response.")
                return {"status": "ERROR_LLM_NO_CODE", "code_string": None, "metadata": None, "logs": logs, "error": "LLM provided no code."}
            logs.append(f"Raw LLM output length: {len(raw_llm_output)}")

            parsed_metadata: Optional[Dict[str, str]] = None
            actual_code_str: str = ""
            if raw_llm_output.startswith("# METADATA:"):
                try:
                    lines = raw_llm_output.split('\n', 1)
                    metadata_line = lines[0]
                    metadata_json_str_match = re.search(r"{\s*.*?\s*}", metadata_line)
                    if metadata_json_str_match:
                        metadata_json_str = metadata_json_str_match.group(0)
                        parsed_metadata = json.loads(metadata_json_str)
                        logs.append(f"Successfully parsed metadata: {parsed_metadata}")
                        actual_code_str = lines[1] if len(lines) > 1 else ""
                    else: # pragma: no cover
                        logs.append("Could not find JSON object in metadata line.")
                        actual_code_str = raw_llm_output
                except Exception as e: # pragma: no cover
                    logger.warning(f"Failed to parse metadata JSON for NEW_TOOL: {e}")
                    logs.append(f"Error parsing metadata: {e}. Treating rest as code.")
                    actual_code_str = raw_llm_output.lstrip("# METADATA:") if raw_llm_output.startswith("# METADATA:") else raw_llm_output
            else:
                logs.append("LLM output for NEW_TOOL did not start with '# METADATA:'.")
                actual_code_str = raw_llm_output

            cleaned_code = re.sub(r"^\s*```python\s*\n?", "", actual_code_str, flags=re.IGNORECASE | re.MULTILINE)
            cleaned_code = re.sub(r"\n?\s*```\s*$", "", cleaned_code, flags=re.IGNORECASE | re.MULTILINE).strip()
            cleaned_code = cleaned_code.replace("\\n", "\n")


            if not cleaned_code: # pragma: no cover
                logs.append("Extracted code is empty after cleaning for NEW_TOOL.")
                return {"status": "ERROR_CODE_EMPTY_POST_METADATA" if parsed_metadata else "ERROR_LLM_NO_CODE", "code_string": None, "metadata": parsed_metadata, "logs": logs, "error": "No actual code block found or code was empty."}
            if not parsed_metadata: # pragma: no cover
                logs.append("Metadata not successfully parsed for NEW_TOOL, which is required.")
                return {"status": "ERROR_METADATA_PARSING", "code_string": cleaned_code, "metadata": None, "logs": logs, "error": "Metadata parsing failed for NEW_TOOL."}

            logs.append(f"Cleaned code length for NEW_TOOL: {len(cleaned_code)}")
            logger.info(f"Successfully generated new tool code and metadata for '{parsed_metadata.get('suggested_tool_name', 'UnknownTool')}'.")

            saved_to_path_val: Optional[str] = None
            final_status_new_tool = "SUCCESS_CODE_GENERATED"
            error_new_tool = None

            if target_path and cleaned_code:
                logs.append(f"Attempting to save generated NEW_TOOL code to {target_path}")
                if write_to_file(target_path, cleaned_code):
                    saved_to_path_val = target_path
                    logs.append(f"Successfully saved NEW_TOOL code to {target_path}")
                else: # pragma: no cover
                    final_status_new_tool = "ERROR_SAVING_CODE"
                    error_new_tool = f"Successfully generated code for NEW_TOOL, but failed to save to {target_path}."
                    logs.append(error_new_tool)
                    logger.error(error_new_tool)

            return {
                "status": final_status_new_tool,
                "code_string": cleaned_code,
                "metadata": parsed_metadata,
                "saved_to_path": saved_to_path_val, # New field
                "logs": logs,
                "error": error_new_tool
            }

        elif context == "GENERATE_UNIT_TEST_SCAFFOLD":
            code_to_test = prompt_or_description # Assume the code is passed in prompt_or_description
            module_name_hint = "your_module_to_test"
            if additional_context and additional_context.get("module_name_hint"): # pragma: no branch
                module_name_hint = additional_context["module_name_hint"]

            formatted_prompt = LLM_UNIT_TEST_SCAFFOLD_PROMPT_TEMPLATE.format(
                code_to_test=code_to_test,
                module_name_hint=module_name_hint
            )
            logs = [f"Using GENERATE_UNIT_TEST_SCAFFOLD context. Module hint: {module_name_hint} Code snippet length: {len(code_to_test)}"]

            raw_llm_output = await self.llm_provider.invoke_ollama_model_async(
                formatted_prompt, model_name=model_to_use, temperature=temperature, max_tokens=max_tokens_to_use
            )

            if not raw_llm_output or not raw_llm_output.strip():
                logger.warning("LLM returned empty response for unit test scaffold generation.")
                logs.append("LLM returned empty response.")
                return {"status": "ERROR_LLM_NO_CODE", "code_string": None, "metadata": None, "logs": logs, "error": "LLM provided no code."}
            logs.append(f"Raw LLM output length for scaffold: {len(raw_llm_output)}")

            # For scaffolds, we expect the entire output to be the code after cleaning. No metadata line.
            cleaned_scaffold = re.sub(r"^\s*```python\s*\n?", "", raw_llm_output, flags=re.IGNORECASE | re.MULTILINE)
            cleaned_scaffold = re.sub(r"\n?\s*```\s*$", "", cleaned_scaffold, flags=re.IGNORECASE | re.MULTILINE).strip()
            cleaned_scaffold = cleaned_scaffold.replace("\\n", "\n")


            if not cleaned_scaffold: # pragma: no cover
                logs.append("Extracted scaffold is empty after cleaning.")
                return {"status": "ERROR_LLM_NO_CODE", "code_string": None, "metadata": None, "logs": logs, "error": "LLM output was empty after cleaning for scaffold."}

            logs.append(f"Cleaned scaffold length: {len(cleaned_scaffold)}")
            logger.info("Successfully generated unit test scaffold.")

            saved_to_path_scaffold: Optional[str] = None
            final_status_scaffold = "SUCCESS_CODE_GENERATED"
            error_scaffold = None

            if target_path and cleaned_scaffold:
                logs.append(f"Attempting to save generated unit test scaffold to {target_path}")
                if write_to_file(target_path, cleaned_scaffold):
                    saved_to_path_scaffold = target_path
                    logs.append(f"Successfully saved unit test scaffold to {target_path}")
                else: # pragma: no cover
                    final_status_scaffold = "ERROR_SAVING_CODE"
                    error_scaffold = f"Successfully generated unit test scaffold, but failed to save to {target_path}."
                    logs.append(error_scaffold)
                    logger.error(error_scaffold)

            return {
                "status": final_status_scaffold,
                "code_string": cleaned_scaffold,
                "metadata": None, # No specific metadata for scaffolds
                "saved_to_path": saved_to_path_scaffold, # New field
                "logs": logs,
                "error": error_scaffold
            }

        elif context == "EXPERIMENTAL_HIERARCHICAL_OUTLINE":
            high_level_description = prompt_or_description
            logs = [f"Using EXPERIMENTAL_HIERARCHICAL_OUTLINE context. Description: {high_level_description[:50]}..."]

            formatted_prompt = LLM_HIERARCHICAL_OUTLINE_PROMPT_TEMPLATE.format(
                high_level_description=high_level_description
            )

            raw_llm_output = await self.llm_provider.invoke_ollama_model_async(
                formatted_prompt, model_name=model_to_use, temperature=temperature, max_tokens=max_tokens_to_use # Uses model_to_use, temp, max_tokens defined earlier in the method
            )

            if not raw_llm_output or not raw_llm_output.strip():
                logger.warning("LLM returned empty response for hierarchical outline generation.")
                logs.append("LLM returned empty response for outline.")
                return {"status": "ERROR_LLM_NO_OUTLINE", "outline_str": raw_llm_output, "parsed_outline": None, "code_string": None, "metadata": None, "logs": logs, "error": "LLM provided no outline."}

            logs.append(f"Raw LLM outline output length: {len(raw_llm_output)}")

            parsed_outline: Optional[Dict[str, Any]] = None
            error_message: Optional[str] = None
            final_status = "SUCCESS_OUTLINE_GENERATED"

            try:
                # LLM might wrap JSON in backticks, try to strip them
                cleaned_json_str = raw_llm_output.strip()
                if cleaned_json_str.startswith("```json"): # pragma: no cover
                    cleaned_json_str = cleaned_json_str[len("```json"):].strip()
                if cleaned_json_str.endswith("```"): # pragma: no cover
                    cleaned_json_str = cleaned_json_str[:-len("```")].strip()

                parsed_outline = json.loads(cleaned_json_str)
                logs.append("Successfully parsed JSON outline from LLM response.")
            except json.JSONDecodeError as e: # pragma: no cover
                logger.warning(f"Failed to parse JSON outline: {e}. Raw output: {raw_llm_output[:200]}...")
                logs.append(f"JSONDecodeError parsing outline: {e}")
                error_message = f"Failed to parse LLM JSON outline: {e}"
                final_status = "ERROR_OUTLINE_PARSING"

            return {"status": final_status, "outline_str": raw_llm_output, "parsed_outline": parsed_outline, "code_string": None, "metadata": None, "logs": logs, "error": error_message}

        elif context == "EXPERIMENTAL_HIERARCHICAL_FULL_TOOL":
            high_level_description = prompt_or_description
            logs = [f"Using EXPERIMENTAL_HIERARCHICAL_FULL_TOOL context. Description: {high_level_description[:50]}..."]

            # Step 1 & 2: Generate Outline and then Component Details
            orchestration_result = await self.generate_code(
                context="EXPERIMENTAL_HIERARCHICAL_FULL_TOOL", # Call existing orchestration
                prompt_or_description=high_level_description,
                language=language,
                llm_config=llm_config,
                additional_context=additional_context
            )

            logs.extend(orchestration_result.get("logs", []))
            parsed_outline = orchestration_result.get("parsed_outline")
            component_details = orchestration_result.get("component_details", {}) # Ensure Dict[str, Optional[str]] type

            if orchestration_result["status"] not in ["SUCCESS_HIERARCHICAL_DETAILS_GENERATED", "PARTIAL_HIERARCHICAL_DETAILS_GENERATED"]:
                logs.append("Outline or detail generation failed, cannot proceed to assembly.") # pragma: no cover
                return { # Propagate error from previous stage
                    "status": orchestration_result["status"],
                    "parsed_outline": parsed_outline, "component_details": component_details,
                    "code_string": None, "metadata": None,
                    "logs": logs, "error": orchestration_result.get("error", "Outline or detail generation failed.")
                }

            if not parsed_outline or component_details is None: # Check if None explicitly for component_details
                logs.append("Missing outline or component details after successful generation steps. Cannot assemble.") # pragma: no cover
                return {
                    "status": "ERROR_ASSEMBLY_MISSING_DATA",
                    "parsed_outline": parsed_outline, "component_details": component_details,
                    "code_string": None, "metadata": None,
                    "logs": logs, "error": "Internal error: Missing data for assembly."
                }

            # Step 3: Assemble Components
            logs.append("Attempting to assemble generated components.")
            try:
                assembled_code = self._assemble_components(parsed_outline, component_details)
                logs.append(f"Assembly successful. Assembled code length: {len(assembled_code)}")

                final_status = "SUCCESS_HIERARCHICAL_ASSEMBLED"
                # If some details failed but assembly proceeded with placeholders:
                if orchestration_result["status"] == "PARTIAL_HIERARCHICAL_DETAILS_GENERATED": # pragma: no cover
                    final_status = "PARTIAL_HIERARCHICAL_ASSEMBLED"

                saved_to_path_hierarchical: Optional[str] = None
                current_error_hierarchical = orchestration_result.get("error") # Preserve error from detail gen if any

                if target_path and assembled_code:
                    logs.append(f"Attempting to save assembled hierarchical code to {target_path}")
                    if write_to_file(target_path, assembled_code):
                        saved_to_path_hierarchical = target_path
                        logs.append(f"Successfully saved assembled code to {target_path}")
                    else: # pragma: no cover
                        # If saving fails, it's a more significant error for this context
                        final_status = "ERROR_SAVING_ASSEMBLED_CODE"
                        current_error_hierarchical = f"Successfully assembled code, but failed to save to {target_path}."
                        logs.append(current_error_hierarchical)
                        logger.error(current_error_hierarchical)

                return {
                    "status": final_status, # This was set based on assembly and detail gen status
                    "parsed_outline": parsed_outline,
                    "component_details": component_details,
                    "code_string": assembled_code,
                    "metadata": None,
                    "saved_to_path": saved_to_path_hierarchical, # New field
                    "logs": logs,
                    "error": current_error_hierarchical
                }
            except Exception as e_assemble: # pragma: no cover
                logger.error(f"Error during code assembly: {e_assemble}", exc_info=True)
                logs.append(f"Exception during assembly: {e_assemble}")
                return {
                    "status": "ERROR_ASSEMBLY_FAILED",
                    "parsed_outline": parsed_outline, "component_details": component_details,
                    "code_string": None, "metadata": None,
                    "logs": logs, "error": f"Assembly failed: {e_assemble}"
                }

        else: # pragma: no cover
            return {
                "status": "ERROR_UNSUPPORTED_CONTEXT", "code_string": None, "metadata": None,
                "logs": [f"generate_code for context '{context}' not supported yet."],
                "error": "Unsupported context for generate_code."
            }

    async def modify_code(
        self, context: str, modification_instruction: str,
        existing_code: Optional[str] = None, language: str = "python",
        module_path: Optional[str] = None, function_name: Optional[str] = None,
        llm_config: Optional[Dict[str, Any]] = None,
        additional_context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        logger.info(f"CodeService.modify_code called for context='{context}', module='{module_path}', function='{function_name}'")
        logs = [f"modify_code invoked with context='{context}', module='{module_path}', function='{function_name}'."]

        if context != "SELF_FIX_TOOL":
            logs.append(f"Context '{context}' not supported for modify_code.")
            return {"status": "ERROR_UNSUPPORTED_CONTEXT", "modified_code_string": None, "logs": logs, "error": "Unsupported context"}

        if not module_path or not function_name:
            logs.append("Missing module_path or function_name for SELF_FIX_TOOL.")
            return {"status": "ERROR_MISSING_DETAILS", "modified_code_string": None, "logs": logs, "error": "Missing details for self-fix."}

        if language != "python":
            logs.append(f"Language '{language}' not supported for SELF_FIX_TOOL.")
            return {"status": "ERROR_UNSUPPORTED_LANGUAGE", "modified_code_string": None, "logs": logs, "error": "Unsupported language."}

        actual_existing_code = existing_code
        if actual_existing_code is None:
            logger.info(f"No existing_code provided for {module_path}.{function_name}, attempting to fetch.")
            logs.append(f"Fetching original code for {module_path}.{function_name}.")
            if not self.self_modification_service:
                logger.error("Self modification service not configured for modify_code.")
                logs.append("Self modification service not configured.")
                return {"status": "ERROR_SELF_MOD_SERVICE_MISSING", "modified_code_string": None, "logs": logs, "error": "Self modification service not configured."}
            actual_existing_code = self.self_modification_service.get_function_source_code(module_path, function_name)

        if actual_existing_code is None: # Check again after fetch attempt
            logger.warning(f"Could not retrieve original code for {module_path}.{function_name}.")
            logs.append(f"Failed to retrieve original source for {module_path}.{function_name}.")
            return {"status": "ERROR_NO_ORIGINAL_CODE", "modified_code_string": None, "logs": logs, "error": "Cannot get original code."}

        prompt = LLM_CODE_FIX_PROMPT_TEMPLATE.format(
            module_path=module_path, function_name=function_name,
            problem_description=modification_instruction, original_code=actual_existing_code
        )

        current_llm_config = llm_config if llm_config else {}
        code_gen_model = current_llm_config.get("model_name", get_model_for_task("code_generation"))
        temperature = current_llm_config.get("temperature", 0.3)
        max_tokens = current_llm_config.get("max_tokens", 1024)

        logger.info(f"CodeService: Sending code fix prompt to LLM (model: {code_gen_model}).")
        logs.append(f"Sending code fix prompt to LLM (model: {code_gen_model}).")

        if not self.llm_provider:
            logger.error("LLM provider not configured for modify_code.")
            logs.append("LLM provider not configured.")
            return {
                "status": "ERROR_LLM_PROVIDER_MISSING", "modified_code_string": None,
                "logs": logs, "error": "LLM provider not configured."
            }

        llm_response = await self.llm_provider.invoke_ollama_model_async(
            prompt, model_name=code_gen_model, temperature=temperature, max_tokens=max_tokens
        )

        if not llm_response or "// NO_CODE_SUGGESTION_POSSIBLE" in llm_response or len(llm_response.strip()) < 10:
            logger.warning(f"LLM did not provide a usable code suggestion. Response: {llm_response}")
            logs.append(f"LLM failed to provide suggestion. Output: {llm_response[:100] if llm_response else 'None'}")
            return {"status": "ERROR_LLM_NO_SUGGESTION", "modified_code_string": None, "logs": logs, "error": "LLM provided no usable suggestion."}

        cleaned_llm_code = llm_response.strip()
        if cleaned_llm_code.startswith("```python"): # pragma: no cover
            cleaned_llm_code = cleaned_llm_code[len("```python"):].strip()
        if cleaned_llm_code.endswith("```"): # pragma: no cover
            cleaned_llm_code = cleaned_llm_code[:-len("```")].strip()

        logs.append(f"LLM successfully generated code suggestion. Length: {len(cleaned_llm_code)}")
        logger.info(f"LLM generated code suggestion for {function_name}. Length: {len(cleaned_llm_code)}")
        return {
            "status": "SUCCESS_CODE_GENERATED",
            "modified_code_string": cleaned_llm_code,
            "logs": logs,
            "error": None
        }

    # --- Conceptual Placeholders for Hierarchical Generation ---

    async def _generate_hierarchical_outline(
        self,
        high_level_description: str,
        context: str, # e.g., HIERARCHICAL_GEN_MODULE
        llm_config: Optional[Dict[str, Any]]
    ) -> Optional[Dict[str, Any]]: # Returns the parsed outline structure (e.g., dict from JSON)
        """
        (Conceptual) Step 1 of Hierarchical Generation: Generate the code outline.
        This would use a specific "outline generation" prompt.
        """
        logger.info(f"Conceptual: _generate_hierarchical_outline for '{high_level_description[:50]}...'")
        # 1. Select/format outline generation prompt.
        # 2. Call self.llm_provider.invoke_ollama_model_async(...)
        # 3. Parse the structured outline (e.g., JSON) from LLM response.
        #    Handle parsing errors.
        # Example:
        # if successful_parse:
        #     return parsed_outline_dict
        # else:
        #     return None
        return {"status": "conceptual_outline_placeholder"} # Placeholder

    async def _generate_detail_for_component(
        self,
        component_definition: Dict[str, Any], # e.g., a function/method dict from the outline
        full_outline: Dict[str, Any], # The complete outline for context
        # context: str, # Original context like "HIERARCHICAL_GEN_MODULE", if needed for LLM params
        llm_config: Optional[Dict[str, Any]] # Specific LLM config for this component
    ) -> Optional[str]: # Returns the generated code string for the component
        """
        Step 2 of Hierarchical Generation: Generate code for a single component.
        Uses LLM_COMPONENT_DETAIL_PROMPT_TEMPLATE.
        """
        component_type = component_definition.get('type', 'unknown_type')
        component_name = component_definition.get('name', 'UnnamedComponent')
        component_signature = component_definition.get('signature', '')
        component_description = component_definition.get('description', '')
        component_body_placeholder = component_definition.get('body_placeholder', '')

        module_imports_list = full_outline.get('imports', [])
        module_imports_str = "\n".join([f"import {imp}" for imp in module_imports_list]) if module_imports_list else "# No specific module-level imports listed in outline."

        # Construct overall_context_summary (can be improved)
        # For now, just use the overall module/class description if available.
        # A more advanced version would serialize relevant parts of full_outline.
        overall_context_summary = full_outline.get('description', 'No overall description provided in outline.')
        if component_type == "method" and full_outline.get("components"): # Try to find class context
            for comp in full_outline["components"]:
                if comp.get("type") == "class" and any(meth.get("name") == component_name for meth in comp.get("methods",[])): # pragma: no branch
                    class_attrs = ", ".join([f"{attr.get('name')}: {attr.get('type')}" for attr in comp.get('attributes',[])])
                    overall_context_summary = (
                        f"Within class '{comp.get('name', 'UnknownClass')}' with attributes ({class_attrs}). "
                        f"Overall class description: {comp.get('description', '')}"
                    )
                    break

        logger.info(f"CodeService: Generating detail for component '{component_name}' (type: {component_type}).")

        if not self.llm_provider: # pragma: no cover
            logger.error("LLM provider not configured for CodeService, cannot generate component detail.")
            return None

        prompt = LLM_COMPONENT_DETAIL_PROMPT_TEMPLATE.format(
            overall_context_summary=overall_context_summary,
            component_type=component_type,
            component_name=component_name,
            component_signature=component_signature,
            component_description=component_description,
            component_body_placeholder=component_body_placeholder,
            module_imports=module_imports_str
        )

        # LLM parameters
        # Use defaults, but allow override from llm_config if provided at higher level or per component
        model_to_use = get_model_for_task("code_generation") # Or "code_detail_generation"
        temperature = 0.2 # More deterministic for filling in details
        max_tokens_to_use = 1024 # Adjust as needed for typical component size

        if llm_config: # pragma: no cover
            model_to_use = llm_config.get("model_name", model_to_use)
            temperature = llm_config.get("temperature", temperature)
            max_tokens_to_use = llm_config.get("max_tokens", max_tokens_to_use)

        raw_llm_output = await self.llm_provider.invoke_ollama_model_async(
            prompt, model_name=model_to_use, temperature=temperature, max_tokens=max_tokens_to_use
        )

        if not raw_llm_output or \
           "# IMPLEMENTATION_ERROR:" in raw_llm_output or \
           len(raw_llm_output.strip()) < 5: # Arbitrary check for minimal viable code
            logger.warning(f"LLM did not provide a usable code snippet for component '{component_name}'. Output: {raw_llm_output}")
            return None

        # Basic cleaning: remove potential markdown backticks
        cleaned_code_snippet = raw_llm_output.strip()
        if cleaned_code_snippet.startswith("```python"): # pragma: no cover
            cleaned_code_snippet = cleaned_code_snippet[len("```python"):].strip()
        if cleaned_code_snippet.endswith("```"): # pragma: no cover
            cleaned_code_snippet = cleaned_code_snippet[:-len("```")].strip()

        # Replace escaped newlines
        cleaned_code_snippet = cleaned_code_snippet.replace("\\n", "\n")

        logger.info(f"Successfully generated code snippet for component '{component_name}'. Length: {len(cleaned_code_snippet)}")
        return cleaned_code_snippet

    def _assemble_components(
        self,
        outline: Dict[str, Any],
        component_details: Dict[str, Optional[str]] # maps component name/id to its generated code string
    ) -> str:
        """
        Step 3 of Hierarchical Generation: Assemble the final code from an outline
        and generated details for each component.
        """
        logger.info("CodeService: Assembling components into final code string.")

        if not outline: # pragma: no cover
            logger.warning("_assemble_components called with no outline.")
            return "# Error: Outline was not provided for assembly."

        code_parts = []

        # 1. Add module-level docstring if present in outline
        module_docstring = outline.get("module_docstring") # Assuming this key might exist
        if module_docstring: # pragma: no branch
            code_parts.append(f'"""{module_docstring}"""')
            code_parts.append("\n\n") # Ensures two blank lines after module docstring

        # 2. Add imports
        imports = outline.get("imports", [])
        if imports: # pragma: no branch
            if code_parts and not code_parts[-1].endswith("\n\n"): # If module docstring was there and ended with \n\n
                if code_parts[-1] == "\n": code_parts[-1] = "\n\n" # Adjust if only one \n
                else: code_parts.append("\n\n") # Should not happen if docstring added \n\n
            elif not code_parts: # No module docstring
                 pass # Imports will be first, no preceding newlines needed from here

            for imp in imports:
                code_parts.append(f"import {imp}")
            code_parts.append("\n\n") # Two blank lines after all imports

        # Remove potentially redundant starting newlines if nothing was added before components
        if not code_parts:
            pass
        elif "".join(code_parts).isspace(): # If only newlines were added
            code_parts = [] # Reset if only whitespace/newlines, let components add their own needed space

        # 3. Add components (functions and classes)
        components = outline.get("components", [])
        for i, component_def in enumerate(components):
            component_type = component_def.get("type")
            component_name = component_def.get("name")

            if not component_name: # pragma: no cover
                logger.warning(f"Skipping component with no name: {component_def}")
                code_parts.append(f"# SKIPPED COMPONENT: No name provided in outline for component {i+1}")
                continue

            if component_type == "function":
                func_code = component_details.get(component_name)
                if func_code:
                    code_parts.append(func_code)
                else: # pragma: no cover
                    # Add placeholder if code for this function is missing
                    signature = component_def.get("signature", "()")
                    desc = component_def.get("description", "No description.")
                    placeholder_body = component_def.get("body_placeholder", "pass # TODO: Implement")
                    code_parts.append(f"# Function '{component_name}' was planned but not generated.")
                    code_parts.append(f"def {component_name}{signature}:")
                    func_docstring_lines = [
                        f"    \"\"\"Placeholder for: {desc.splitlines()[0] if desc else ''}"]
                    if desc and '\n' in desc: # pragma: no cover
                        for line in desc.splitlines()[1:]:
                            func_docstring_lines.append(f"    {line.strip()}")
                    func_docstring_lines.append(f"    Original placeholder: {placeholder_body.splitlines()[0] if placeholder_body else ''}")
                    if placeholder_body and '\n' in placeholder_body: # pragma: no cover
                        for line in placeholder_body.splitlines()[1:]:
                            func_docstring_lines.append(f"    {line.strip()}")
                    func_docstring_lines.append("    \"\"\"")
                    code_parts.extend(func_docstring_lines)
                    code_parts.append(f"    pass")
                code_parts.append("\n\n") # Ensure two blank lines after a function

            elif component_type == "class":
                class_name = component_name
                # Basic class definition line (bases, keywords not handled in this simple outline version)
                code_parts.append(f"class {class_name}:")

                class_docstring = component_def.get("description") # Class description as docstring
                if class_docstring: # pragma: no branch
                    # Indent docstring
                    indented_docstring = f'    """{class_docstring}"""'
                    code_parts.append(indented_docstring)
                    code_parts.append("") # Blank line after class docstring if present

                # Class attributes (as comments or type hints if possible, simplified here)
                attributes = component_def.get("attributes", [])
                if attributes: # pragma: no branch
                    # For now, just list them as comments within the class, or as type hints if __init__ not detailed
                    # A more robust way would be to ensure __init__ handles them or use dataclass/attrs.
                    # This simple version will just put them as comments for now if no __init__ method handles them.
                    # If an __init__ method is generated by LLM, it should handle attribute initialization.
                    has_init_method = any(m.get("name") == "__init__" for m in component_def.get("methods", []))
                    if not has_init_method and attributes: # pragma: no cover
                        code_parts.append("    # Defined attributes (from outline):")
                        for attr in attributes:
                            attr_name = attr.get('name', 'unknown_attr')
                            attr_type = attr.get('type', '')
                            attr_desc = attr.get('description', '')
                            type_hint_str = f": {attr_type}" if attr_type else ""
                            comment_str = f" # {attr_desc}" if attr_desc else ""
                            code_parts.append(f"    {attr_name}{type_hint_str}{comment_str}")
                        code_parts.append("")


                methods = component_def.get("methods", [])
                if not methods and not attributes and not class_docstring : # If class is empty
                     code_parts.append("    pass") # Add pass to empty class

                for method_def in methods:
                    method_name = method_def.get("name")
                    method_key = f"{class_name}.{method_name}"
                    method_code = component_details.get(method_key)

                    if method_code:
                        # Indent the whole method code block
                        indented_method_code = "\n".join([f"    {line}" for line in method_code.splitlines()])
                        code_parts.append(indented_method_code)
                    else: # pragma: no cover
                        # Add placeholder if code for this method is missing
                        signature = method_def.get("signature", "(self)")
                        desc = method_def.get("description", "No description.")
                        placeholder_body = method_def.get("body_placeholder", "pass # TODO: Implement")
                        code_parts.append(f"    # Method '{method_name}' was planned but not generated.")
                        code_parts.append(f"    def {method_name}{signature}:")
                    docstring_lines = [ # Start with 8 spaces for method docstring
                        f"        \"\"\"Placeholder for: {desc.splitlines()[0] if desc else ''}"]
                    if desc and '\n' in desc: # pragma: no cover
                        for line in desc.splitlines()[1:]:
                            docstring_lines.append(f"        {line.strip()}")
                    docstring_lines.append(f"        Original placeholder: {placeholder_body.splitlines()[0] if placeholder_body else ''}")
                    if placeholder_body and '\n' in placeholder_body: # pragma: no cover
                        for line in placeholder_body.splitlines()[1:]:
                            docstring_lines.append(f"        {line.strip()}")
                    docstring_lines.append("        \"\"\"") # End of docstring
                    code_parts.extend(docstring_lines)
                    code_parts.append(f"        pass") # Pass statement for the method
                    # Add a single newline after each method, _assemble_components will handle overall spacing
                    code_parts.append("")

                # Add two newlines after the class block
                if code_parts and code_parts[-1] == "": # If last part was a blank line from a method
                    code_parts[-1] = "\n\n" # Make it two newlines
                else: # Class might be empty or end without a blank line
                    code_parts.append("\n\n")


            else: # pragma: no cover
                logger.warning(f"Unsupported component type '{component_type}' in outline for assembly.")
                code_parts.append(f"# UNSUPPORTED COMPONENT TYPE: {component_type} - {component_name}")

        # 4. Add main execution block if present
        main_block = outline.get("main_execution_block")
        if main_block: # pragma: no branch
            code_parts.append("") # Ensure a blank line before it
            code_parts.append(main_block)
            code_parts.append("")

        # Join all parts with double newlines between top-level blocks for PEP8,
        # but single newlines were mostly added already.
        # A simple join and then cleanup might be easier.
        final_code = "\n".join(code_parts)

        # Basic cleanup for excessive newlines (e.g., >2 consecutive newlines)
        final_code = re.sub(r"\n{3,}", "\n\n", final_code)

        logger.info(f"Code assembly complete. Total length: {len(final_code)}")
        return final_code.strip()

    # --- End Conceptual Placeholders ---

if __name__ == '__main__': # pragma: no cover
    import os
    import asyncio
    import tempfile # For __main__ test outputs
    import shutil   # For __main__ test outputs

    # Mock providers for the illustrative test main()
    # These would need to be actual objects with the expected methods for the test to fully run.
    class MockLLMProvider:
        async def invoke_ollama_model_async(self, prompt, model_name, temperature, max_tokens):
            logger.info(f"MockLLMProvider.invoke_ollama_model_async called with model: {model_name}")
            if "generate_code" in prompt: # Simple check for generate_code vs modify_code
                 return '# METADATA: {"suggested_function_name": "mock_sum_function", "suggested_tool_name": "mockSumTool", "suggested_description": "A mock sum function."}\ndef mock_sum_function(a: int, b: int) -> int:\n    """Adds two integers."""\n    return a + b'
            elif "function_to_be_fixed_by_main_svc" in prompt :
                 return "def function_to_be_fixed_by_main_svc(val: int) -> int:\n    return val + 10 # Fixed!\n"
            return "// NO_CODE_SUGGESTION_POSSIBLE"

    class MockSelfModService:
        def get_function_source_code(self, module_path, function_name):
            logger.info(f"MockSelfModService.get_function_source_code called for {module_path}.{function_name}")
            if module_path == "ai_assistant.custom_tools.dummy_tool_main_test_svc" and \
               function_name == "function_to_be_fixed_by_main_svc":
                return "def function_to_be_fixed_by_main_svc(val: int) -> int:\n    return val * 10 # Should be val + 10\n"
            return None

    async def main_illustrative_test():
        # Instantiate with mock objects that have the required methods
        mock_llm_provider_instance = MockLLMProvider()
        mock_self_mod_service_instance = MockSelfModService()

        code_service = CodeService(
            llm_provider=mock_llm_provider_instance,
            self_modification_service=mock_self_mod_service_instance
        )

        # Setup test output directory for __main__
        test_output_dir = tempfile.mkdtemp(prefix="codeservice_test_outputs_")
        print(f"Test outputs will be saved in: {test_output_dir}")


        print("\n--- Testing generate_code (NEW_TOOL context) ---")
        new_tool_path = os.path.join(test_output_dir, "newly_generated_tool.py")
        gen_result = await code_service.generate_code(
            context="NEW_TOOL",
            prompt_or_description="A Python function that takes two integers and returns their sum.",
            # llm_config={"model_name": get_model_for_task("code_generation")} # Using default from class for this test
            target_path=new_tool_path
        )
        print(f"Generate Code Result: Status: {gen_result.get('status')}, Saved to: {gen_result.get('saved_to_path')}")
        if gen_result.get('code_string'):
            print(f"Generated Code (first 150 chars): {gen_result['code_string'][:150]}...")
        if gen_result.get('metadata'):
            print(f"Generated Metadata: {gen_result['metadata']}")
        if gen_result.get('error'): # pragma: no cover
            print(f"Error: {gen_result['error']}")
        # print(f"Logs: {gen_result.get('logs')}")


        # Test modify_code
        # For this test, we rely on MockSelfModService to provide the "original" code
        test_module_path_for_main = "ai_assistant.custom_tools.dummy_tool_main_test_svc"

        print("\n--- Testing modify_code (SELF_FIX_TOOL context) ---")
        # We'll simulate that the file doesn't exist locally, so CodeService relies on MockSelfModService
        mod_result = await code_service.modify_code(
            context="SELF_FIX_TOOL",
            existing_code=None, # Force fetching via self.self_modification_service
            modification_instruction="The function should add 10, not multiply by 10.",
            module_path=test_module_path_for_main, # Used by MockSelfModService
            function_name="function_to_be_fixed_by_main_svc" # Used by MockSelfModService
        )
        print(f"Modify Code Result: {mod_result.get('status')}")
        if mod_result.get('modified_code_string'):
            print(f"Modified Code (first 150 chars): {mod_result['modified_code_string'][:150]}...")
        if mod_result.get('error'):
            print(f"Error: {mod_result['error']}")
        print(f"Logs: {mod_result.get('logs')}")
        # No need to create/delete dummy files as mocks handle the code provision

        print("\n--- Testing generate_code (GENERATE_UNIT_TEST_SCAFFOLD) ---")
        sample_code_to_test = "def my_function(x, y):\n    return x + y\n\nclass MyClass:\n    def do_stuff(self):\n        pass"
        test_scaffold_path = os.path.join(test_output_dir, "test_my_module_scaffold.py")
        scaffold_result = await code_service.generate_code(
            context="GENERATE_UNIT_TEST_SCAFFOLD",
            prompt_or_description=sample_code_to_test, # This is the code_to_test
            additional_context={"module_name_hint": "my_module"},
            target_path=test_scaffold_path
        )
        print(f"Generate Unit Test Scaffold Result: Status: {scaffold_result.get('status')}, Saved to: {scaffold_result.get('saved_to_path')}")
        if scaffold_result.get("code_string"):
            # The mock provider for main() doesn't actually return a scaffold, so this might be empty.
            # This test is more about plumbing the context through.
            print(f"Generated Scaffold (first 200 chars):\n{scaffold_result['code_string'][:200]}...")
        if scaffold_result.get("error"): # pragma: no cover
            print(f"Error: {scaffold_result.get('error')}")
        # print(f"Logs: {scaffold_result.get('logs')}")


        print("\n--- Testing generate_code (EXPERIMENTAL_HIERARCHICAL_OUTLINE) ---")
        outline_desc = "A Python CLI tool to manage a simple to-do list stored in a JSON file. It needs add, remove, and list functions."
        # Mock the LLM provider for this specific call if you want to control output in __main__
        # For now, it will make a real call if provider is configured.
        # To test parsing, you might need to mock self.llm_provider.invoke_ollama_model_async here
        # for this specific test call if not running in a full unit test environment.

        # Example of how one might mock for main test, if needed for controlled output:
        # original_invoke = mock_llm_provider_instance.invoke_ollama_model_async
        # async def mock_invoke_for_outline(*args, **kwargs):
        #     if "JSON Outline:" in args[0]: # Check if it's the outline prompt
        #         return json.dumps({"module_name": "todo.py", "imports": ["json"], "components": []})
        #     return await original_invoke(*args, **kwargs) # Call original for other tests
        # mock_llm_provider_instance.invoke_ollama_model_async = mock_invoke_for_outline

        outline_result = await code_service.generate_code(
            context="EXPERIMENTAL_HIERARCHICAL_OUTLINE",
            prompt_or_description=outline_desc
        )
        print(f"Generate Outline Result: Status: {outline_result.get('status')}")
        if outline_result.get("parsed_outline"):
            print(f"Parsed Outline (first level keys): {list(outline_result['parsed_outline'].keys())}")
        elif outline_result.get("outline_str"): # pragma: no cover
            print(f"Raw Outline Str (first 200 chars): {outline_result['outline_str'][:200]}...")
        if outline_result.get("error"): # pragma: no cover
            print(f"Error: {outline_result.get('error')}")
        # print(f"Logs: {outline_result.get('logs')}")

        # mock_llm_provider_instance.invoke_ollama_model_async = original_invoke # Restore if mocked

        print("\n--- Testing generate_code (EXPERIMENTAL_HIERARCHICAL_FULL_TOOL) ---")
        full_tool_desc = "A Python CLI tool to manage a simple to-do list stored in a JSON file. It needs add, remove, and list functions using argparse."

        # To test this without full LLM calls for outline AND details,
        # we'd need to mock _generate_detail_for_component or have the LLM provider return
        # very predictable outline and then predictable details.
        # For __main__ test, this will make actual LLM calls.

        full_tool_result = await code_service.generate_code(
            context="EXPERIMENTAL_HIERARCHICAL_FULL_TOOL",
            prompt_or_description=full_tool_desc
            # No target_path for this context as it doesn't produce a single final code string directly
        )
        print(f"Generate Full Tool (Outline+Details) Result: Status: {full_tool_result.get('status')}")
        if full_tool_result.get("parsed_outline"):
            print(f"Parsed Outline (keys): {list(full_tool_result['parsed_outline'].keys())}")
        if full_tool_result.get("component_details"):
            print("Component Details Generated:")
            for name, code_prev_obj in full_tool_result["component_details"].items():
                code_prev = str(code_prev_obj)
                print(f"  Component: {name}, Code (first 30 chars): {code_prev[:30].replace(chr(10), ' ')}...")
        if full_tool_result.get("error"): # pragma: no cover
            print(f"Error: {full_tool_result.get('error')}")
        # print(f"Logs: {full_tool_result.get('logs')}")


        print("\n--- Testing generate_code (HIERARCHICAL_GEN_COMPLETE_TOOL) ---")
        complete_tool_desc = "A Python module with a function to add two numbers and a class MyMath with a method to multiply them."
        complete_tool_path = os.path.join(test_output_dir, "hierarchically_generated_tool.py")

        complete_tool_result = await code_service.generate_code(
            context="HIERARCHICAL_GEN_COMPLETE_TOOL",
            prompt_or_description=complete_tool_desc,
            target_path=complete_tool_path
        )
        print(f"Generate Complete Tool Result: Status: {complete_tool_result.get('status')}, Saved to: {complete_tool_result.get('saved_to_path')}")
        if complete_tool_result.get("code_string"):
            print(f"Assembled Code (first 300 chars):\n{complete_tool_result['code_string'][:300]}...")
        else: # pragma: no cover
            print(f"No final code string generated. Outline: {complete_tool_result.get('parsed_outline') is not None}, Details: {complete_tool_result.get('component_details') is not None}")
        if complete_tool_result.get("error"): # pragma: no cover
            print(f"Error: {complete_tool_result.get('error')}")
        # print(f"Logs: {complete_tool_result.get('logs')}")

        # Cleanup
        try:
            print(f"\nIllustrative test finished. Cleaning up test output directory: {test_output_dir}")
            shutil.rmtree(test_output_dir)
            print(f"Successfully removed {test_output_dir}")
        except Exception as e_cleanup: # pragma: no cover
            print(f"Error cleaning up test directory: {e_cleanup}")


    if os.name == 'nt': # pragma: no cover
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main_illustrative_test())

# ### END FILE: ai_assistant/code_services/service.py ###

# ### START FILE: ai_assistant/code_synthesis/__init__.py ###
# ai_assistant/code_synthesis/__init__.py
from .data_structures import CodeTaskType, CodeTaskStatus, CodeTaskRequest, CodeTaskResult
from .service import CodeSynthesisService

__all__ = [
    "CodeTaskType",
    "CodeTaskStatus",
    "CodeTaskRequest",
    "CodeTaskResult",
    "CodeSynthesisService",
]

# ### END FILE: ai_assistant/code_synthesis/__init__.py ###

# ### START FILE: ai_assistant/code_synthesis/data_structures.py ###
# ai_assistant/code_synthesis/data_structures.py
from enum import Enum, auto
from dataclasses import dataclass, field
from typing import Dict, Any, Optional
import uuid

class CodeTaskType(Enum):
    """
    Defines the different types of code tasks the system can handle.
    Aligns with contexts defined in unified_code_writing_system.md.
    """
    NEW_TOOL_CREATION_LLM = auto()
    EXISTING_TOOL_SELF_FIX_LLM = auto()
    EXISTING_TOOL_SELF_FIX_AST = auto()
    # Add more types as the system evolves

class CodeTaskStatus(Enum):
    """
    Defines the status of a code synthesis task.
    """
    PENDING = auto()
    IN_PROGRESS = auto()
    SUCCESS = auto()
    FAILURE_PRECONDITION = auto()      # e.g., missing required input
    FAILURE_LLM_GENERATION = auto()    # LLM failed to generate usable code
    FAILURE_CODE_APPLICATION = auto()  # e.g., AST modification failed, generated code invalid
    FAILURE_UNSUPPORTED_TASK = auto()
    NEEDS_REVIEW = auto()              # Code generated, but requires manual review
    PARTIAL_SUCCESS = auto()           # Some parts succeeded, others failed
    # Add more statuses as needed

@dataclass
class CodeTaskRequest:
    """
    Represents a request to the CodeSynthesisService.
    """
    task_type: CodeTaskType
    context_data: Dict[str, Any] # Payload specific to the task_type
    request_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    llm_config_overrides: Optional[Dict[str, Any]] = None
    expected_output_format: Optional[str] = None # e.g., "python_function_string"
    # Add other generic fields if needed, e.g., priority, user_id

@dataclass
class CodeTaskResult:
    """
    Represents the result of a code synthesis task from CodeSynthesisService.
    """
    request_id: str # Corresponds to the CodeTaskRequest.request_id
    status: CodeTaskStatus
    generated_code: Optional[str] = None
    modified_code_path: Optional[str] = None # For modifications applied directly to files
    metadata: Optional[Dict[str, Any]] = field(default_factory=dict) # e.g., suggested func name, LLM logs
    error_message: Optional[str] = None
    review_comments: Optional[str] = None # Placeholder for future review integration

# ### END FILE: ai_assistant/code_synthesis/data_structures.py ###

# ### START FILE: ai_assistant/code_synthesis/service.py ###
# ai_assistant/code_synthesis/service.py
from .data_structures import CodeTaskRequest, CodeTaskResult, CodeTaskType, CodeTaskStatus
from typing import Dict, Any, Optional
import re
import os
import json
import sys

from ai_assistant.core import self_modification
from ai_assistant.llm_interface.ollama_client import invoke_ollama_model_async # Already imported
from ai_assistant.config import get_model_for_task
# from ai_assistant.core.reflection import global_reflection_log # Not directly used in service for now


LLM_CODE_FIX_PROMPT_TEMPLATE = """
The following Python function (from module '{module_path}', function name '{function_name}') has an issue.
Original Problem Description / Goal for Fix:
{problem_description}

Original Function Code:
```python
{original_code}
```

Your task is to provide a corrected version of this Python function.
- Only output the complete, raw Python code for the corrected function.
- Do NOT include any explanations, markdown formatting (like ```python), or any text other than the function code itself.
- Ensure the function signature (name, parameters, type hints) remains the same unless the problem description explicitly requires changing it.
- If you cannot determine a fix or the original code is not a single function, return only the text: "// NO_CODE_SUGGESTION_POSSIBLE"

Corrected Python function code:
"""

LLM_NEW_TOOL_PROMPT_TEMPLATE_SYNTHESIS = """Based on the following high-level description of a desired tool, your task is to generate a single Python function and associated metadata.

Tool Description: "{description}"

Instructions:
1.  **Metadata Line (First Line of Response):** At the very beginning of your response, include a line starting with '# METADATA: ' followed by a JSON string. This JSON string *MUST* contain:
    - 'suggested_function_name': A Pythonic function name (snake_case) for the generated function.
    - 'suggested_tool_name': A short, user-friendly name for tool registration (camelCase or snake_case is acceptable).
    - 'suggested_description': A concise description (max 1-2 sentences) of what the tool does, suitable for a tool registry.
    Example of the first line of the response:
    # METADATA: {{"suggested_function_name": "calculate_circle_area", "suggested_tool_name": "calculateCircleArea", "suggested_description": "Calculates the area of a circle given its radius."}}

2.  **Python Function Code (Following Metadata):** After the metadata line, provide the raw Python code for the function.
    - The function should be self-contained if possible, or use common Python standard libraries.
    - Include type hints for all parameters and the return value.
    - Include a comprehensive docstring explaining what the function does, its arguments (name, type, description), and what it returns.
    - Implement basic error handling using try-except blocks where appropriate (e.g., for type conversions if arguments might not be of the expected type, or for file operations).

Constraints:
- Respond ONLY with the metadata line followed by the raw Python code.
- Do not include any other explanations, comments outside the function's docstring (except the metadata line), or markdown formatting like ```python.

Response Structure:
# METADATA: {{"suggested_function_name": "...", "suggested_tool_name": "...", "suggested_description": "..."}}
def generated_function_name(param1: type, ...) -> return_type:
    \"\"\"Docstring for the function.\"\"\"
    # Function implementation
    ...

Now, generate the metadata and Python function based on the Tool Description provided above.
"""

class CodeSynthesisService:
    """
    Main service class for the Unified Code Writing System (UCWS).
    Acts as an entry point for all code synthesis tasks.
    """

    def __init__(self):
        """
        Initializes the CodeSynthesisService.
        """
        print("CodeSynthesisService initialized.")

    async def submit_task(self, request: CodeTaskRequest) -> CodeTaskResult:
        """
        Primary method to request code synthesis.
        Dispatches to specific handlers based on request.task_type.
        """
        print(f"CodeSynthesisService: Received task {request.request_id} of type {request.task_type.name}")

        if request.task_type == CodeTaskType.NEW_TOOL_CREATION_LLM:
            return await self._handle_new_tool_creation_llm(request)
        elif request.task_type == CodeTaskType.EXISTING_TOOL_SELF_FIX_LLM:
            return await self._handle_existing_tool_self_fix_llm(request)
        elif request.task_type == CodeTaskType.EXISTING_TOOL_SELF_FIX_AST:
            return await self._handle_existing_tool_self_fix_ast(request)
        else:
            print(f"Warning: Unsupported task type: {request.task_type}") # pragma: no cover
            return CodeTaskResult(
                request_id=request.request_id,
                status=CodeTaskStatus.FAILURE_UNSUPPORTED_TASK,
                error_message=f"Task type {request.task_type.name} is not supported."
            )

    async def _handle_new_tool_creation_llm(self, request: CodeTaskRequest) -> CodeTaskResult:
        """Handles new tool creation using LLM, with added error handling for LLM calls."""
        print(f"CodeSynthesisService: Handling NEW_TOOL_CREATION_LLM for request {request.request_id}")
        
        tool_description = request.context_data.get("description")
        if not tool_description:
            return CodeTaskResult(
                request_id=request.request_id,
                status=CodeTaskStatus.FAILURE_PRECONDITION,
                error_message="Missing 'description' in context_data for new tool creation."
            )

        prompt = LLM_NEW_TOOL_PROMPT_TEMPLATE_SYNTHESIS.format(description=tool_description)

        llm_config = request.llm_config_overrides or {}
        model_name = llm_config.get("model_name", get_model_for_task("code_generation"))
        temperature = llm_config.get("temperature", 0.3)
        max_tokens = llm_config.get("max_tokens", 2048) # Increased for potentially larger tools

        print(f"CodeSynthesisService: Sending new tool prompt to LLM (model: {model_name})...")
        try:
            llm_response = await invoke_ollama_model_async(
                prompt, model_name=model_name, temperature=temperature, max_tokens=max_tokens
            )
        except Exception as e:
            error_msg = f"LLM invocation failed for new tool generation: {e}"
            print(f"CodeSynthesisService: {error_msg}")
            return CodeTaskResult(
                request_id=request.request_id,
                status=CodeTaskStatus.FAILURE_LLM_GENERATION,
                error_message=error_msg,
                metadata={"llm_model_used": model_name, "llm_prompt_preview": prompt[:300]+"..."}
            )

        response_metadata_log = {
            "llm_model_used": model_name,
            "llm_prompt_preview": prompt[:300]+"...",
            "llm_response_preview": (llm_response[:200] + "...") if llm_response else "None"
        }

        if not llm_response or not llm_response.strip():
            error_msg = "LLM did not provide a response for new tool generation."
            print(f"CodeSynthesisService: {error_msg}")
            return CodeTaskResult(request_id=request.request_id, status=CodeTaskStatus.FAILURE_LLM_GENERATION,
                                  error_message=error_msg, metadata=response_metadata_log)

        parsed_metadata: Optional[Dict[str, str]] = None
        actual_code_str: str = ""

        if llm_response.startswith("# METADATA:"):
            try:
                lines = llm_response.split('\n', 1)
                metadata_line = lines[0]
                metadata_json_str_match = re.search(r"{\s*.*?\s*}", metadata_line)
                if metadata_json_str_match:
                    metadata_json_str = metadata_json_str_match.group(0)
                    parsed_metadata = json.loads(metadata_json_str)
                    actual_code_str = lines[1] if len(lines) > 1 else ""
                else:
                    actual_code_str = llm_response # Assume no valid metadata line
            except Exception as e:
                print(f"CodeSynthesisService: Error parsing metadata for new tool: {e}. Treating response as code only.")
                actual_code_str = llm_response.lstrip("# METADATA:") if llm_response.startswith("# METADATA:") else llm_response
        else:
            actual_code_str = llm_response

        cleaned_llm_code = actual_code_str.strip()
        if cleaned_llm_code.startswith("```python"):
            cleaned_llm_code = cleaned_llm_code[len("```python"):].strip()
        if cleaned_llm_code.endswith("```"):
            cleaned_llm_code = cleaned_llm_code[:-len("```")].strip()

        if not cleaned_llm_code or not parsed_metadata:
            error_msg = "LLM response for new tool generation was missing code or parsable metadata."
            print(f"CodeSynthesisService: {error_msg}")
            return CodeTaskResult(request_id=request.request_id, status=CodeTaskStatus.FAILURE_LLM_GENERATION,
                                  error_message=error_msg, generated_code=cleaned_llm_code, metadata=response_metadata_log)
        
        response_metadata_log["parsed_tool_metadata"] = parsed_metadata
        response_metadata_log["generated_code_length"] = len(cleaned_llm_code)
        return CodeTaskResult(
            request_id=request.request_id,
            status=CodeTaskStatus.SUCCESS,
            generated_code=cleaned_llm_code,
            metadata=response_metadata_log
        )

    async def _handle_existing_tool_self_fix_llm(self, request: CodeTaskRequest) -> CodeTaskResult:
        """Handles fixing existing tools using LLM (full function replacement)."""
        context = request.context_data
        module_path = context.get("module_path")
        function_name = context.get("function_name")
        problem_description = context.get("problem_description")
        original_code_from_context = context.get("original_code")

        if not all([module_path, function_name, problem_description]):
            # Log carefully as module_path or function_name might be None
            mp_log = str(module_path) if module_path is not None else "None"
            fn_log = str(function_name) if function_name is not None else "None"
            print(f"CodeSynthesisService: Precondition failed for EXISTING_TOOL_SELF_FIX_LLM. Module: {mp_log}, Function: {fn_log}. Missing one or more required fields.")
            return CodeTaskResult(request_id=request.request_id, status=CodeTaskStatus.FAILURE_PRECONDITION,
                                  error_message="Missing module_path, function_name, or problem_description.")

        # Now module_path, function_name, and problem_description are guaranteed to be truthy.
        # Pylance should be happier with the assertions below.
        print(f"CodeSynthesisService: Handling EXISTING_TOOL_SELF_FIX_LLM for {module_path}.{function_name}.")

        assert isinstance(module_path, str), "module_path must be a string after validation"
        assert isinstance(function_name, str), "function_name must be a string after validation"

        if original_code_from_context:
            original_code = original_code_from_context
        else:
            original_code = self_modification.get_function_source_code(module_path, function_name)
        if not original_code:
            error_msg = f"Could not retrieve original code for {module_path}.{function_name}."
            print(f"CodeSynthesisService: {error_msg}")
            return CodeTaskResult(request_id=request.request_id, status=CodeTaskStatus.FAILURE_PRECONDITION,
                                  error_message=error_msg)

        prompt = LLM_CODE_FIX_PROMPT_TEMPLATE.format(
            module_path=module_path, function_name=function_name,
            problem_description=problem_description, original_code=original_code
        )

        llm_config = request.llm_config_overrides or {}
        model_name = llm_config.get("model_name", get_model_for_task("code_generation"))
        temperature = llm_config.get("temperature", 0.3)
        max_tokens = llm_config.get("max_tokens", 1024)

        print(f"CodeSynthesisService: Sending code fix prompt to LLM (model: {model_name})...")

        llm_response = await invoke_ollama_model_async(prompt, model_name=model_name, temperature=temperature, max_tokens=max_tokens)

        response_metadata = {
            "llm_model_used": model_name,
            "llm_prompt_preview": prompt[:300]+"...",
            "llm_response_preview": (llm_response[:200] + "...") if llm_response else "None"
        }

        if not llm_response or "// NO_CODE_SUGGESTION_POSSIBLE" in llm_response or len(llm_response.strip()) < 10:
            error_msg = f"LLM did not provide a usable code suggestion. Response: {llm_response}"
            print(f"CodeSynthesisService: {error_msg}")
            return CodeTaskResult(request_id=request.request_id, status=CodeTaskStatus.FAILURE_LLM_GENERATION,
                                  error_message=error_msg, metadata=response_metadata)

        cleaned_llm_code = llm_response.strip()
        if cleaned_llm_code.startswith("```python"): # pragma: no cover
            cleaned_llm_code = cleaned_llm_code[len("```python"):].strip()
        if cleaned_llm_code.endswith("```"): # pragma: no cover
            cleaned_llm_code = cleaned_llm_code[:-len("```")].strip()

        print(f"CodeSynthesisService: LLM generated code suggestion for {function_name}.")
        response_metadata["llm_generated_code_length"] = len(cleaned_llm_code)
        return CodeTaskResult(
            request_id=request.request_id,
            status=CodeTaskStatus.SUCCESS,
            generated_code=cleaned_llm_code,
            metadata=response_metadata
        )

    async def _handle_existing_tool_self_fix_ast(self, request: CodeTaskRequest) -> CodeTaskResult:
        """Handles fixing existing tools by applying a provided code string using AST."""
        print(f"CodeSynthesisService: Handling EXISTING_TOOL_SELF_FIX_AST for request {request.request_id}")
        
        context = request.context_data
        module_path = context.get("module_path")
        function_name = context.get("function_name")
        new_code_string = context.get("new_code_string")
        project_root_path_from_context = context.get("project_root_path")

        if not all([module_path, function_name, new_code_string]):
            return CodeTaskResult(
                request_id=request.request_id,
                status=CodeTaskStatus.FAILURE_PRECONDITION,
                error_message="Missing module_path, function_name, or new_code_string for AST fix."
            )

        # At this point, module_path, function_name, and new_code_string are guaranteed to be truthy.
        # Add assertions to satisfy Pylance and ensure they are strings.
        assert isinstance(module_path, str), "module_path must be a string after validation"
        assert isinstance(function_name, str), "function_name must be a string after validation"
        assert isinstance(new_code_string, str), "new_code_string must be a string after validation"

        # Determine project_root_path. If not provided in context, calculate relative to this file.
        # This assumes CodeSynthesisService is located at ai_assistant/code_synthesis/service.py
        project_root_path = project_root_path_from_context or \
                            os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))

        try:
            modification_result_msg = self_modification.edit_function_source_code(
                module_path=module_path,
                function_name=function_name,
                new_code_string=new_code_string,
                project_root_path=project_root_path
            )
            
            if "success" in modification_result_msg.lower():
                return CodeTaskResult(request_id=request.request_id, status=CodeTaskStatus.SUCCESS,
                                      modified_code_path=f"{module_path}.{function_name}", metadata={"message": modification_result_msg})
            else:
                return CodeTaskResult(request_id=request.request_id, status=CodeTaskStatus.FAILURE_CODE_APPLICATION,
                                      error_message=modification_result_msg, metadata={"details": "AST modification reported failure."})
        except Exception as e:
            print(f"CodeSynthesisService: Exception during AST self-fix for {module_path}.{function_name}: {e}")
            return CodeTaskResult(request_id=request.request_id, status=CodeTaskStatus.FAILURE_CODE_APPLICATION,
                                  error_message=f"Exception during AST self-fix: {e}", metadata={"traceback": str(e)})

if __name__ == '__main__': # pragma: no cover
    import asyncio
    import os
    import sys

    async def main_test_service():
        service = CodeSynthesisService()

        # Test NEW_TOOL_CREATION_LLM
        print("\n--- Testing NEW_TOOL_CREATION_LLM ---")
        new_tool_req_data = {"description": "Create a Python tool that calculates the factorial of a non-negative integer."}
        # Mock invoke_ollama_model_async for this specific test
        new_tool_request = CodeTaskRequest(
            task_type=CodeTaskType.NEW_TOOL_CREATION_LLM,
            context_data=new_tool_req_data
        )
        result1 = await service.submit_task(new_tool_request)
        print(f"Result for {new_tool_request.task_type.name} ({new_tool_request.request_id}): {result1.status.name} - {result1.error_message or ''}")

        DUMMY_MODULE_DIR = "ai_assistant_dummy_tools_ucws"
        DUMMY_MODULE_NAME = "dummy_math_tools_ucws"
        DUMMY_FILE_PATH = os.path.join(DUMMY_MODULE_DIR, f"{DUMMY_MODULE_NAME}.py")

        os.makedirs(DUMMY_MODULE_DIR, exist_ok=True)
        if not os.path.exists(os.path.join(DUMMY_MODULE_DIR, "__init__.py")):
            with open(os.path.join(DUMMY_MODULE_DIR, "__init__.py"), "w") as f: f.write("")

        if not os.path.exists(DUMMY_FILE_PATH):
            with open(DUMMY_FILE_PATH, "w") as f:
                f.write("def add(a, b):\n    return a + b # Original simple add\n")

        project_root_for_test = os.path.abspath(".")
        if project_root_for_test not in sys.path:
             sys.path.insert(0, project_root_for_test)


        tool_fix_llm_req_data = {
            "module_path": f"{DUMMY_MODULE_DIR}.{DUMMY_MODULE_NAME}",
            "function_name": "add",
            "problem_description": "The add function should handle string concatenation as well.",
        }
        tool_fix_llm_request = CodeTaskRequest(
            task_type=CodeTaskType.EXISTING_TOOL_SELF_FIX_LLM,
            context_data=tool_fix_llm_req_data
        )

        original_invoke = invoke_ollama_model_async
        async def mock_invoke(*args, **kwargs):
            print(f"Mocked LLM call for {tool_fix_llm_request.task_type.name}. Returning a sample fix.")
            return "def add(a, b):\n    # LLM-generated fix for string concatenation\n    if isinstance(a, str) and isinstance(b, str):\n        return a + b\n    elif isinstance(a, (int,float)) and isinstance(b, (int,float)):\n        return a + b\n    else:\n        return str(a) + str(b) # Fallback for mixed types"

        from ai_assistant.llm_interface import ollama_client as ollama_client_module
        ollama_client_module.invoke_ollama_model_async = mock_invoke

        result2 = await service.submit_task(tool_fix_llm_request)
        print(f"Result for {tool_fix_llm_request.task_type.name} ({tool_fix_llm_request.request_id}): {result2.status.name} - {result2.error_message or ''}")
        if result2.generated_code:
            print(f"Generated code:\n{result2.generated_code}")

        ollama_client_module.invoke_ollama_model_async = original_invoke

        # Clean up dummy files
        try:
            os.remove(DUMMY_FILE_PATH)
            os.remove(os.path.join(DUMMY_MODULE_DIR, "__init__.py"))
            if not os.listdir(DUMMY_MODULE_DIR): os.rmdir(DUMMY_MODULE_DIR)
        except OSError as e:
            print(f"Error during cleanup: {e}")


    if __name__ == "__main__":
        asyncio.run(main_test_service())

# ### END FILE: ai_assistant/code_synthesis/service.py ###

# ### START FILE: ai_assistant/communication/__init__.py ###
# This file marks communication as a package.

# ### END FILE: ai_assistant/communication/__init__.py ###

# ### START FILE: ai_assistant/communication/cli.py ###
# ai_assistant/communication/cli.py
import importlib
import re
import asyncio
import os
import json
import sys
import traceback
from prompt_toolkit.formatted_text import ANSI
from ai_assistant.goals import goal_management
from ai_assistant.tools import tool_system # Direct import for tool_system_instance 
from ai_assistant.planning.planning import PlannerAgent
from ..planning.execution import ExecutionAgent # Assuming execution is also in a sibling package like core
from ..core.reflection import global_reflection_log, analyze_last_failure, get_learnings_from_reflections
from ai_assistant.core import self_modification
from ai_assistant.memory.awareness import get_tool_associations
from ai_assistant.llm_interface.ollama_client import invoke_ollama_model_async
from ai_assistant.config import get_model_for_task, is_debug_mode 
from typing import Tuple, List, Dict, Any, Optional # Tuple already imported
from ai_assistant.core.conversation_intelligence import detect_missed_tool_opportunity, formulate_tool_description_from_conversation, generate_conversational_response
from ai_assistant.memory.event_logger import log_event, get_recent_events
from ai_assistant.core.autonomous_reflection import run_self_reflection_cycle, select_suggestion_for_autonomous_action
from ai_assistant.tools.tool_system import tool_system_instance # Already imported
from ai_assistant.learning.autonomous_learning import learn_facts_from_interaction
from ai_assistant.config import AUTONOMOUS_LEARNING_ENABLED, CONVERSATION_HISTORY_TURNS
from ai_assistant.utils.display_utils import (
    CLIColors, color_text, format_header, format_message,
    format_input_prompt, format_thinking, format_tool_execution,
    format_status, draw_separator
)
from ai_assistant.core.refinement import RefinementAgent
from ai_assistant.code_services.service import CodeService
from ai_assistant.core.fs_utils import write_to_file
from ai_assistant.core.orchestrator import DynamicOrchestrator # Orchestrator instance
from ai_assistant.core import project_manager
from ai_assistant.core import suggestion_manager
from ai_assistant.core import status_reporting # For status commands
from prompt_toolkit import PromptSession, print_formatted_text
from prompt_toolkit.patch_stdout import patch_stdout

# State variable for pending tool confirmation
_pending_tool_confirmation_details: Optional[Dict[str, Any]] = None
_results_queue: Optional[asyncio.Queue] = None # Queue for background task results
_orchestrator: Optional[DynamicOrchestrator] = None # Orchestrator instance

def _is_search_like_query(user_input: str) -> bool: # pragma: no cover
    user_input_lower = user_input.lower()
    search_keywords = [
        "what is", "who is", "what are", "who are", "explain", "tell me about",
        "search for", "find information on", "how does", "why does", "when did",
        "what's the capital of", "define "
    ]
    tool_creation_keywords = ["make a tool", "create a tool", "new tool", "generate a tool"]
    if any(keyword in user_input_lower for keyword in tool_creation_keywords): return False
    if any(user_input_lower.startswith(keyword) for keyword in search_keywords): return True
    if user_input.endswith("?") and len(user_input_lower) > 10:
        if not user_input_lower.startswith("/"): return True
    return False

async def _handle_code_generation_and_registration(tool_description_for_generation: str):
    """
    Handles the process of generating code via CodeService, saving it, and registering it as a tool.
    """
    if not tool_description_for_generation: # pragma: no cover
        print_formatted_text(ANSI(color_text("Error: Tool description for generation is empty.", CLIColors.ERROR_MESSAGE)))
        return

    print_formatted_text(ANSI(color_text(f"\nReceived tool description for CodeService generation: \"{tool_description_for_generation}\"", CLIColors.SYSTEM_MESSAGE)))

    from ai_assistant.llm_interface import ollama_client as default_llm_provider
    from ai_assistant.core import self_modification as default_self_modification_service
    code_service = CodeService(
        llm_provider=default_llm_provider,
        self_modification_service=default_self_modification_service
    )

    print(color_text(f"Requesting CodeService to generate new tool (context='NEW_TOOL')...", CLIColors.DEBUG_MESSAGE))

    generation_result = await code_service.generate_code(
        context="NEW_TOOL",
        prompt_or_description=tool_description_for_generation,
        target_path=None
    )

    if is_debug_mode():
        print_formatted_text(ANSI(color_text(f"[DEBUG] CodeService generation result: {generation_result}", CLIColors.DEBUG_MESSAGE)))

    if generation_result.get("status") != "SUCCESS_CODE_GENERATED":
        error_msg = generation_result.get("error", "CodeService failed to generate code or parse metadata.")
        logs = generation_result.get("logs", [])
        print_formatted_text(ANSI(color_text(f"CodeService Error: {error_msg}", CLIColors.ERROR_MESSAGE)))
        if logs: # pragma: no cover
            for log_entry in logs: print(color_text(f"  Log: {log_entry}", CLIColors.DEBUG_MESSAGE))

        global_reflection_log.log_execution(
            goal_description=f"CodeService new tool generation attempt for: {tool_description_for_generation}",
            plan=[{'action_type': 'CODESERVICE_GENERATE_NEW_TOOL', 'description': tool_description_for_generation}],
            execution_results=[f"CodeService failed. Status: {generation_result.get('status')}, Error: {error_msg}"],
            overall_success=False, status_override=f"CODESERVICE_GEN_FAILED_{generation_result.get('status','UNKNOWN_ERR')}"
        )
        return

    cleaned_code = generation_result.get("code_string")
    parsed_metadata = generation_result.get("metadata")

    if is_debug_mode():
        print_formatted_text(ANSI(color_text(f"[DEBUG] Cleaned code: {cleaned_code[:200] if cleaned_code else 'None'}...", CLIColors.DEBUG_MESSAGE)))
        print_formatted_text(ANSI(color_text(f"[DEBUG] Parsed metadata: {parsed_metadata}", CLIColors.DEBUG_MESSAGE)))

    if not cleaned_code or not parsed_metadata: # pragma: no cover
        print_formatted_text(ANSI(color_text("CodeService returned success status but missing code or metadata.", CLIColors.ERROR_MESSAGE)))
        global_reflection_log.log_execution(
            goal_description=f"CodeService new tool generation for: {tool_description_for_generation}",
            plan=[{'action_type': 'CODESERVICE_GENERATE_NEW_TOOL'}],
            execution_results=["CodeService reported success but returned incomplete data (missing code/metadata)."],
            overall_success=False, status_override="CODESERVICE_INCOMPLETE_DATA"
        )
        return

    global_reflection_log.log_execution(
        goal_description=f"CodeService new tool generation for: {tool_description_for_generation}",
        plan=[{'action_type': 'CODESERVICE_GENERATE_NEW_TOOL', 'description': tool_description_for_generation,
               'output_preview': cleaned_code[:150] + "..."}],
        execution_results=["CodeService successfully generated code and metadata. Review and registration to follow."],
        overall_success=True, status_override="CODESERVICE_GEN_SUCCESS"
    )

    print_formatted_text(ANSI(color_text("\n--- CodeService Generated Python Code ---", CLIColors.SYSTEM_MESSAGE)))
    print_formatted_text(ANSI(cleaned_code)) # Assuming cleaned_code does not have ANSI
    print_formatted_text(ANSI(color_text("--- End of CodeService Generated Code ---", CLIColors.SYSTEM_MESSAGE)))

    if len(cleaned_code.splitlines()) > 3:
        print_formatted_text(ANSI(color_text("\nConducting initial automated code review...", CLIColors.SYSTEM_MESSAGE)))
        current_code = cleaned_code
        review_results: Optional[Dict[str, Any]] = None

        try:
            review_results = await tool_system_instance.execute_tool(
                "request_code_review_tool",
                args=(current_code, tool_description_for_generation),
                kwargs={'attempt_number': 1}
            )
        except tool_system.ToolNotFoundError: # pragma: no cover
            print_formatted_text(ANSI(color_text("Error: 'request_code_review_tool' not found. Proceeding without review.", CLIColors.ERROR_MESSAGE)))
            review_results = {"status": "review_tool_missing", "comments": "Review tool not found."}
        except Exception as e: # pragma: no cover
            print_formatted_text(ANSI(color_text(f"Error during initial code review: {e}. Proceeding without further refinement.", CLIColors.ERROR_MESSAGE)))
            review_results = {"status": "review_error", "comments": f"Initial review failed: {e}"}

        if review_results:
            initial_review_status_str = review_results.get('status', 'N/A').upper()
            status_color = CLIColors.SYSTEM_MESSAGE
            if initial_review_status_str == 'APPROVED': status_color = CLIColors.AI_RESPONSE
            elif initial_review_status_str in ['REJECTED', 'ERROR', 'REVIEW_TOOL_MISSING', 'REVIEW_ERROR']: status_color = CLIColors.ERROR_MESSAGE # pragma: no cover
            print_formatted_text(ANSI(color_text(f"Initial Review Status: {initial_review_status_str}", status_color)))
            if review_results.get('comments'): print_formatted_text(ANSI(color_text(f"Initial Review Comments: {review_results.get('comments', 'No comments.')}", CLIColors.SYSTEM_MESSAGE)))
            if review_results.get('suggestions'): print_formatted_text(ANSI(color_text(f"Initial Review Suggestions:\n{review_results['suggestions']}", CLIColors.SYSTEM_MESSAGE)))


        if review_results and review_results.get('status') == "requires_changes": # pragma: no cover
            print_formatted_text(ANSI(color_text("\nCode requires changes. Attempting automated refinement...", CLIColors.SYSTEM_MESSAGE)))
            refinement_agent = RefinementAgent()
            max_refinement_attempts = 2
            for attempt in range(max_refinement_attempts):
                refinement_attempt_count = attempt + 1
                print_formatted_text(ANSI(color_text(f"Refinement Attempt {refinement_attempt_count}/{max_refinement_attempts}...", CLIColors.SYSTEM_MESSAGE)))
                # Skip refinement if review_results is None
                if not review_results:
                    break
                refined_code_str = await refinement_agent.refine_code(current_code, tool_description_for_generation, review_results)
                if not refined_code_str or not refined_code_str.strip(): break
                current_code = refined_code_str
                try:
                    review_results = await tool_system_instance.execute_tool("request_code_review_tool", args=(current_code, tool_description_for_generation), kwargs={'attempt_number': refinement_attempt_count + 1})
                except Exception as e:
                    review_results = {"status": "review_error", "comments": f"Failed to review refined code: {e}"}
                    break
                if review_results and (
                    review_results.get('status') == "approved" or 
                    review_results.get('status') == "rejected" or 
                    review_results.get('status') == "review_error"
                ): break
            cleaned_code = current_code

        if review_results and review_results.get('status') not in ["approved", None]: # pragma: no cover
            print_formatted_text(ANSI(color_text("\nLLM-generated code did not pass automated review or review process encountered issues.", CLIColors.ERROR_MESSAGE)))
            try:
                user_choice_after_all_reviews = await asyncio.to_thread(input, color_text("Options: [s]ave anyway, [d]iscard code. Default is discard (d): ", CLIColors.USER_INPUT))
                if user_choice_after_all_reviews.strip().lower() == 's':
                    print_formatted_text(ANSI(color_text("Proceeding to save code despite final review findings...", CLIColors.SYSTEM_MESSAGE)))
                else:
                    print_formatted_text(ANSI(color_text("Generated code discarded based on final review and user choice.", CLIColors.SYSTEM_MESSAGE)))
                    return
            except EOFError: # pragma: no cover
                print_formatted_text(ANSI(color_text("\nInput cancelled. Discarding code.", CLIColors.SYSTEM_MESSAGE)))
                return
        elif review_results and review_results.get('status') == 'approved':
             print_formatted_text(ANSI(color_text("Code review approved!", CLIColors.AI_RESPONSE)))
    else:
        print_formatted_text(ANSI(color_text("\nGenerated code is too short, automated code review was skipped.", CLIColors.SYSTEM_MESSAGE)))

    print_formatted_text(ANSI(color_text("\n" + "="*60, CLIColors.SYSTEM_MESSAGE)))
    print_formatted_text(ANSI(color_text("WARNING: This is LLM-generated code. Review carefully before saving or using.", CLIColors.ERROR_MESSAGE)))
    print_formatted_text(ANSI(color_text("="*60 + "\n", CLIColors.SYSTEM_MESSAGE)))

    use_suggested_details = False
    derived_filename = ""
    function_name_from_meta = parsed_metadata.get("suggested_function_name")
    tool_name_from_meta = parsed_metadata.get("suggested_tool_name")
    description_from_meta = parsed_metadata.get("suggested_description")

    if function_name_from_meta and tool_name_from_meta and description_from_meta:
        derived_filename_base = re.sub(r'[^\w_]', '', function_name_from_meta)
        if not derived_filename_base: derived_filename_base = "generated_tool" # pragma: no cover
        derived_filename = os.path.basename(derived_filename_base + ".py")

        print_formatted_text(ANSI(color_text("\nCodeService Suggested Details:", CLIColors.SYSTEM_MESSAGE)))
        print_formatted_text(ANSI(color_text(f"  Filename:      {derived_filename}", CLIColors.SYSTEM_MESSAGE)))
        print_formatted_text(ANSI(color_text(f"  Function name: {function_name_from_meta}", CLIColors.SYSTEM_MESSAGE)))
        print_formatted_text(ANSI(color_text(f"  Tool name:     {tool_name_from_meta}", CLIColors.SYSTEM_MESSAGE)))
        print_formatted_text(ANSI(color_text(f"  Description:   {description_from_meta}", CLIColors.SYSTEM_MESSAGE)))

        try: # pragma: no cover
            confirm_suggested = input(color_text("Use these details to save and register? (y/n): ", CLIColors.USER_INPUT)).strip().lower()
            if confirm_suggested == 'y': use_suggested_details = True
            elif confirm_suggested == 'n': print_formatted_text(ANSI(color_text("Okay, you can provide details manually or cancel.", CLIColors.SYSTEM_MESSAGE)))
            else: print_formatted_text(ANSI(color_text("Invalid choice. Proceeding with manual input.", CLIColors.ERROR_MESSAGE)))
        except EOFError: # pragma: no cover
            print_formatted_text(ANSI(color_text("\nInput cancelled. Proceeding with manual input or cancellation option.", CLIColors.SYSTEM_MESSAGE)))
            return
    else:
        print_formatted_text(ANSI(color_text("CodeService did not provide complete suggestions for all fields (function name, tool name, description).", CLIColors.SYSTEM_MESSAGE)))

    filepath_to_save: Optional[str] = None
    module_path_for_registration: Optional[str] = None
    function_to_register_final: Optional[str] = None
    tool_name_for_registration_final: Optional[str] = None
    tool_description_for_registration_final: Optional[str] = None
    should_save_and_register = False

    if use_suggested_details:
        filepath_to_save = os.path.join("ai_assistant", "custom_tools", derived_filename)
        function_to_register_final = function_name_from_meta
        tool_name_for_registration_final = tool_name_from_meta
        tool_description_for_registration_final = description_from_meta
        filename_base_no_py = derived_filename[:-3] if derived_filename.endswith(".py") else derived_filename
        module_path_for_registration = f"ai_assistant.custom_tools.{filename_base_no_py}"
        should_save_and_register = True # No print here, will print below
    else: # pragma: no cover
        try:
            save_choice = input(color_text("Save generated code to `ai_assistant/custom_tools/`? (y/n): ", CLIColors.USER_INPUT)).strip().lower()
            if save_choice == 'y':
                filename_input = input(color_text("Filename (e.g., my_tool.py): ", CLIColors.USER_INPUT)).strip()
                if not filename_input: return
                sanitized_basename = os.path.basename(filename_input)
                if not sanitized_basename.endswith(".py"): sanitized_basename += ".py"
                filepath_to_save = os.path.join("ai_assistant", "custom_tools", sanitized_basename)

                register_choice = input(color_text(f"Register function from '{filepath_to_save}'? (y/n): ", CLIColors.USER_INPUT)).strip().lower()
                if register_choice == 'y':
                    function_to_register_final = input(color_text("Function name: ", CLIColors.USER_INPUT)).strip()
                    tool_name_for_registration_final = input(color_text("Tool name for registration: ", CLIColors.USER_INPUT)).strip()
                    tool_description_for_registration_final = input(color_text("Tool description: ", CLIColors.USER_INPUT)).strip()
                    if all([function_to_register_final, tool_name_for_registration_final, tool_description_for_registration_final]):
                        module_path_for_registration = f"ai_assistant.custom_tools.{sanitized_basename[:-3]}"
                        should_save_and_register = True
            else:
                print_formatted_text(ANSI(color_text("Code not saved.", CLIColors.SYSTEM_MESSAGE)))
                return
        except EOFError: return

    if filepath_to_save:
        print_formatted_text(ANSI(color_text(f"Attempting to save code to {filepath_to_save} using fs_utils...", CLIColors.DEBUG_MESSAGE)))
        if write_to_file(filepath_to_save, cleaned_code):
            print_formatted_text(ANSI(color_text(f"Code successfully saved to {filepath_to_save}.", CLIColors.SYSTEM_MESSAGE)))
            global_reflection_log.log_execution(
                goal_description=f"File saving for generated tool: {tool_description_for_generation}",
                plan=[{'action_type': 'TOOL_CODE_SAVE_CLI', 'filepath': filepath_to_save}],
                execution_results=[f"Generated code successfully saved by CLI to {filepath_to_save}."],
                overall_success=True, status_override="TOOL_CODE_SAVE_SUCCESS_CLI"
            )

            if should_save_and_register and module_path_for_registration and \
               function_to_register_final and tool_name_for_registration_final and tool_description_for_registration_final:
                reg_success, reg_message = _perform_tool_registration(
                    module_path_for_registration, function_to_register_final,
                    tool_name_for_registration_final, tool_description_for_registration_final
                )
                print_formatted_text(ANSI(color_text(reg_message, CLIColors.SYSTEM_MESSAGE if reg_success else CLIColors.ERROR_MESSAGE)))
                global_reflection_log.log_execution(
                    goal_description=f"Tool registration attempt: {tool_name_for_registration_final}",
                    plan=[{'action_type': 'TOOL_REGISTER_CLI', 'details': {'name': tool_name_for_registration_final, 'module': module_path_for_registration}}],
                    execution_results=[reg_message],
                    overall_success=reg_success, status_override="TOOL_REGISTRATION_SUCCESS_CLI" if reg_success else "TOOL_REGISTRATION_FAILED_CLI"
                )
                if reg_success:
                    print_formatted_text(ANSI(color_text(f"Tool '{tool_name_for_registration_final}' registered. Attempting to generate unit test scaffold...", CLIColors.SYSTEM_MESSAGE)))

                    if not module_path_for_registration or not cleaned_code: # pragma: no cover
                        print_formatted_text(ANSI(color_text("Error: Missing module path or code content for scaffold generation.", CLIColors.ERROR_MESSAGE)))
                    else:
                        base_module_name = module_path_for_registration.split('.')[-1]
                        test_filename = f"test_{base_module_name}.py"
                        test_target_dir = os.path.join("tests", "custom_tools") 
                        os.makedirs(test_target_dir, exist_ok=True) 
                        test_target_path = os.path.join(test_target_dir, test_filename)

                        scaffold_gen_result = await code_service.generate_code(
                            context="GENERATE_UNIT_TEST_SCAFFOLD",
                            prompt_or_description=cleaned_code,
                            additional_context={"module_name_hint": module_path_for_registration},
                            target_path=test_target_path
                        )

                        # Safe handling of scaffold_gen_result
                        if not scaffold_gen_result:
                            print_formatted_text(ANSI(color_text("Error: Failed to generate unit test scaffold - no result returned", CLIColors.ERROR_MESSAGE)))
                            return

                        status = scaffold_gen_result.get("status")
                        saved_path = scaffold_gen_result.get("saved_to_path")
                        error_msg = scaffold_gen_result.get("error", "Unknown error occurred")

                        if status == "SUCCESS_CODE_GENERATED" and saved_path:
                            print_formatted_text(ANSI(color_text(f"Unit test scaffold successfully generated and saved to: {saved_path}", CLIColors.AI_RESPONSE)))
                            global_reflection_log.log_execution(
                                goal_description=f"Unit test scaffold generation for tool {tool_name_for_registration_final}",
                                plan=[{'action_type': 'SCAFFOLD_GENERATION_CLI', 'tool_module': module_path_for_registration}],
                                execution_results=[f"Scaffold saved to {saved_path}"],
                                overall_success=True, status_override="SCAFFOLD_GEN_SAVE_SUCCESS"
                            )
                        else:
                            print_formatted_text(ANSI(color_text(f"Failed to generate or save unit test scaffold: {error_msg}", CLIColors.ERROR_MESSAGE)))
                            global_reflection_log.log_execution(
                                goal_description=f"Unit test scaffold generation for tool {tool_name_for_registration_final}",
                                plan=[{'action_type': 'SCAFFOLD_GENERATION_CLI', 'tool_module': module_path_for_registration}],
                                execution_results=[f"Scaffold generation failed. Status: {status}, Error: {error_msg}"],
                                overall_success=False, status_override=f"SCAFFOLD_GEN_FAILED_{status or 'UNKNOWN_ERR'}"
                            )
            elif filepath_to_save and not should_save_and_register: 
                 global_reflection_log.log_execution(
                    goal_description=f"File saving for generated tool (no registration): {tool_description_for_generation}",
                    plan=[{'action_type': 'TOOL_CODE_SAVE_ONLY_CLI', 'filepath': filepath_to_save}],
                    execution_results=[f"Code saved to {filepath_to_save}, registration skipped by user."],
                    overall_success=True, status_override="TOOL_CODE_SAVE_ONLY_SUCCESS_CLI"
                )
        else: 
            print_formatted_text(ANSI(color_text(f"Failed to save code to {filepath_to_save} using fs_utils.", CLIColors.ERROR_MESSAGE)))
            global_reflection_log.log_execution(
                goal_description=f"File saving for generated tool: {tool_description_for_generation}",
                plan=[{'action_type': 'TOOL_CODE_SAVE_CLI', 'filepath': filepath_to_save}],
                execution_results=[f"Failed to save code by CLI to {filepath_to_save}."],
                overall_success=False, status_override="TOOL_CODE_SAVE_FAILED_CLI"
            )
            return 

def _perform_tool_registration(module_path: str, function_name: str, tool_name: str, description: str) -> Tuple[bool, str]:
    try:
        if module_path in sys.modules:
            importlib.reload(sys.modules[module_path])
        imported_module = importlib.import_module(module_path)
        function_object = getattr(imported_module, function_name)
        tool_system_instance.register_tool(
            tool_name=tool_name,
            description=description,
            module_path=module_path,
            function_name_in_module=function_name,
            tool_type="dynamic", 
            func_callable=function_object
        )
        log_event(event_type="TOOL_REGISTERED_MANUAL",description=f"Tool '{tool_name}' was manually registered via CLI.",source="cli._perform_tool_registration",metadata={"tool_name": tool_name,"module_path": module_path,"function_name": function_name, "description": description})
        return True, f"Tool '{tool_name}' from '{module_path}.{function_name}' registered successfully."
    except ModuleNotFoundError: return False, f"Error: Module '{module_path}' not found."
    except AttributeError: return False, f"Error: Function '{function_name}' not found in module '{module_path}'."
    except tool_system.ToolAlreadyRegisteredError: return False, f"Error: Tool name '{tool_name}' is already registered." 
    except Exception as e: return False, f"An unexpected error occurred during tool registration: {e}"


async def _process_command_wrapper(prompt: str, orchestrator: DynamicOrchestrator, queue: asyncio.Queue):
    """Wraps orchestrator processing, handles learning, and puts results on a queue."""
    try:
        success, response = await orchestrator.process_prompt(prompt)
        status_message_str = format_status("Task completed", True) if success else format_status("Task failed", False)

        # Queue the status message first
        await queue.put({
            "type": "status_update",
            "message": status_message_str,
            "prompt_context": prompt # For context if needed
        })

        if AUTONOMOUS_LEARNING_ENABLED and response and success: # Only learn from successful interactions with a response
            learned_facts = await learn_facts_from_interaction(prompt, response, AUTONOMOUS_LEARNING_ENABLED)
            if learned_facts:
                await queue.put({
                    "type": "learning_result",
                    "facts": learned_facts,
                    "original_prompt": prompt
                })

        # Then queue the main command result
        await queue.put({"type": "command_result", "prompt": prompt, "success": success, "response": response})

    except Exception as e:
        error_msg_text = f"Error processing '{prompt}': {type(e).__name__}"
        log_event(
            event_type="CLI_WRAPPER_ERROR",
            description=error_msg_text,
            source="cli._process_command_wrapper",
            metadata={"prompt": prompt, "error": str(e), "traceback": traceback.format_exc()}
        )
        # Queue status update for the error
        await queue.put({
            "type": "status_update",
            "message": format_message("ERROR", error_msg_text, CLIColors.ERROR_MESSAGE),
            "prompt_context": prompt
        })
        # Queue command result indicating failure
        await queue.put({
            "type": "command_result",
            "prompt": prompt,
            "success": False,
            "response": error_msg_text # Or a more user-friendly error from orchestrator if available
        })

async def _handle_cli_results(queue: asyncio.Queue):
    """Checks the queue and prints any results."""
    while not queue.empty():
        try:
            result_item = queue.get_nowait()
            item_type = result_item.get("type")

            if item_type == "status_update":
                # This message is usually short and indicates completion/failure of a background task.
                # It's printed on its own line.
                print_formatted_text(result_item.get('message')) # Assumes message is ANSI
            elif item_type == "command_result":
                original_prompt = result_item.get("prompt", "Unknown prompt")
                success = result_item.get("success")
                response_msg = result_item.get("response")

                print_formatted_text(format_header(f"Result for: {original_prompt}"))
                if success:
                    print_formatted_text(format_message("AI", response_msg, CLIColors.AI_RESPONSE))
                else:
                    print_formatted_text(format_message("ERROR", response_msg, CLIColors.ERROR_MESSAGE))
                # The main loop will handle drawing a separator after all results for a cycle are printed.
            elif item_type == "learning_result":
                facts = result_item.get("facts", [])
                learned_from_prompt = result_item.get("original_prompt", "a recent interaction")
                if facts:
                    print_formatted_text(format_message("LEARNED", f"From '{learned_from_prompt}', I've noted: {', '.join(facts)}", CLIColors.SUCCESS))
            queue.task_done()
        except asyncio.QueueEmpty:
            break # Should not happen with `while not queue.empty()` but good practice
        except Exception as e: # pragma: no cover
            print_formatted_text(ANSI(color_text(f"\nError displaying result: {e}", CLIColors.ERROR_MESSAGE)))

# Task to periodically process results from the queue
async def periodic_results_processor(queue: asyncio.Queue, running_event: asyncio.Event):
    """Periodically checks the queue and displays results if the CLI is running."""
    while running_event.is_set():
        if not queue.empty():
            await _handle_cli_results(queue) # This function already handles multiple items
        await asyncio.sleep(0.1) # Check every 100ms, yield control

async def start_cli():
    global _pending_tool_confirmation_details, _orchestrator, _results_queue

    # Welcome banner
    print_formatted_text(ANSI("\n"))
    print_formatted_text(draw_separator())
    print_formatted_text(format_header("AI Assistant CLI"))
    print_formatted_text(format_message("WELCOME", "Interactive AI Assistant Ready", CLIColors.SUCCESS))
    print_formatted_text(format_message("INFO", "Type /help to see available commands", CLIColors.SYSTEM_MESSAGE))
    print_formatted_text(draw_separator())
    print_formatted_text(ANSI("\n"))
    
    # Initialize components
    from ai_assistant.learning.learning import LearningAgent
    learning_agent = LearningAgent()
    executor = ExecutionAgent()
    planner = PlannerAgent()

    _orchestrator = DynamicOrchestrator(planner, executor, learning_agent)
    _results_queue = asyncio.Queue()

    # Use PromptSession for async input
    session = PromptSession(format_input_prompt())

    user_command_tasks: List[asyncio.Task] = [] # Tasks spawned by user commands

    cli_running_event = asyncio.Event()
    cli_running_event.set() # Signal that the CLI is active

    # Start the dedicated task for processing and displaying results from the queue
    results_processor_task = asyncio.create_task(
        periodic_results_processor(_results_queue, cli_running_event)
    )

    # patch_stdout ensures that print statements from other asyncio tasks
    # don't mess with the prompt_toolkit's rendering of the input line.
    with patch_stdout():
        try:
            while True:
                try:
                    user_command_tasks = [t for t in user_command_tasks if not t.done()] # Cleanup completed user command tasks

                    # Now, show the prompt for new input using prompt_toolkit
                    user_input = await session.prompt_async() # Removed the explicit prompt string here as it's in PromptSession
                    
                    if user_input.strip():
                        # If user provided meaningful input, log it and draw a separator
                        log_event(event_type="USER_INPUT_RECEIVED", description=user_input, source="cli.start_cli", metadata={"length": len(user_input)})
                        print_formatted_text(draw_separator())
                    else:
                        # If user just pressed Enter (empty input), skip further processing and loop to show results/prompt again
                        await asyncio.sleep(0.01) # Small sleep to allow other tasks to run if queue was empty
                        continue

                except EOFError:
                    print_formatted_text(format_message("SYSTEM", "\nGracefully shutting down (EOF)...", CLIColors.SYSTEM_MESSAGE))
                    break
                except KeyboardInterrupt:
                    print_formatted_text(format_message("SYSTEM", "\nGracefully shutting down (Ctrl+C)...", CLIColors.SYSTEM_MESSAGE))
                    break

                if not user_input.strip(): # Should be caught by the continue above, but as a safeguard
                    continue

                if user_input.startswith("/"):
                    _pending_tool_confirmation_details = None 
                    parts = user_input.split()
                    command = parts[0].lower()
                    args_cmd = parts[1:] 

                    if command == "/exit" or command == "/quit":
                        print_formatted_text(ANSI(color_text("Exiting assistant...", CLIColors.SYSTEM_MESSAGE)))
                        break
                    elif command == "/help":
                        print_formatted_text(format_header("Available Commands"))
                        
                        # Tool Management
                        print_formatted_text(format_message("CMD", "/tools <action> [tool_name]", CLIColors.COMMAND))
                        print_formatted_text(ANSI(color_text("      Manage tools (list, add, remove, update)", CLIColors.SYSTEM_MESSAGE)))
                        print_formatted_text(ANSI("\n    " + color_text("Tool Actions:", CLIColors.BOLD)))
                        print_formatted_text(ANSI(color_text("      • list               : List all available tools", CLIColors.SYSTEM_MESSAGE)))
                        print_formatted_text(ANSI(color_text("      • add <description>  : Generate and add a new tool", CLIColors.SYSTEM_MESSAGE)))
                        print_formatted_text(ANSI(color_text("      • remove <name>      : Remove a tool", CLIColors.SYSTEM_MESSAGE)))
                        print_formatted_text(ANSI(color_text("      • info <name>        : Show tool details", CLIColors.SYSTEM_MESSAGE)))
                        print_formatted_text(ANSI(color_text("      • update <name>      : Update tool details", CLIColors.SYSTEM_MESSAGE)))

                        # Project Management  
                        print_formatted_text(format_message("CMD", "/projects <action> [project_name]", CLIColors.COMMAND))
                        print_formatted_text(ANSI(color_text("      Manage AI projects", CLIColors.SYSTEM_MESSAGE)))
                        print_formatted_text(ANSI("\n    " + color_text("Project Actions:", CLIColors.BOLD)))
                        print_formatted_text(ANSI(color_text("      • list              : List all projects", CLIColors.SYSTEM_MESSAGE)))
                        print_formatted_text(ANSI(color_text("      • new <name> [desc]: Create a new project", CLIColors.SYSTEM_MESSAGE)))
                        print_formatted_text(ANSI(color_text("      • remove <name_or_id>: Remove a project", CLIColors.SYSTEM_MESSAGE)))
                        print_formatted_text(ANSI(color_text("      • info <name_or_id> : Show project details", CLIColors.SYSTEM_MESSAGE)))
                        print_formatted_text(ANSI(color_text("      • status [name_or_id]: Show project status (overall if no name/id)", CLIColors.SYSTEM_MESSAGE)))
                        print_formatted_text(ANSI(color_text("      • set_status <name_or_id> <status_val>: Set project status", CLIColors.SYSTEM_MESSAGE)))

                        # Suggestions Management
                        print_formatted_text(format_message("CMD", "/suggestions <action>", CLIColors.COMMAND))
                        print_formatted_text(ANSI(color_text("      Manage AI suggestions and improvements", CLIColors.SYSTEM_MESSAGE)))
                        print_formatted_text(ANSI("\n    " + color_text("Suggestion Actions:", CLIColors.BOLD)))
                        print_formatted_text(ANSI(color_text("      • list              : List all suggestions", CLIColors.SYSTEM_MESSAGE)))
                        print_formatted_text(ANSI(color_text("      • approve <id>      : Approve a suggestion", CLIColors.SYSTEM_MESSAGE)))
                        print_formatted_text(ANSI(color_text("      • deny <id> [reason]: Deny a suggestion", CLIColors.SYSTEM_MESSAGE)))
                        print_formatted_text(ANSI(color_text("      • status           : Show suggestions status", CLIColors.SYSTEM_MESSAGE)))

                        # System Status
                        print_formatted_text(format_message("CMD", "/status [component | all]", CLIColors.COMMAND))
                        print_formatted_text(ANSI(color_text("      Show system status", CLIColors.SYSTEM_MESSAGE)))
                        print_formatted_text(ANSI("\n    " + color_text("Status Components:", CLIColors.BOLD)))
                        print_formatted_text(ANSI(color_text("      • tools            : Show tools status", CLIColors.SYSTEM_MESSAGE)))
                        print_formatted_text(ANSI(color_text("      • projects         : Show projects status", CLIColors.SYSTEM_MESSAGE)))
                        print_formatted_text(ANSI(color_text("      • suggestions      : Show suggestions status", CLIColors.SYSTEM_MESSAGE)))
                        print_formatted_text(ANSI(color_text("      • system           : Show overall system status", CLIColors.SYSTEM_MESSAGE)))
                        print_formatted_text(ANSI(color_text("      • all              : Show all status information", CLIColors.SYSTEM_MESSAGE)))

                        # Tool Generation and Reviews
                        print_formatted_text(format_message("CMD", "/generate_tool_code_with_llm \"<description>\"", CLIColors.COMMAND))
                        print_formatted_text(ANSI(color_text("      Generate and register a new tool from description", CLIColors.SYSTEM_MESSAGE)))
                        
                        print_formatted_text(format_message("CMD", "/review_insights", CLIColors.COMMAND))
                        print_formatted_text(ANSI(color_text("      Review insights and propose actions", CLIColors.SYSTEM_MESSAGE)))
                        
                        # Tool Confirmation Management  
                        print_formatted_text(format_message("CMD", "/manage_confirmation <action> [tool_name]", CLIColors.COMMAND))
                        print_formatted_text(ANSI(color_text("      Manage tool confirmation settings", CLIColors.SYSTEM_MESSAGE)))
                        print_formatted_text(ANSI("\n    " + color_text("Confirmation Actions:", CLIColors.BOLD)))
                        print_formatted_text(ANSI(color_text("      • add <tool_name>     : Require confirmation for specific tool", CLIColors.SYSTEM_MESSAGE)))
                        print_formatted_text(ANSI(color_text("      • remove <tool_name>  : Auto-approve specific tool", CLIColors.SYSTEM_MESSAGE)))
                        print_formatted_text(ANSI(color_text("      • list                : Show tools requiring confirmation", CLIColors.SYSTEM_MESSAGE)))
                        print_formatted_text(ANSI(color_text("      • add_all            : Require confirmation for all tools", CLIColors.SYSTEM_MESSAGE)))
                        print_formatted_text(ANSI(color_text("      • remove_all         : Auto-approve all tools", CLIColors.SYSTEM_MESSAGE)))
                        
                        # Exit Commands  
                        print_formatted_text(format_message("CMD", "/exit or /quit", CLIColors.COMMAND))
                        print_formatted_text(ANSI(color_text("      Exit the assistant", CLIColors.SYSTEM_MESSAGE)))
                        
                        print_formatted_text(draw_separator())

                    elif command == "/tools":
                        if not args_cmd:
                            print_formatted_text(format_message("ERROR", "Usage: /tools <list|add|remove|info|update> [tool_name]", CLIColors.ERROR_MESSAGE))
                            continue

                        action = args_cmd[0].lower()
                        tool_name = args_cmd[1] if len(args_cmd) > 1 else None

                        if action == "list":
                            tools = tool_system_instance.list_tools()
                            print_formatted_text(format_header("Available Tools"))
                            for name, desc in tools.items():
                                print_formatted_text(ANSI(f"{color_text(name, CLIColors.COMMAND)}: {color_text(desc, CLIColors.SYSTEM_MESSAGE)}"))
                        elif action == "add":
                            if len(args_cmd) < 2:
                                print_formatted_text(format_message("ERROR", "Usage: /tools add <tool_description>", CLIColors.ERROR_MESSAGE))
                                continue
                            description = " ".join(args_cmd[1:])
                            await _handle_code_generation_and_registration(description)
                        elif action == "remove":
                            if not tool_name:
                                print_formatted_text(format_message("ERROR", "Usage: /tools remove <tool_name>", CLIColors.ERROR_MESSAGE))
                                continue
                            if tool_system_instance.remove_tool(tool_name):
                                print_formatted_text(format_message("SUCCESS", f"Tool '{tool_name}' removed successfully", CLIColors.SUCCESS))
                            else:
                                print_formatted_text(format_message("ERROR", f"Could not remove tool '{tool_name}'. It may not exist or be a system tool.", CLIColors.ERROR_MESSAGE))
                        elif action == "info":
                            if not tool_name:
                                print_formatted_text(format_message("ERROR", "Usage: /tools info <tool_name>", CLIColors.ERROR_MESSAGE))
                                continue
                            tool_info = tool_system_instance.get_tool(tool_name)
                            if tool_info:
                                print_formatted_text(format_header(f"Tool Information: {tool_name}"))
                                print_formatted_text(ANSI(f"Description: {color_text(tool_info['description'], CLIColors.SYSTEM_MESSAGE)}"))
                                print_formatted_text(ANSI(f"Module: {color_text(tool_info['module_path'], CLIColors.SYSTEM_MESSAGE)}"))
                                print_formatted_text(ANSI(f"Function: {color_text(tool_info['function_name'], CLIColors.SYSTEM_MESSAGE)}"))
                                print_formatted_text(ANSI(f"Type: {color_text(tool_info['type'], CLIColors.SYSTEM_MESSAGE)}"))
                            else:
                                print_formatted_text(format_message("ERROR", f"Tool '{tool_name}' not found", CLIColors.ERROR_MESSAGE))
                        elif action == "update":
                            if not tool_name:
                                print_formatted_text(format_message("ERROR", "Usage: /tools update <tool_name> <new_description>", CLIColors.ERROR_MESSAGE))
                                continue
                            description = " ".join(args_cmd[2:]) if len(args_cmd) > 2 else None
                            if not description:
                                print_formatted_text(format_message("ERROR", "Please provide a new description for the tool", CLIColors.ERROR_MESSAGE))
                                continue
                                
                            try:
                                result = await tool_system_instance.execute_tool(
                                    "system_update_tool_metadata",
                                    args=(tool_name, description)
                                )
                                if result:
                                    print_formatted_text(format_message("SUCCESS", f"Tool '{tool_name}' updated successfully", CLIColors.SUCCESS))
                                else:
                                    print_formatted_text(format_message("ERROR", f"Failed to update tool '{tool_name}'", CLIColors.ERROR_MESSAGE))
                            except Exception as e:
                                print_formatted_text(format_message("ERROR", f"Error updating tool '{tool_name}': {e}", CLIColors.ERROR_MESSAGE))
                        else:
                            print_formatted_text(format_message("ERROR", f"Unknown tools action: {action}", CLIColors.ERROR_MESSAGE))

                    elif command == "/projects":
                        if not args_cmd:
                            print_formatted_text(format_message("ERROR", "Usage: /projects <list|new|remove|info|status> [project_name]", CLIColors.ERROR_MESSAGE))
                            continue

                        action = args_cmd[0].lower()
                        project_name_or_id = args_cmd[1] if len(args_cmd) > 1 else None

                        if action == "list":
                            projects = project_manager.list_projects()
                            if projects:
                                print_formatted_text(format_header("Projects List"))
                                for proj in projects:
                                    print_formatted_text(ANSI(f"- Name: {color_text(proj['name'], CLIColors.COMMAND)} (ID: {proj['project_id']})"))
                                    print_formatted_text(ANSI(f"  Status: {color_text(proj['status'], CLIColors.SYSTEM_MESSAGE)}, Created: {proj['created_at']}"))
                                    if proj.get('description'):
                                        print_formatted_text(ANSI(f"  Description: {proj['description']}"))
                            else:
                                print_formatted_text(format_message("INFO", "No projects found.", CLIColors.SYSTEM_MESSAGE))
                        elif action == "new":
                            if not project_name_or_id:
                                print_formatted_text(format_message("ERROR", "Usage: /projects new <project_name>", CLIColors.ERROR_MESSAGE))
                                continue
                            description = " ".join(args_cmd[2:]) if len(args_cmd) > 2 else None
                            project_manager.create_project(project_name_or_id, description)
                        elif action == "remove":
                            if not project_name_or_id:
                                print_formatted_text(format_message("ERROR", "Usage: /projects remove <project_name_or_id>", CLIColors.ERROR_MESSAGE))
                                continue
                            project_manager.remove_project(project_name_or_id)
                        elif action == "info":
                            if not project_name_or_id:
                                print_formatted_text(format_message("ERROR", "Usage: /projects info <project_name_or_id>", CLIColors.ERROR_MESSAGE))
                                continue
                            info = project_manager.get_project_info(project_name_or_id)
                            if info:
                                print_formatted_text(format_header(f"Project Info: {info['name']} (ID: {info['project_id']})"))
                                for key, value in info.items():
                                    print_formatted_text(ANSI(f"- {key.capitalize()}: {color_text(str(value), CLIColors.SYSTEM_MESSAGE)}"))
                            else:
                                 # get_project_info already prints not found
                                 pass
                        elif action == "status":
                            if not project_name_or_id:
                                print_formatted_text(format_header("Overall Projects Status"))
                                status_info = project_manager.get_all_projects_summary_status()
                                print_formatted_text(ANSI(color_text(status_info, CLIColors.SYSTEM_MESSAGE)))
                            else:
                                status = project_manager.get_project_status(project_name_or_id)
                                if status:
                                    project_info = project_manager.get_project_info(project_name_or_id) # To get name if ID was used
                                    print_formatted_text(format_header(f"Project Status: {project_info['name'] if project_info else project_name_or_id}"))
                                    print_formatted_text(ANSI(f"Status: {color_text(status, CLIColors.SYSTEM_MESSAGE)}"))
                                else:
                                    # get_project_status doesn't print, so we do if not found
                                    print_formatted_text(format_message("ERROR", f"Project '{project_name_or_id}' not found.", CLIColors.ERROR_MESSAGE))
                        elif action == "set_status":
                            if len(args_cmd) < 3:
                                print_formatted_text(format_message("ERROR", "Usage: /projects set_status <project_name_or_id> <new_status>", CLIColors.ERROR_MESSAGE))
                                continue
                            project_identifier = args_cmd[1]
                            new_status_val = args_cmd[2]
                            # Basic validation for status, can be expanded
                            valid_statuses = ["planning", "active", "completed", "on_hold", "archived"]
                            if new_status_val.lower() not in valid_statuses:
                                print_formatted_text(format_message("ERROR", f"Invalid status '{new_status_val}'. Valid statuses are: {', '.join(valid_statuses)}", CLIColors.ERROR_MESSAGE))
                                continue
                            project_manager.update_project_status(project_identifier, new_status_val.lower())
                        else:
                            print_formatted_text(format_message("ERROR", f"Unknown projects action: {action}", CLIColors.ERROR_MESSAGE))

                    elif command == "/suggestions":
                        if not args_cmd:
                            print_formatted_text(format_message("ERROR", "Usage: /suggestions <list|approve|deny|status> [id] [reason]", CLIColors.ERROR_MESSAGE))
                            continue

                        action = args_cmd[0].lower()
                        suggestion_id = args_cmd[1] if len(args_cmd) > 1 else None
                        reason = " ".join(args_cmd[2:]) if len(args_cmd) > 2 else None

                        if action == "list":
                            suggestions = suggestion_manager.list_suggestions()
                            if suggestions:
                                print_formatted_text(format_header("Suggestions List"))
                                for sug in suggestions:
                                    print_formatted_text(ANSI(f"- ID: {color_text(sug['suggestion_id'], CLIColors.COMMAND)}, Type: {sug['type']}, Status: {color_text(sug['status'], CLIColors.SYSTEM_MESSAGE)}"))
                                    print_formatted_text(ANSI(f"  Description: {color_text(sug['description'], CLIColors.SYSTEM_MESSAGE)}"))
                                    if sug.get('reason_for_status'):
                                        print_formatted_text(ANSI(f"  Reason: {color_text(sug['reason_for_status'], CLIColors.SYSTEM_MESSAGE)}"))
                            else:
                                print_formatted_text(format_message("INFO", "No suggestions found.", CLIColors.SYSTEM_MESSAGE))
                        elif action == "approve":
                            if not suggestion_id:
                                print_formatted_text(format_message("ERROR", "Usage: /suggestions approve <suggestion_id>", CLIColors.ERROR_MESSAGE))
                                continue
                            suggestion_manager.approve_suggestion(suggestion_id, reason)
                        elif action == "deny":
                            if not suggestion_id:
                                print_formatted_text(format_message("ERROR", "Usage: /suggestions deny <suggestion_id>", CLIColors.ERROR_MESSAGE))
                                continue
                            suggestion_manager.deny_suggestion(suggestion_id, reason)
                        elif action == "status":
                            print_formatted_text(format_header("Overall Suggestions Status"))
                            status_info = suggestion_manager.get_suggestions_summary_status()
                            print_formatted_text(ANSI(color_text(status_info, CLIColors.SYSTEM_MESSAGE)))
                        else:
                            print_formatted_text(format_message("ERROR", f"Unknown suggestions action: {action}", CLIColors.ERROR_MESSAGE))

                    elif command == "/status":
                        component = args_cmd[0].lower() if args_cmd else "all"
                        active_tasks_count = len(user_command_tasks) # Get current count

                        if component in ["tools", "all"]:
                            print_formatted_text(format_header("Tools Status"))
                            print_formatted_text(ANSI(color_text(status_reporting.get_tools_status(), CLIColors.SYSTEM_MESSAGE)))

                        if component in ["projects", "all"]:
                            print_formatted_text(format_header("Projects Status"))
                            print_formatted_text(ANSI(color_text(status_reporting.get_projects_status(), CLIColors.SYSTEM_MESSAGE)))

                        if component in ["suggestions", "all"]:
                            print_formatted_text(format_header("Suggestions Status"))
                            print_formatted_text(ANSI(color_text(status_reporting.get_suggestions_status(), CLIColors.SYSTEM_MESSAGE)))

                        if component in ["system", "all"]:
                            print_formatted_text(format_header("System Status"))
                            print_formatted_text(ANSI(color_text(status_reporting.get_system_status(active_tasks_count), CLIColors.SYSTEM_MESSAGE)))
                        
                        if component == "all": # Already handled by individual sections
                            pass

                        if component not in ["tools", "projects", "suggestions", "system", "all"]:
                            print_formatted_text(format_message("ERROR", f"Unknown status component: {component}", CLIColors.ERROR_MESSAGE))
                else: 
                    if is_debug_mode():
                        print_formatted_text(format_message("DEBUG", f"User input: {user_input}", CLIColors.DEBUG_MESSAGE))
                        
                    processed_as_confirmation = False
                    ai_response_for_learning = None

                    if _pending_tool_confirmation_details:
                        if is_debug_mode():
                            print_formatted_text(format_message("DEBUG", f"Pending confirmation: {_pending_tool_confirmation_details}", CLIColors.DEBUG_MESSAGE))
                            
                        pending_tool_name = _pending_tool_confirmation_details.get("tool_name")
                        if not pending_tool_name: 
                            if is_debug_mode():
                                print_formatted_text(ANSI(color_text(f"[DEBUG CLI] Pending confirmation details are incomplete (missing tool_name). Clearing.", CLIColors.ERROR_MESSAGE)))
                            _pending_tool_confirmation_details = None
                        elif user_input.lower() in ["yes", "y", "ok", "sure", "yeah", "yep"]:
                            if is_debug_mode():
                                print(color_text(f"[DEBUG CLI] User confirmed pending tool: {pending_tool_name}", CLIColors.DEBUG_MESSAGE))
                            
                            tool_to_run = pending_tool_name
                            inferred_args_list = _pending_tool_confirmation_details.get("inferred_args", [])
                            inferred_args_tuple = tuple(inferred_args_list) # type: ignore
                            inferred_kwargs_dict = _pending_tool_confirmation_details.get("inferred_kwargs", {}) # type: ignore
                            
                            try:
                                tool_result = await tool_system_instance.execute_tool(
                                    tool_to_run, 
                                    args=inferred_args_tuple, 
                                    kwargs=inferred_kwargs_dict
                                )
                                ai_response_for_learning = f"OK, I've run the '{tool_to_run}' tool. Result: {str(tool_result)[:500]}"
                                print_formatted_text(format_tool_execution(tool_to_run))
                                print_formatted_text(format_message("AI", ai_response_for_learning, CLIColors.AI_RESPONSE))
                                log_event(event_type="AI_TOOL_EXECUTION_RESPONSE", description=ai_response_for_learning, source="cli.handle_confirmation", metadata={"tool_name": tool_to_run, "user_input": user_input})
                            except Exception as e_exec: # pragma: no cover
                                ai_response_for_learning = f"Sorry, I encountered an error trying to run the '{tool_to_run}' tool: {e_exec}"
                                print_formatted_text(format_message("ERROR", ai_response_for_learning, CLIColors.ERROR_MESSAGE))
                                log_event(event_type="AI_TOOL_EXECUTION_FAILURE", description=ai_response_for_learning, source="cli.handle_confirmation", metadata={"tool_name": tool_to_run, "error": str(e_exec)})
                            processed_as_confirmation = True
                        elif user_input.lower() in ["no", "n", "nope", "cancel"]:
                            if is_debug_mode():
                                print_formatted_text(ANSI(color_text(f"[DEBUG CLI] User declined pending tool: {pending_tool_name}", CLIColors.DEBUG_MESSAGE)))
                            ai_response_for_learning = "Okay, I won't run that tool."
                            print_formatted_text(ANSI(color_text(f"AI: {ai_response_for_learning}", CLIColors.AI_RESPONSE)))
                            log_event(event_type="AI_TOOL_EXECUTION_DECLINED", description=ai_response_for_learning, source="cli.handle_confirmation", metadata={"tool_name": pending_tool_name})
                            processed_as_confirmation = True
                        
                        if processed_as_confirmation: 
                            _pending_tool_confirmation_details = None 
                            if is_debug_mode(): 
                                print_formatted_text(ANSI(color_text(f"[DEBUG CLI] Cleared _pending_tool_confirmation_details after yes/no.", CLIColors.DEBUG_MESSAGE)))
                            
                            if AUTONOMOUS_LEARNING_ENABLED and ai_response_for_learning:
                                learned_facts = await learn_facts_from_interaction(user_input, ai_response_for_learning, AUTONOMOUS_LEARNING_ENABLED)
                                if learned_facts:
                                    await _results_queue.put({
                                        "type": "learning_result",
                                        "facts": learned_facts,
                                        "original_prompt": f"Confirmation for '{pending_tool_name}' (User: {user_input})"
                                    })
                                    for fact in learned_facts:
                                        log_event(
                                            event_type="AUTONOMOUS_FACT_LEARNED",
                                            description=fact,
                                            source="autonomous_learning.learn_facts_from_interaction",
                                            metadata={"interaction_context": user_input[:50], "trigger": "tool_confirmation_flow"}
                                        )
                            continue # Go to next prompt cycle
                    
                    # If not processed as confirmation, it's a general command to be backgrounded
                    print_formatted_text(format_message("AI", f"Working on: '{user_input}'...", CLIColors.THINKING))
                    print_formatted_text(format_thinking())
                    if _orchestrator and _results_queue:
                        task = asyncio.create_task(
                            _process_command_wrapper(user_input, _orchestrator, _results_queue)
                        )
                        user_command_tasks.append(task)
                    else: # pragma: no cover
                        print_formatted_text(ANSI(color_text("Error: Orchestrator or results queue not initialized. Cannot process in background.", CLIColors.ERROR_MESSAGE)))
                    # Learning for these backgrounded tasks is now handled inside _process_command_wrapper
        finally:
            cli_running_event.clear() # Signal the results_processor_task to stop

            # Wait for the results processor to finish gracefully
            if results_processor_task:
                try:
                    # Allow it a moment to finish its current iteration and exit its loop
                    await asyncio.wait_for(results_processor_task, timeout=1.0)
                except asyncio.TimeoutError:
                    if not results_processor_task.done(): # pragma: no cover
                        results_processor_task.cancel() # Force cancel if it doesn't stop
                except asyncio.CancelledError: # pragma: no cover
                    pass # Expected if results_processor_task or main task was cancelled
                
                # Await it one last time to ensure it's fully cleaned up and to catch any exceptions
                try:
                    await results_processor_task
                except asyncio.CancelledError: # pragma: no cover
                    pass # Expected if it was cancelled

            # Cleanup for user-initiated command tasks
            if user_command_tasks:
                print_formatted_text(ANSI("\n"))
                print_formatted_text(draw_separator())
                print_formatted_text(format_message("SYSTEM", "Cleaning up pending user commands...", CLIColors.SYSTEM_MESSAGE))
                for task in user_command_tasks: # pragma: no cover
                    if not task.done(): # Only cancel tasks that are not yet done
                        task.cancel()
                # Wait for tasks to actually cancel or complete
                await asyncio.gather(*user_command_tasks, return_exceptions=True)
                print_formatted_text(format_status("User commands cleanup attempt complete", True))
            
            # Process any remaining results one last time after all tasks are handled
            await _handle_cli_results(_results_queue)
            
            # Save reflection log
            if global_reflection_log:
                global_reflection_log.save_log()
                print_formatted_text(format_status("Reflection log saved", True))

            # Final exit message
            print_formatted_text(draw_separator())
            print(format_message("GOODBYE", "AI Assistant shutting down. Have a great day!", CLIColors.SUCCESS))
            print_formatted_text(draw_separator())
            print_formatted_text(ANSI("\n"))


if __name__ == '__main__': # pragma: no cover
    try:
        asyncio.run(start_cli())
    except KeyboardInterrupt:
        print_formatted_text(ANSI(color_text("\nCLI terminated by user (KeyboardInterrupt in __main__).", CLIColors.SYSTEM_MESSAGE)))
    except Exception as e:
        print_formatted_text(ANSI(color_text(f"\nCLI terminated due to unexpected error: {e}", CLIColors.ERROR_MESSAGE)))
        traceback.print_exc()

# ### END FILE: ai_assistant/communication/cli.py ###

# ### START FILE: ai_assistant/config.py ###
# ai_assistant/config.py

# Default model to be used by the Ollama client if no specific model is requested for a task.
DEFAULT_MODEL = "qwen3:8B"  # Example default model, can be changed as needed
import os
from typing import Optional, Dict, List

# Define models that support native thinking
THINKING_SUPPORTED_MODELS: List[str] = [
    "qwen3:latest",
    "deepseek-r1:latest",
    "qwen3:8B"
]

# Enable or disable thinking capability globally (overrides per-model settings)
ENABLE_THINKING = True  # Set to False to disable thinking output globally

# Chain of thought settings for models that don't support native thinking
ENABLE_CHAIN_OF_THOUGHT = True  # Enable chain of thought prompting for non-thinking models
DEFAULT_TEMPERATURE_THINKING = 0.7  # Temperature for thinking phase
DEFAULT_TEMPERATURE_RESPONSE = 0.5  # Temperature for response phase (slightly lower for more focused responses)

# Thinking output configuration
THINKING_CONFIG = {
    "display": {
        "prefix": "[Thinking] ",
        "suffix": "...done thinking.",
        "plan_prefix": "[Action Plan] ",
        "step_prefix": "Step ",
        "max_steps": 5,
        "show_working": True,     # Show thinking steps in debug mode
        "show_in_release": False  # Never show thinking in release mode
    },
    "components": {
        "planner": "[Planner] ",
        "reviewer": "[Reviewer] ",
        "executor": "[Executor] ",
        "thinker": "[Thinker] "
    }
}

# Task-specific model configurations.
# This allows using different models for different capabilities (e.g., code generation, planning, reflection).
# If a task is not listed here, or if its value is None, the DEFAULT_MODEL will be used.
TASK_MODELS: Dict[str, Optional[str]] = {
    "code_generation": DEFAULT_MODEL,       # For generating new tool code via LLM
    "planning": DEFAULT_MODEL,              # For LLM-based planning
    "reflection": DEFAULT_MODEL,            # For LLM-based reflection, pattern identification, suggestion generation
    "conversation_intelligence": DEFAULT_MODEL, # For detecting missed tool opportunities, formulating descriptions
    "argument_population": DEFAULT_MODEL,   # For populating tool arguments from goal descriptions
    "goal_preprocessing": DEFAULT_MODEL,    # For preprocessing user goals
    "summarization": DEFAULT_MODEL,              # For summarizing text, e.g., search results
    "reviewing": DEFAULT_MODEL,                  # For reviewing AI's own suggestions/actions
    "fact_extraction": DEFAULT_MODEL,              # For extracting facts for autonomous learning
    "tool_design": DEFAULT_MODEL,                # For designing tool components (name, params, code) from a description
    "tool_creation": DEFAULT_MODEL,              # For the AI to create new tools
    # Add other tasks here as needed, e.g.:
    # "translation": "another_model:latest",
}

# Number of recent conversational turns (user/AI exchanges) to include in LLM prompts for context
CONVERSATION_HISTORY_TURNS = 5

# Number of seconds to wait before re-executing the project plan.
PROJECT_EXECUTION_INTERVAL_SECONDS = 150 

# Number of seconds to wait before running the background fact store curation.
FACT_CURATION_INTERVAL_SECONDS = 3600  # Default to 1 hour

# Enable or disable the AI's ability to autonomously learn facts from conversation.
AUTONOMOUS_LEARNING_ENABLED = True # MODIFIED FOR SCENARIO 5

# --- Fresh Start Configuration ---
# If True, the application should attempt to clear existing knowledge (context, memory, reflections, suggestions, learned facts etc.) on startup.
CLEAR_EXISTING_KNOWLEDGE_ON_STARTUP = False # Default to False to preserve data

# Debug mode flag (set in config file)
DEBUG_MODE = True

# --- Google Custom Search API Configuration ---
# IMPORTANT: These are sensitive keys.
# Option 1: Set them as environment variables (GOOGLE_API_KEY, GOOGLE_CSE_ID) - Recommended for production.
# Option 2: Set them here directly for local development. If you do, ensure this file is appropriately .gitignored or manage secrets carefully.
GOOGLE_API_KEY: Optional[str] = "AIzaSyCx8GehlHEy21kg156f3PON3mfCRAW60YI"
GOOGLE_CSE_ID: Optional[str] = "3354e92e98ab54b31"

def is_debug_mode() -> bool:
    """Returns True if debug mode is enabled in config."""
    return DEBUG_MODE

# Name of the directory to store data files like learned facts, logs, etc.
DATA_DIR_NAME = "data"
# Subdirectory within ai_assistant package where the DATA_DIR_NAME will be located.
CORE_SUBDIR_FOR_DATA = "core"
# Define a subdirectory within the main data_dir for projects
PROJECTS_SUBDIR = "projects"

def get_data_dir() -> str:
    """
    Returns the absolute path to the data directory (ai_assistant/core/data).
    Creates the directory if it does not exist.
    """
    # Assumes config.py is in the 'ai_assistant' directory.
    # os.path.dirname(__file__) is the 'ai_assistant' directory path
    ai_assistant_package_dir = os.path.dirname(__file__)
    data_path = os.path.abspath(os.path.join(ai_assistant_package_dir, CORE_SUBDIR_FOR_DATA, DATA_DIR_NAME))
    os.makedirs(data_path, exist_ok=True)
    return data_path

def get_projects_dir() -> str:
    """
    Returns the absolute path to the directory for storing project-related files.
    Creates the directory if it does not exist.
    """
    base_data_dir = get_data_dir()
    projects_path = os.path.join(base_data_dir, PROJECTS_SUBDIR)
    os.makedirs(projects_path, exist_ok=True)
    return projects_path

# Function to get the model name for a specific task, falling back to the default.
def get_model_for_task(task_name: str) -> str:
    """
    Retrieves the configured model for a given task, or the default model if not specified.
    
    Args:
        task_name (str): The name of the task (e.g., "code_generation", "planning").
        
    Returns:
        str: The name of the Ollama model to use.
    """
    return TASK_MODELS.get(task_name, DEFAULT_MODEL) or DEFAULT_MODEL

if __name__ == '__main__':
    print("--- Testing Configuration ---")

    # Test 1: Get default model
    default_model_retrieved = get_model_for_task("some_undefined_task")
    print(f"Model for 'some_undefined_task': {default_model_retrieved} (Expected: {DEFAULT_MODEL})")
    assert default_model_retrieved == DEFAULT_MODEL

    # Test 2: Get model for a defined task
    code_gen_model_retrieved = get_model_for_task("code_generation")
    expected_code_gen_model = TASK_MODELS.get("code_generation", DEFAULT_MODEL)
    print(f"Model for 'code_generation': {code_gen_model_retrieved} (Expected: {expected_code_gen_model})")
    assert code_gen_model_retrieved == expected_code_gen_model

    # Test 3: Get model for a task defined to use default (if any, or add one for test)
    # For this test, let's assume "planning" uses the default or is explicitly set to it.
    planning_model_retrieved = get_model_for_task("planning")
    expected_planning_model = TASK_MODELS.get("planning", DEFAULT_MODEL) # Should be DEFAULT_MODEL if not overridden
    print(f"Model for 'planning': {planning_model_retrieved} (Expected: {expected_planning_model})")
    assert planning_model_retrieved == expected_planning_model
    
    # Test new config value
    print(f"Fact curation interval: {FACT_CURATION_INTERVAL_SECONDS} (Expected: 3600)")
    assert FACT_CURATION_INTERVAL_SECONDS == 3600

    # Test new config value for clearing knowledge
    print(f"Clear existing knowledge on startup: {CLEAR_EXISTING_KNOWLEDGE_ON_STARTUP} (Expected: False)")
    assert CLEAR_EXISTING_KNOWLEDGE_ON_STARTUP == False # Assuming default is False


    # Test 4: Task where model is explicitly None in TASK_MODELS (should fallback to DEFAULT_MODEL)
    # Add a temporary entry for this test case
    TASK_MODELS["test_task_with_none_model"] = None
    model_for_none_task = get_model_for_task("test_task_with_none_model")
    print(f"Model for 'test_task_with_none_model' (set to None): {model_for_none_task} (Expected: {DEFAULT_MODEL})")
    assert model_for_none_task == DEFAULT_MODEL
    del TASK_MODELS["test_task_with_none_model"] # Clean up

    # Test 5: Get model for the new "summarization" task
    summarization_model_retrieved = get_model_for_task("summarization")
    expected_summarization_model = TASK_MODELS.get("summarization", DEFAULT_MODEL)
    print(f"Model for 'summarization': {summarization_model_retrieved} (Expected: {expected_summarization_model})")
    assert summarization_model_retrieved == expected_summarization_model

    # Test 6: Get model for the new "reviewing" task
    reviewing_model_retrieved = get_model_for_task("reviewing")
    expected_reviewing_model = TASK_MODELS.get("reviewing", DEFAULT_MODEL)
    print(f"Model for 'reviewing': {reviewing_model_retrieved} (Expected: {expected_reviewing_model})")
    assert reviewing_model_retrieved == expected_reviewing_model

    # Test 7: Get data directory
    data_dir = get_data_dir()
    print(f"Data directory: {data_dir}")
    assert os.path.exists(data_dir)
    assert os.path.basename(data_dir) == DATA_DIR_NAME

    # Test 8: Get projects directory
    projects_dir = get_projects_dir()
    print(f"Projects directory: {projects_dir}")
    assert os.path.exists(projects_dir)
    assert os.path.basename(projects_dir) == PROJECTS_SUBDIR

    print("--- Configuration Tests Passed ---")

# ### END FILE: ai_assistant/config.py ###

# ### START FILE: ai_assistant/core/__init__.py ###
# This file makes the 'core' directory a Python package.
# ### END FILE: ai_assistant/core/__init__.py ###

# ### START FILE: ai_assistant/core/autonomous_reflection.py ###
"""
This module implements the AI assistant's self-reflection and autonomous
improvement capabilities. It analyzes past operational logs to identify
recurring failure patterns or areas for enhancement.

Based on these identified patterns, it generates specific improvement suggestions.
Crucially, each suggestion is then evaluated by an LLM to assign quantitative
scores for its potential Impact, associated Risk, and estimated Effort (each on
a 1-5 scale).

These scores are embedded into the suggestion objects. The module then provides
functionality to select a high-priority suggestion for potential autonomous action,
considering these scores alongside the suggestion's action type and details.
This allows the assistant to make informed decisions about which self-improvement
tasks to undertake.
"""
import json 
from typing import List, Dict, Any, Optional
import re # Added for robust JSON extraction
import logging # Added for explicit logger usage
from ai_assistant.llm_interface.ollama_client import invoke_ollama_model 
from ai_assistant.core.reflection import global_reflection_log, ReflectionLogEntry 
from ..memory.event_logger import log_event # Adjusted for consistency, though original might work if core is in path
from ai_assistant.config import get_model_for_task, is_debug_mode # Removed review_reflection_suggestion import
from ai_assistant.learning.evolution import apply_code_modification # Added import
from datetime import datetime, timezone, timedelta 

logger = logging.getLogger(__name__) # Ensure logger is defined for the module

# Define a threshold for what constitutes "enough" entries for analysis
DEFAULT_MIN_ENTRIES_FOR_ANALYSIS = 5 
DEFAULT_MAX_ENTRIES_TO_FETCH = 50    

IDENTIFY_FAILURE_PATTERNS_PROMPT_TEMPLATE = """
You are an AI assistant analyzing a summary of your own past operational reflection logs. Your task is to identify recurring failure patterns, problematic tools or goals, and other insights that could lead to self-improvement.

Here is the summary of recent reflection log entries:
---
{reflection_log_summary}
---

Based on this summary, identify and list any significant patterns or specific issues. Focus on:
1.  **Frequently Failing Tools**: Are there any tools that appear in multiple failed or partially successful plans? Note the tool name and the errors associated with it if possible.
2.  **Common Error Types**: Are there specific error messages or error types (e.g., TypeError, ValueError, ToolNotFoundError) that recur, perhaps with certain tools or types of goals?
3.  **Problematic Goal Types**: Do certain kinds of goals (e.g., goals involving complex calculations, specific external interactions) frequently lead to failures or partial successes?
4.  **Retries & Reliability**: Are there tools or goals that often succeed only after retries? This might indicate unreliability or sensitivity.
5.  **Other Notable Observations**: Any other patterns or anomalies you observe.
6.  **Effectiveness of Past Self-Modifications**: Review any entries explicitly marked as 'SELF-MODIFICATION ATTEMPT' in the log summary. Note whether these modifications were successful (e.g., passed tests, were committed) and if they appear to have resolved prior issues or inadvertently introduced new ones. Include observations about the efficacy of these attempts in your findings if significant patterns emerge (e.g., if a type of self-modification often fails its tests).

Please provide your findings as a JSON object containing a single key "identified_patterns", which is a list of observation objects. Each observation object should detail the pattern and provide brief evidence or examples from the log summary (e.g., including "pattern_type", "details", and "related_entries" as keys).

Example JSON Output Format:
{{
  "identified_patterns": [
    {{
      "pattern_type": "FREQUENTLY_FAILING_TOOL",
      "tool_name": "tool_B",
      "details": "Tool 'tool_B' appeared in 2 failure entries (Entry 2, Entry 5) with errors like 'TypeError' and 'ConnectionTimeout'.",
      "related_entries": ["Entry 2", "Entry 5"]
    }}
  ]
}}

If no significant patterns are found, return an empty list for "identified_patterns".
Focus on clear, data-driven observations based *only* on the provided log summary. Respond ONLY with the JSON object.
"""

GENERATE_IMPROVEMENT_SUGGESTIONS_PROMPT_TEMPLATE = """
You are an AI assistant tasked with generating self-improvement suggestions based on an analysis of your operational patterns. You have been provided with a JSON list of identified issues and patterns from your reflection logs. You also have a list of your currently available tools.

Identified Patterns (JSON list):
---
{identified_patterns_json_list_str}
---

Available Tools (JSON - Name: Description):
---
{available_tools_json_str}
---

Based on the "Identified Patterns" and your "Available Tools":
Generate a list of specific, actionable improvement suggestions. For each suggestion, indicate which pattern(s) it addresses.
Suggestions could include (but are not limited to):
- Modifying the code of an existing tool (e.g., adding error handling, improving input validation, enhancing functionality).
- Adjusting a tool's description for clarity if it seems to be misunderstood or misused.
- Deprecating a problematic tool if a better alternative exists or can be easily created.
- Suggesting the creation of a new tool if a clear gap in capabilities is identified.
- Modifying internal prompts or planning strategies for certain types of goals.
- Alerting a human developer to a complex issue that requires manual intervention.

If `action_type` involves specific parameters (e.g., for `UPDATE_TOOL_DESCRIPTION`, the `tool_name` and `new_description`), include these in a nested 'action_details' object.

Please provide your suggestions as a JSON object containing a single key "improvement_suggestions", which is a list of suggestion objects. Each suggestion object should have:
- "suggestion_id": A unique identifier for the suggestion (e.g., "SUG_001").
- "suggestion_text": The detailed description of the improvement.
- "addresses_patterns": A list of identifiers or descriptions of the patterns it addresses (e.g., references to `pattern_type` and `tool_name` from the input patterns).
- "priority": A suggested priority (e.g., "High", "Medium", "Low") based on perceived impact and urgency.
- "action_type": A proposed type of action (e.g., "MODIFY_TOOL_CODE", "CREATE_NEW_TOOL", "UPDATE_TOOL_DESCRIPTION", "CHANGE_PLANNING_LOGIC", "MANUAL_REVIEW_NEEDED").
- "action_details": (Conditionally present based on action_type) A nested JSON object containing specific parameters needed for the action.
    - For "UPDATE_TOOL_DESCRIPTION": {{"tool_name": "tool_to_update", "new_description": "The new description."}}
    - For "CREATE_NEW_TOOL": {{"tool_description_prompt": "A concise description of the new tool's functionality, suitable for a code generation model.", "suggested_tool_name": "suggested_python_function_name_for_tool"}}
    - For "MODIFY_TOOL_CODE": {{
        "module_path": "path.to.your.module", 
        "function_name": "function_to_modify", 
        "suggested_code_change": "def function_to_modify(param1, param2):\n    # New, complete function code here\n    return result",
        "original_code_snippet": "(Optional) Few lines of the original code for context, if available and relevant for your suggestion.",
        "suggested_change_description": "Detailed textual description of what was changed and why, suitable for a commit message body."
      }}
      (Instruction to LLM: For MODIFY_TOOL_CODE, 'module_path', 'function_name', and 'suggested_code_change' (the new complete function source code) are mandatory. 'original_code_snippet' is optional. 'suggested_change_description' is for the commit message.)

Example JSON Output Format:
{{
  "improvement_suggestions": [
    {{
      "suggestion_id": "SUG_001",
      "suggestion_text": "Add robust input validation to the 'tool_B' function to handle potential TypeErrors.",
      "addresses_patterns": ["FREQUENTLY_FAILING_TOOL: tool_B"],
      "priority": "High",
      "action_type": "MODIFY_TOOL_CODE",
      "action_details": {{
        "module_path": "ai_assistant.custom_tools.tool_utils",
        "function_name": "tool_B",
        "suggested_code_change": "def tool_B(param1):\n    try:\n        # Improved logic with validation\n        num = int(param1)\n        if num == 0:\n            return 'Error: Division by zero not allowed.'\n        return 100 / num\n    except ValueError:\n        return 'Error: Invalid input, expected an integer.'\n    except Exception as e:\n        return f'An unexpected error occurred: {{str(e)}}'",
        "original_code_snippet": "def tool_B(param1):\n    return 100 / param1 # Original potentially unsafe code",
        "suggested_change_description": "Implemented try-except block to handle ValueError for non-integer inputs and check for zero division. This addresses recurrent TypeErrors and potential ZeroDivisionErrors observed in logs."
      }}
    }},
    {{
      "suggestion_id": "SUG_002",
      "suggestion_text": "Clarify the description of 'tool_C' to mention it only accepts positive integers.",
      "addresses_patterns": ["MISUNDERSTOOD_TOOL_USAGE: tool_C"],
      "priority": "Medium",
      "action_type": "UPDATE_TOOL_DESCRIPTION",
      "action_details": {{
        "tool_name": "tool_C",
        "new_description": "This tool performs action X and only accepts positive integers as input."
      }}
    }},
    {{
      "suggestion_id": "SUG_003",
      "suggestion_text": "Identified a recurring need for calculating differences between dates. Suggest creating a new tool for this.",
      "addresses_patterns": ["Problematic Goal Type: Date Calculations", "User query re: date math"],
      "priority": "High",
      "action_type": "CREATE_NEW_TOOL",
      "action_details": {{
        "tool_description_prompt": "A Python function that takes two date strings (e.g., 'YYYY-MM-DD') as input and returns the difference between them in days as an integer. It should handle basic date parsing errors.",
        "suggested_tool_name": "calculate_date_difference"
      }}
    }}
  ]
}}

If no actionable suggestions can be derived from the patterns, return an empty list for "improvement_suggestions".
Focus on practical and impactful suggestions. Respond ONLY with the JSON object.
"""

LLM_REVIEW_IMPROVEMENT_SUGGESTION_PROMPT_TEMPLATE = """
You are an AI assistant acting as a meta-reviewer. Your task is to critically evaluate an *internally generated improvement suggestion* for the AI system itself.

**Improvement Suggestion to Review:**
- Suggestion ID: {suggestion_id}
- Suggestion Text: {suggestion_text}
- Addresses Patterns: {addresses_patterns}
- Priority (Original): {priority}
- Proposed Action Type: {action_type}
- Proposed Action Details (JSON):
  ```json
  {action_details_json_str}
  ```
- Initial Scores: Impact={impact_score}, Risk={risk_score}, Effort={effort_score}

**Review Criteria:**
1.  **Clarity & Actionability**: Is the suggestion clear, specific, and actionable?
2.  **Relevance**: Does the suggestion directly address the identified patterns?
3.  **Appropriateness of Action**: Is the proposed `action_type` and `action_details` suitable for the suggestion?
    - For `MODIFY_TOOL_CODE`: Are `module_path`, `function_name`, and `suggested_code_change` (the complete new function code) present and plausible? Is `suggested_change_description` adequate for a commit message?
    - For `CREATE_NEW_TOOL`: Is `tool_description_prompt` clear enough for a code generation LLM? Is `suggested_tool_name` Pythonic?
    - For `UPDATE_TOOL_DESCRIPTION`: Are `tool_name` and `new_description` present and sensible?
4.  **Potential Impact vs. Risk/Effort**: Considering the initial scores (Impact, Risk, Effort), does this seem like a worthwhile improvement to pursue?
5.  **Overall Soundness**: Does the suggestion make sense? Are there any obvious flaws or better alternatives?

**Output Structure:**
You *MUST* respond with a single JSON object. Do not include any other text or explanations before or after the JSON object.
The JSON object must contain the following keys:
-   `"review_looks_good"`: Boolean - `true` if the suggestion is generally sound and worth considering for action, `false` otherwise.
-   `"qualitative_review"`: String - A concise textual summary of your review, highlighting strengths and weaknesses.
-   `"confidence_score"`: Float (0.0 to 1.0) - Your confidence that this suggestion, if implemented as proposed, will lead to a net positive outcome.
-   `"suggested_modifications_to_proposal"`: String (Optional) - If the suggestion is promising but could be improved (e.g., clearer action details, different action type), describe the modifications here. If none, use an empty string or omit.

Now, please review the provided improvement suggestion.
"""

EVALUATE_IMPROVEMENT_SUGGESTION_PROMPT_TEMPLATE = """
You are an AI assistant evaluating a proposed improvement suggestion for a software system. Your task is to assess the suggestion based on Impact, Risk, and Effort, each on a scale of 1 to 5.

**Suggestion Details:**
- Suggestion: {suggestion_text}
- Action Type: {suggestion_action_type}
- Action Details (JSON): {suggestion_action_details_json_str}

**Evaluation Criteria:**

1.  **Impact Score (1-5):** How significant is the potential positive effect if this suggestion is implemented successfully?
    - 1: Very Low (Minimal or negligible improvement)
    - 2: Low (Slight improvement, noticeable but not major)
    - 3: Medium (Moderate improvement, clearly beneficial)
    - 4: High (Significant improvement, substantial benefits)
    - 5: Very High (Transformative improvement, game-changing)

2.  **Risk Score (1-5):** What is the potential for negative consequences, or how difficult would it be if the implementation fails or introduces new problems?
    - 1: Very Low (Minimal chance of issues, easy to revert)
    - 2: Low (Slight chance of minor issues, manageable)
    - 3: Medium (Moderate chance of noticeable issues, requires effort to fix)
    - 4: High (Significant chance of major issues, difficult to resolve)
    - 5: Very High (Almost certain to cause critical problems, very hard to recover)

3.  **Effort Score (1-5):** How much work or resources (time, complexity, dependencies) are estimated to be required to implement this suggestion?
    - 1: Very Low (Trivial change, can be done in minutes/hours)
    - 2: Low (Minor change, a few hours to a day)
    - 3: Medium (Moderate change, a few days of work)
    - 4: High (Significant change, a week or more, complex)
    - 5: Very High (Major undertaking, weeks/months, many dependencies)

Based on your assessment of the suggestion against these criteria, provide your evaluation *only* as a JSON object with the following three keys: "impact_score", "risk_score", and "effort_score". The values for these keys must be integers between 1 and 5.

Example JSON Output Format:
{{
  "impact_score": 4,
  "risk_score": 2,
  "effort_score": 3
}}

Respond ONLY with the JSON object.
"""


def get_reflection_log_summary_for_analysis(
    max_entries: int = DEFAULT_MAX_ENTRIES_TO_FETCH,
    min_entries_for_analysis: int = DEFAULT_MIN_ENTRIES_FOR_ANALYSIS
) -> Optional[str]:
    """
    Retrieves recent reflection log entries and formats them into a summary string
    suitable for LLM analysis to identify patterns or suggest improvements.

    Args:
        max_entries: The maximum number of recent log entries to fetch.
        min_entries_for_analysis: Minimum entries required to proceed with analysis.

    Returns:
        A formatted string summarizing relevant log entries, or None if
        there are not enough entries for meaningful analysis.
    """
    entries: List[ReflectionLogEntry] = global_reflection_log.get_entries(limit=max_entries)

    if len(entries) < min_entries_for_analysis:
        print(f"Info: Not enough reflection log entries ({len(entries)}) for analysis. Minimum required: {min_entries_for_analysis}.")
        return None

    formatted_summary_parts: List[str] = ["Recent Reflection Log Summary for Analysis:\n"]
    relevant_entry_count = 0

    for i, entry in enumerate(entries): 
        entry_details = []
        entry_details.append(f"Entry {relevant_entry_count + 1} (Timestamp: {entry.timestamp.strftime('%Y-%m-%d %H:%M:%S UTC')})")
        entry_details.append(f"  Goal: {entry.goal_description}")
        entry_details.append(f"  Status: {entry.status}")

        if entry.error_type or entry.error_message:
            entry_details.append(f"  Error: {entry.error_type} - {entry.error_message}")
        
        if entry.notes:
            entry_details.append(f"  Notes: {entry.notes}")

        if entry.plan:
            plan_steps_summary = []
            for step_idx, step in enumerate(entry.plan):
                tool_name = step.get('tool_name', 'N/A')
                args_preview = str(step.get('args', 'N/A'))[:50] 
                step_result_preview = ""
                if entry.execution_results and step_idx < len(entry.execution_results):
                    res = entry.execution_results[step_idx]
                    if isinstance(res, Exception):
                        step_result_preview = f" -> Failed: {type(res).__name__}"
                plan_steps_summary.append(f"    Step {step_idx + 1}: Tool: {tool_name}, Args: {args_preview}{step_result_preview}")
            
            if plan_steps_summary:
                entry_details.append("  Plan:")
                entry_details.extend(plan_steps_summary)

        # Add self-modification details if applicable
        if entry.is_self_modification_attempt:
            entry_details.append("  --- SELF-MODIFICATION ATTEMPT ---")
            if entry.source_suggestion_id:
                entry_details.append(f"    Source Suggestion ID: {entry.source_suggestion_id}")
            if entry.modification_type:
                entry_details.append(f"    Modification Type: {entry.modification_type}")
            
            test_outcome_str = "N/A"
            if entry.post_modification_test_passed is True:
                test_outcome_str = "PASSED"
            elif entry.post_modification_test_passed is False:
                test_outcome_str = "FAILED"
            entry_details.append(f"    Test Outcome: {test_outcome_str}")

            if entry.post_modification_test_details and isinstance(entry.post_modification_test_details, dict):
                test_notes = entry.post_modification_test_details.get('notes', '')
                entry_details.append(f"    Test Notes: {test_notes[:100]}{'...' if len(test_notes) > 100 else ''}")
            
            commit_status_str = "N/A"
            if entry.commit_info and isinstance(entry.commit_info, dict):
                commit_success = entry.commit_info.get('status') # Assuming 'status' key from previous structure
                commit_msg_snippet = str(entry.commit_info.get('message', ''))[:50] # 'message' key for commit message
                commit_err_snippet = str(entry.commit_info.get('error', ''))[:50]

                if commit_success is True:
                    commit_status_str = f"Committed (Msg: {commit_msg_snippet}{'...' if len(commit_msg_snippet) == 50 else ''})"
                elif commit_success is False:
                    commit_status_str = f"Commit FAILED ({commit_err_snippet}{'...' if len(commit_err_snippet) == 50 else ''})"
                else: # Status is None or not present
                    commit_status_str = f"Commit status unknown (Info: {commit_msg_snippet}{'...' if len(commit_msg_snippet) == 50 else ''})"
            entry_details.append(f"    Commit Status: {commit_status_str}")
            entry_details.append("  ---------------------------------")
        
        formatted_summary_parts.append("\n".join(entry_details))
        relevant_entry_count += 1
    
    if relevant_entry_count == 0 :
        return "No relevant reflection log entries found for analysis based on current criteria."

    return "\n\n".join(formatted_summary_parts)

def _invoke_pattern_identification_llm(log_summary_str: str, llm_model_name: Optional[str] = None) -> Optional[Dict[str, Any]]:
    model_to_use = llm_model_name if llm_model_name is not None else get_model_for_task("reflection")
    prompt = IDENTIFY_FAILURE_PATTERNS_PROMPT_TEMPLATE.format(reflection_log_summary=log_summary_str)
    llm_response_str = invoke_ollama_model(prompt, model_name=model_to_use)

    if not llm_response_str:
        logger.warning(f"Received no response from LLM ({model_to_use}) for pattern identification.")
        return None
    
    # More robust JSON extraction for objects
    json_match = re.search(r"```json\s*(\{[\s\S]*?\})\s*```", llm_response_str, re.DOTALL)
    if json_match:
        cleaned_response = json_match.group(1).strip()
    else:
        # Fallback to find first '{' and last '}'
        first_brace = llm_response_str.find('{')
        last_brace = llm_response_str.rfind('}')
        if first_brace != -1 and last_brace != -1 and last_brace > first_brace:
            cleaned_response = llm_response_str[first_brace : last_brace+1].strip()
        else:
            cleaned_response = llm_response_str.strip() # Use as is if no clear JSON object found
    
    try:
        data = json.loads(cleaned_response)
        if not isinstance(data, dict):
            logger.warning(f"LLM response for pattern identification was not a dictionary. Response: {cleaned_response}")
            return None
        if "identified_patterns" not in data or not isinstance(data["identified_patterns"], list):
            logger.warning(f"LLM response for pattern identification missing 'identified_patterns' list or incorrect type. Response: {cleaned_response}")
            return None
        return data
    except json.JSONDecodeError as e:
        logger.error(f"Error decoding JSON from pattern identification LLM: {e}. Raw response snippet:\n---\n{llm_response_str[:1000]}...\n---")
        return None

def _invoke_suggestion_generation_llm(identified_patterns_json_list_str: str, available_tools_json_str: str, llm_model_name: Optional[str] = None) -> Optional[Dict[str, Any]]:
    model_to_use = llm_model_name if llm_model_name is not None else get_model_for_task("reflection")
    prompt = GENERATE_IMPROVEMENT_SUGGESTIONS_PROMPT_TEMPLATE.format(
        identified_patterns_json_list_str=identified_patterns_json_list_str,
        available_tools_json_str=available_tools_json_str
    )
    llm_response_str = invoke_ollama_model(prompt, model_name=model_to_use)

    if not llm_response_str:
        logger.warning(f"Received no response from LLM ({model_to_use}) for suggestion generation.")
        return None
        
    # More robust JSON extraction for objects
    json_match = re.search(r"```json\s*(\{[\s\S]*?\})\s*```", llm_response_str, re.DOTALL)
    if json_match:
        cleaned_response = json_match.group(1).strip()
    else:
        first_brace = llm_response_str.find('{')
        last_brace = llm_response_str.rfind('}')
        if first_brace != -1 and last_brace != -1 and last_brace > first_brace:
            cleaned_response = llm_response_str[first_brace : last_brace+1].strip()
        else:
            cleaned_response = llm_response_str.strip()

    try:
        data = json.loads(cleaned_response)
        if not isinstance(data, dict):
            logger.warning(f"LLM response for suggestion generation was not a dictionary. Response: {cleaned_response}")
            return None
        if "improvement_suggestions" not in data or not isinstance(data["improvement_suggestions"], list):
            logger.warning(f"LLM response for suggestion generation missing 'improvement_suggestions' list or incorrect type. Response: {cleaned_response}")
            return None
        return data
    except json.JSONDecodeError as e:
        logger.error(f"Error decoding JSON from suggestion generation LLM: {e}. Raw response snippet:\n---\n{llm_response_str[:1000]}...\n---")
        return None

def _invoke_suggestion_scoring_llm(suggestion: Dict[str, Any], llm_model_name: Optional[str] = None) -> Optional[Dict[str, int]]:
    """
    Invokes an LLM to score an improvement suggestion based on impact, risk, and effort.

    Args:
        suggestion: A dictionary containing the suggestion details.
                    Expected keys: "suggestion_text", "action_type", and optionally "action_details".
        llm_model_name: The name of the Ollama model to use.

    Returns:
        A dictionary containing "impact_score", "risk_score", and "effort_score" as integers,
        or None if the LLM call, JSON parsing, or validation fails.
    """
    suggestion_text = suggestion.get("suggestion_text", "")
    action_type = suggestion.get("action_type", "")
    action_details = suggestion.get("action_details")

    try:
        if action_details is None:
            action_details_json_str = "{}"
        else:
            action_details_json_str = json.dumps(action_details)
    except TypeError as e:
        logger.warning(f"Could not serialize action_details to JSON for suggestion scoring. Error: {e}. Details: {action_details}")
        action_details_json_str = "{}"

    prompt = EVALUATE_IMPROVEMENT_SUGGESTION_PROMPT_TEMPLATE.format(
        suggestion_text=suggestion_text,
        suggestion_action_type=action_type,
        suggestion_action_details_json_str=action_details_json_str
    )

    model_to_use = llm_model_name if llm_model_name is not None else get_model_for_task("reflection")
    llm_response_str = invoke_ollama_model(prompt, model_name=model_to_use)

    if not llm_response_str:
        logger.warning(f"Received no response from LLM for suggestion scoring (model: {model_to_use}). Suggestion ID: {suggestion.get('suggestion_id', 'N/A')}")
        return None

    # More robust JSON extraction for objects
    json_match = re.search(r"```json\s*(\{[\s\S]*?\})\s*```", llm_response_str, re.DOTALL)
    if json_match:
        cleaned_response = json_match.group(1).strip()
    else:
        first_brace = llm_response_str.find('{')
        last_brace = llm_response_str.rfind('}')
        if first_brace != -1 and last_brace != -1 and last_brace > first_brace:
            cleaned_response = llm_response_str[first_brace : last_brace+1].strip()
        else:
            cleaned_response = llm_response_str.strip()

    try:
        data = json.loads(cleaned_response)
        if not isinstance(data, dict):
            logger.warning(f"LLM response for suggestion scoring was not a dictionary. Response: {cleaned_response}")
            return None

        required_keys = ["impact_score", "risk_score", "effort_score"]
        for key in required_keys:
            if key not in data:
                logger.warning(f"LLM response for suggestion scoring missing key '{key}'. Response: {cleaned_response}")
                return None
            if not isinstance(data[key], int):
                logger.warning(f"LLM response for suggestion scoring key '{key}' is not an integer. Value: {data[key]}. Response: {cleaned_response}")
                return None
        
        # Optional: could add range validation here (1-5) if strictly needed by callers immediately.
        # For now, type and presence are the primary validation.

        return {
            "impact_score": data["impact_score"],
            "risk_score": data["risk_score"],
            "effort_score": data["effort_score"],
        }
    except json.JSONDecodeError as e:
        logger.error(f"Error decoding JSON from suggestion scoring LLM: {e}. Raw response snippet:\n---\n{llm_response_str[:1000]}...\n---")
        return None
    except Exception as e: # Catch any other unexpected errors during validation
        logger.error(f"An unexpected error occurred during suggestion scoring validation: {e}. Response: {cleaned_response}")
        return None

def _invoke_suggestion_review_llm(suggestion: Dict[str, Any], llm_model_name: Optional[str] = None) -> Optional[Dict[str, Any]]:
    """
    Invokes an LLM to review an improvement suggestion.
    Helper for review_reflection_suggestion.
    """
    suggestion_id = suggestion.get("suggestion_id", "N/A")
    suggestion_text = suggestion.get("suggestion_text", "")
    addresses_patterns = suggestion.get("addresses_patterns", [])
    priority = suggestion.get("priority", "N/A")
    action_type = suggestion.get("action_type", "N/A")
    action_details = suggestion.get("action_details", {})
    impact_score = suggestion.get("impact_score", "N/A")
    risk_score = suggestion.get("risk_score", "N/A")
    effort_score = suggestion.get("effort_score", "N/A")

    try:
        action_details_json_str = json.dumps(action_details)
    except TypeError:
        action_details_json_str = str(action_details) # Fallback

    prompt = LLM_REVIEW_IMPROVEMENT_SUGGESTION_PROMPT_TEMPLATE.format(
        suggestion_id=suggestion_id,
        suggestion_text=suggestion_text,
        addresses_patterns=str(addresses_patterns), # Convert list to string for prompt
        priority=priority,
        action_type=action_type,
        action_details_json_str=action_details_json_str,
        impact_score=impact_score,
        risk_score=risk_score,
        effort_score=effort_score
    )

    model_to_use = llm_model_name if llm_model_name is not None else get_model_for_task("reflection") # Or a new "suggestion_review" task type
    llm_response_str = invoke_ollama_model(prompt, model_name=model_to_use)

    if not llm_response_str:
        logger.warning(f"Received no response from LLM for suggestion review (model: {model_to_use}). Suggestion ID: {suggestion_id}")
        return None

    # More robust JSON extraction for objects
    json_match = re.search(r"```json\s*(\{[\s\S]*?\})\s*```", llm_response_str, re.DOTALL)
    if json_match:
        cleaned_response = json_match.group(1).strip()
    else:
        first_brace = llm_response_str.find('{')
        last_brace = llm_response_str.rfind('}')
        if first_brace != -1 and last_brace != -1 and last_brace > first_brace:
            cleaned_response = llm_response_str[first_brace : last_brace+1].strip()
        else:
            cleaned_response = llm_response_str.strip()
            
    try:
        data = json.loads(cleaned_response)
        if not isinstance(data, dict) or \
           "review_looks_good" not in data or not isinstance(data["review_looks_good"], bool) or \
           "qualitative_review" not in data or not isinstance(data["qualitative_review"], str) or \
           "confidence_score" not in data or not isinstance(data["confidence_score"], float):
            logger.warning(f"LLM response for suggestion review has missing/invalid keys. Response: {cleaned_response}")
            return None
        return data
    except json.JSONDecodeError as e:
        logger.error(f"Error decoding JSON from suggestion review LLM: {e}. Raw response snippet:\n---\n{llm_response_str[:1000]}...\n---")
        return None
    except Exception as e:
        logger.error(f"An unexpected error occurred during suggestion review LLM response processing: {e}. Raw response snippet:\n---\n{llm_response_str[:1000]}...\n---")
        return None

def run_self_reflection_cycle(
    available_tools: Dict[str, str], 
    llm_model_name: Optional[str] = None, # Changed default to None
    max_log_entries: int = DEFAULT_MAX_ENTRIES_TO_FETCH, 
    min_entries_for_analysis: int = DEFAULT_MIN_ENTRIES_FOR_ANALYSIS 
) -> Optional[List[Dict[str, Any]]]:
    """
    Runs a full self-reflection cycle:
    1. Retrieves and summarizes reflection log entries.
    2. Invokes an LLM to identify patterns from the summary.
    3. Invokes an LLM to generate improvement suggestions based on patterns and available tools.
    4. Invokes an LLM to score each generated suggestion for impact, risk, and effort.
    5. Embeds these scores (impact_score, risk_score, effort_score) into each suggestion dictionary.
       If scoring fails for a suggestion, error values (-1) are assigned for its scores.

    Args:
        available_tools: A dictionary of available tools (name: description) for the AI.
        llm_model_name: The name of the Ollama model to use for all LLM invocations.
        max_log_entries: Max number of recent log entries to fetch for analysis.
        min_entries_for_analysis: Min log entries required to proceed with analysis.

    Returns:
        A list of suggestion dictionaries, each augmented with "impact_score",
        "risk_score", and "effort_score" keys. Returns None if a critical step
        (like log summary or pattern identification) fails, or an empty list
        if no suggestions are generated.
    """
    logger.info("\n--- Starting Self-Reflection Cycle ---") # Changed to logger
    log_event(
        event_type="AUTONOMOUS_REFLECTION_CYCLE_STARTED",
        description="Self-reflection cycle initiated.",
        source="autonomous_reflection.run_self_reflection_cycle",
        metadata={"max_log_entries": max_log_entries, "min_entries_for_analysis": min_entries_for_analysis}
    )
    
    log_summary = get_reflection_log_summary_for_analysis(
        max_entries=max_log_entries, 
        min_entries_for_analysis=min_entries_for_analysis
    )
    if not log_summary:
        logger.info("Self-Reflection Cycle: Aborted due to insufficient log data or no relevant entries found.") # Changed to logger
        log_event(
            event_type="AUTONOMOUS_REFLECTION_CYCLE_ABORTED",
            description="Self-reflection cycle aborted: Insufficient log data.",
            source="autonomous_reflection.run_self_reflection_cycle",
            metadata={"reason": "Insufficient log data from get_reflection_log_summary_for_analysis"}
        )
        return None

    if is_debug_mode():
        logger.debug(f"Reflection log summary for analysis: {log_summary}") # Changed to logger

    logger.info("Self-Reflection Cycle: Identifying failure patterns from log summary...") # Changed to logger
    # Pass llm_model_name to _invoke_pattern_identification_llm if it's provided, otherwise it will use the default from config
    patterns_data = _invoke_pattern_identification_llm(log_summary, llm_model_name=llm_model_name) 
    
    if not patterns_data: 
        logger.warning("Self-Reflection Cycle: Could not identify any significant patterns (LLM call failed or invalid format).") # Changed to logger
        log_event(
            event_type="AUTONOMOUS_REFLECTION_PATTERN_ID_FAILED",
            description="Pattern identification failed or returned invalid format from LLM.",
            source="autonomous_reflection.run_self_reflection_cycle",
            metadata={"llm_model_name": llm_model_name or get_model_for_task("reflection")} # Log actual model used
        )
        return None
        
    identified_patterns_list = patterns_data.get("identified_patterns")
    if identified_patterns_list is None: 
        logger.warning("Self-Reflection Cycle: 'identified_patterns' key missing in LLM response for patterns.") # Changed to logger
        log_event(
            event_type="AUTONOMOUS_REFLECTION_PATTERN_ID_ERROR",
            description="'identified_patterns' key missing in LLM response.",
            source="autonomous_reflection.run_self_reflection_cycle",
            metadata={"llm_model_name": llm_model_name or get_model_for_task("reflection"), "response_preview": str(patterns_data)[:200]}
        )
        return None
    
    log_event(
        event_type="AUTONOMOUS_REFLECTION_PATTERNS_IDENTIFIED",
        description=f"Pattern identification complete. Found {len(identified_patterns_list)} pattern(s).",
        source="autonomous_reflection.run_self_reflection_cycle",
        metadata={"num_patterns": len(identified_patterns_list), "patterns_preview": identified_patterns_list[:3], "model_used": llm_model_name or get_model_for_task("reflection")}
    )

    if not identified_patterns_list: 
        logger.info("Self-Reflection Cycle: No specific patterns were identified by the LLM.") # Changed to logger
        pass 

    logger.info(f"Self-Reflection Cycle: Identified {len(identified_patterns_list)} pattern(s). Generating improvement suggestions...") # Changed to logger

    try:
        patterns_json_list_str = json.dumps(identified_patterns_list, indent=2)
        available_tools_json_str = json.dumps(available_tools, indent=2)
    except TypeError as e:
        logger.error(f"Error serializing patterns or tools to JSON for suggestion generation: {e}") # Changed to logger
        log_event(
            event_type="AUTONOMOUS_REFLECTION_SERIALIZATION_ERROR",
            description="Error serializing patterns or tools to JSON.",
            source="autonomous_reflection.run_self_reflection_cycle",
            metadata={"error": str(e)}
        )
        return None

    suggestions_data = _invoke_suggestion_generation_llm(
        patterns_json_list_str, 
        available_tools_json_str, 
        llm_model_name=llm_model_name # Pass through llm_model_name
    )

    if not suggestions_data: 
        logger.warning("Self-Reflection Cycle: Could not generate improvement suggestions (LLM call failed or invalid format).") # Changed to logger
        log_event(
            event_type="AUTONOMOUS_REFLECTION_SUGGESTION_GEN_FAILED",
            description="Suggestion generation failed or returned invalid format from LLM.",
            source="autonomous_reflection.run_self_reflection_cycle",
            metadata={"llm_model_name": llm_model_name or get_model_for_task("reflection"), "num_patterns_input": len(identified_patterns_list)}
        )
        return None
        
    final_suggestions = suggestions_data.get("improvement_suggestions")
    if final_suggestions is None: 
        logger.warning("Self-Reflection Cycle: 'improvement_suggestions' key missing in LLM response for suggestions.") # Changed to logger
        log_event(
            event_type="AUTONOMOUS_REFLECTION_SUGGESTION_GEN_ERROR",
            description="'improvement_suggestions' key missing in LLM response.",
            source="autonomous_reflection.run_self_reflection_cycle",
            metadata={"llm_model_name": llm_model_name or get_model_for_task("reflection"), "response_preview": str(suggestions_data)[:200]}
        )
        return None
    
    log_event(
        event_type="AUTONOMOUS_REFLECTION_SUGGESTIONS_GENERATED",
        description=f"Suggestion generation complete. Generated {len(final_suggestions)} suggestion(s).",
        source="autonomous_reflection.run_self_reflection_cycle",
        metadata={"num_suggestions": len(final_suggestions), "suggestions_preview": final_suggestions[:3], "model_used": llm_model_name or get_model_for_task("reflection")} 
    )
    
    if not final_suggestions: 
        logger.info("Self-Reflection Cycle: No improvement suggestions were generated by the LLM.") # Changed to logger
        # No suggestions to score, so we can pass, and the function will return the empty list or None.
    else:
        logger.info(f"Self-Reflection Cycle: Generated {len(final_suggestions)} improvement suggestion(s). Scoring them now...") # Changed to logger
        for suggestion in final_suggestions:
            if not isinstance(suggestion, dict): # Should not happen with current generation logic
                logger.warning(f"Skipping scoring for an invalid suggestion item: {suggestion}") # Changed to logger
                continue

            scores = _invoke_suggestion_scoring_llm(suggestion, llm_model_name=llm_model_name) # Pass through llm_model_name
            if scores:
                suggestion["impact_score"] = scores.get("impact_score")
                suggestion["risk_score"] = scores.get("risk_score")
                suggestion["effort_score"] = scores.get("effort_score")
                # print(f"DEBUG: Scored suggestion {suggestion.get('suggestion_id', 'N/A')}: Impact={scores.get('impact_score')}, Risk={scores.get('risk_score')}, Effort={scores.get('effort_score')}")
            else:
                logger.warning(f"Failed to score suggestion ID: {suggestion.get('suggestion_id', 'Unknown ID')}. Assigning default error scores (-1).") # Changed to logger
                suggestion["impact_score"] = -1
                suggestion["risk_score"] = -1
                suggestion["effort_score"] = -1
        logger.info(f"Self-Reflection Cycle: Scoring completed for {len(final_suggestions)} suggestions.") # Changed to logger

        # --- Add Review Step ---
        logger.info(f"Self-Reflection Cycle: Reviewing {len(final_suggestions)} scored suggestion(s)...") # Changed to logger
        for suggestion in final_suggestions:
            if not isinstance(suggestion, dict): continue

            review_data = _invoke_suggestion_review_llm(suggestion, llm_model_name=llm_model_name)
            if review_data:
                suggestion["review_looks_good"] = review_data.get("review_looks_good")
                suggestion["qualitative_review"] = review_data.get("qualitative_review")
                suggestion["reviewer_confidence"] = review_data.get("confidence_score") # Use a distinct key
                suggestion["reviewer_modifications"] = review_data.get("suggested_modifications_to_proposal") # Match key from prompt
            else:
                logger.warning(f"Failed to review suggestion ID: {suggestion.get('suggestion_id', 'Unknown ID')}. Assigning default review error values.")
                suggestion["review_looks_good"] = False # Default to False if review fails
                suggestion["qualitative_review"] = "Review process failed."
                suggestion["reviewer_confidence"] = 0.0
                suggestion["reviewer_modifications"] = ""
        logger.info(f"Self-Reflection Cycle: Reviewing completed for {len(final_suggestions)} suggestions.") # Changed to logger


    logger.info("--- Self-Reflection Cycle Finished ---") # Changed to logger
    log_event(
        event_type="AUTONOMOUS_REFLECTION_CYCLE_COMPLETED",
        description=f"Self-reflection cycle finished. Produced {len(final_suggestions) if final_suggestions is not None else 0} suggestions, attempted scoring and review.",
        source="autonomous_reflection.run_self_reflection_cycle",
        metadata={"num_suggestions_produced": len(final_suggestions) if final_suggestions is not None else 0}
    )
    return final_suggestions

def select_suggestion_for_autonomous_action(
    suggestions: List[Dict[str, Any]],
    supported_action_types: Optional[List[str]] = None,
    # Add tool_system if needed by future action types, not currently used directly by MODIFY_TOOL_CODE handler
) -> Optional[Dict[str, Any]]:
    """
    Selects a suitable suggestion for an initial phase of autonomous action based on
    calculated priority and validation of action details.

    The selection process involves:
    1. Filtering suggestions by `supported_action_types`.
    2. Filtering out suggestions that lack valid `impact_score`, `risk_score`, and
       `effort_score` (e.g., if scoring failed and they have -1 values).
    3. Calculating a `_priority_score` for each valid suggestion using the formula:
       `impact_score - risk_score - (effort_score * 0.5)`.
       This prioritizes suggestions that are impactful, lower-risk, and require
       reasonably low effort.
    4. Sorting suggestions by this `_priority_score` in descending order.
    5. Iterating through the sorted list and selecting the first suggestion that
       passes validation of its `action_details` (e.g., required fields for
       the specific `action_type`).

    Args:
        suggestions: A list of suggestion dictionaries, typically from
                     `run_self_reflection_cycle`, which should include
                     `impact_score`, `risk_score`, `effort_score`, and `review_looks_good`.
        supported_action_types: A list of `action_type` strings to consider.
                                 Defaults to include "UPDATE_TOOL_DESCRIPTION", 
                                 "CREATE_NEW_TOOL", and "MODIFY_TOOL_CODE".

    Returns:
        The selected suggestion dictionary if a suitable and valid one is found
        and successfully actioned (or attempt was made for MODIFY_TOOL_CODE),
        otherwise None.
    """
    if supported_action_types is None: 
        supported_action_types = ["UPDATE_TOOL_DESCRIPTION", "CREATE_NEW_TOOL", "MODIFY_TOOL_CODE"]

    if not suggestions:
        logger.debug("No suggestions provided to select_suggestion_for_autonomous_action.")
        return None

    # 1. Filter by Supported Action Types
    actionable_suggestions = [s for s in suggestions if s.get("action_type") in supported_action_types]
    if not actionable_suggestions:
        logger.debug(f"No suggestions match supported action types: {supported_action_types}")
        return None
    
    # 2. Filter out Suggestions with Failed Scoring
    valid_scored_suggestions = []
    for s in actionable_suggestions:
        impact = s.get("impact_score")
        risk = s.get("risk_score")
        effort = s.get("effort_score")
        
        if isinstance(impact, int) and impact != -1 and \
           isinstance(risk, int) and risk != -1 and \
           isinstance(effort, int) and effort != -1:
            valid_scored_suggestions.append(s)
        else:
            logger.debug(f"Suggestion {s.get('suggestion_id', 'N/A')} filtered out due to missing/failed I/R/E scores (Impact: {impact}, Risk: {risk}, Effort: {effort}).")
            
    if not valid_scored_suggestions:
        logger.debug("No suggestions remaining after filtering for valid I/R/E scores.")
        return None

    # 3. Filter by Review Outcome
    reviewed_and_approved_suggestions = []
    for s in valid_scored_suggestions:
        # Consider suggestion if review_looks_good is True and confidence is above a threshold
        # Example threshold: 0.6
        if s.get("review_looks_good") is True and s.get("reviewer_confidence", 0.0) >= 0.6:
            reviewed_and_approved_suggestions.append(s)
        else:
            logger.debug(f"Suggestion {s.get('suggestion_id', 'N/A')} filtered out due to review_looks_good ({s.get('review_looks_good')}) or low confidence ({s.get('reviewer_confidence', 0.0)}). Review: '{s.get('qualitative_review', 'N/A')}'")

    if not reviewed_and_approved_suggestions:
        logger.debug("No suggestions remaining after filtering by review outcome and confidence.")
        return None

    logger.debug(f"{len(reviewed_and_approved_suggestions)} suggestions remaining after review filter.")

    # 4. Calculate a Priority Metric
    for s in reviewed_and_approved_suggestions:
        impact_score = s["impact_score"] # Already validated
        risk_score = s["risk_score"]
        effort_score = s["effort_score"]
        
        s["_priority_score"] = impact_score - risk_score - (effort_score * 0.5)
        logger.debug(f"Suggestion {s.get('suggestion_id', 'N/A')} (Action: {s.get('action_type')}) calculated priority_score: {s['_priority_score']} (I:{impact_score}, R:{risk_score}, E:{effort_score}) Reviewer Confidence: {s.get('reviewer_confidence', 'N/A')}")

    # 5. Sort Suggestions
    sorted_suggestions = sorted(reviewed_and_approved_suggestions, key=lambda x: x["_priority_score"], reverse=True)
    
    logger.debug(f"{len(sorted_suggestions)} suggestions sorted by priority_score.")
    if sorted_suggestions:
        logger.debug(f"Top sorted suggestion ID {sorted_suggestions[0].get('suggestion_id', 'N/A')} with score {sorted_suggestions[0]['_priority_score']}")

    # 6. Select the Best Valid Suggestion and attempt action
    for suggestion in sorted_suggestions:
        action_type = suggestion.get("action_type")
        action_details = suggestion.get("action_details") # This is the dict for apply_code_modification

        # Clean up temporary score before returning or further processing
        priority_score_for_log = suggestion.pop("_priority_score", None)

        if action_type == "UPDATE_TOOL_DESCRIPTION":
            if isinstance(action_details, dict) and \
               isinstance(action_details.get("tool_name"), str) and action_details.get("tool_name") and \
               action_details.get("new_description") is not None and isinstance(action_details.get("new_description"), str):
                logger.info(f"Selected suggestion ID {suggestion.get('suggestion_id', 'N/A')} (Update Tool Desc) with priority score {priority_score_for_log}.")
                # Actual update logic would be called here if this function did more than selection.
                # For now, returning the selected suggestion is the "action".
                return suggestion # Actionable suggestion found
        
        elif action_type == "CREATE_NEW_TOOL":
            if isinstance(action_details, dict) and \
               isinstance(action_details.get("tool_description_prompt"), str) and action_details.get("tool_description_prompt"):
                logger.info(f"Selected suggestion ID {suggestion.get('suggestion_id', 'N/A')} (Create New Tool) with priority score {priority_score_for_log}.")
                # Actual tool creation logic would be called here.
                return suggestion # Actionable suggestion found

        elif action_type == "MODIFY_TOOL_CODE":
            if isinstance(action_details, dict) and \
               isinstance(action_details.get("module_path"), str) and action_details.get("module_path") and \
               isinstance(action_details.get("function_name"), str) and action_details.get("function_name") and \
               isinstance(action_details.get("suggested_code_change"), str) and action_details.get("suggested_code_change"):
                
                # Prepare the dictionary for apply_code_modification
                # It expects "module_path", "function_name", "suggested_code_change" directly.
                # The 'action_details' from the suggestion should directly map to this.
                code_mod_params = {
                    "module_path": action_details["module_path"],
                    "function_name": action_details["function_name"],
                    "suggested_code_change": action_details["suggested_code_change"]
                }

                logger.info(f"Attempting to apply code modification for tool '{code_mod_params['function_name']}' "+
                            f"in module '{code_mod_params['module_path']}' based on suggestion "+
                            f"{suggestion.get('suggestion_id', 'N/A')} (Priority: {priority_score_for_log}).")
                
                # apply_code_modification now returns a detailed dictionary
                code_mod_result = apply_code_modification(code_mod_params) 
                
                overall_success_from_apply = code_mod_result['overall_status']
                detailed_message_from_apply = code_mod_result['overall_message']
                
                test_outcome_details = code_mod_result.get('test_outcome')
                test_passed_for_log = test_outcome_details.get('passed') if test_outcome_details else None
                
                commit_outcome_details = code_mod_result.get('commit_outcome')
                commit_info_for_log = None
                if commit_outcome_details:
                    commit_info_for_log = {
                        "message": commit_outcome_details.get("commit_message_generated"),
                        "status": commit_outcome_details.get("status"),
                        "error": commit_outcome_details.get("error_message")
                    }

                modification_details_for_log_dict = {
                    "module": code_mod_params["module_path"],
                    "function": code_mod_params["function_name"],
                    # Using the original suggestion's action_details for the 'why' part
                    "suggested_change_description": suggestion.get("action_details", {}).get("suggested_change_description"),
                    "original_code_snippet": suggestion.get("action_details", {}).get("original_code_snippet"),
                    # Storing the actual code change applied might be too verbose for this log entry,
                    # but could be logged elsewhere or inferred from commit if needed.
                    # "applied_code_change": code_mod_params["suggested_code_change"] 
                }

                # Log the detailed self-modification attempt to ReflectionLog
                global_reflection_log.log_execution(
                    goal_description=f"Self-modification attempt for suggestion {suggestion.get('suggestion_id', 'N/A')}",
                    plan=[{
                        "tool_name": "apply_code_modification", 
                        "args": [code_mod_params], # Be mindful of logging sensitive data if code_mod_params contains raw code
                        "status": "attempted"
                    }],
                    execution_results=[code_mod_result], # Log the entire detailed result from apply_code_modification
                    overall_success=overall_success_from_apply,
                    notes=detailed_message_from_apply,
                    is_self_modification_attempt=True,
                    source_suggestion_id=suggestion.get('suggestion_id'),
                    modification_type="MODIFY_TOOL_CODE",
                    modification_details=modification_details_for_log_dict,
                    post_modification_test_passed=test_passed_for_log,
                    post_modification_test_details=test_outcome_details, # Log the full test outcome
                    commit_info=commit_info_for_log
                )
                
                # Update the event log with more details
                logger.info(detailed_message_from_apply) # The overall message from apply_code_modification is now the main log
                log_event(
                    event_type="AUTONOMOUS_ACTION_MODIFY_TOOL_CODE_ATTEMPT",
                    description=detailed_message_from_apply, # Use the detailed message
                    source="autonomous_reflection.select_suggestion_for_autonomous_action",
                    metadata={
                        "suggestion_id": suggestion.get("suggestion_id"),
                        "tool_name": code_mod_params['function_name'],
                        "module_path": code_mod_params['module_path'],
                        "overall_outcome_success": overall_success_from_apply,
                        "edit_status": code_mod_result.get("edit_outcome", {}).get("status"),
                        "test_status": test_passed_for_log,
                        "revert_status": code_mod_result.get("revert_outcome", {}).get("status"),
                        "commit_status": commit_info_for_log.get("status") if commit_info_for_log else None,
                        "priority_score": priority_score_for_log # Keep this for selection context
                    }
                )
                # This suggestion is considered "actioned" regardless of success/failure of the modification itself.
                return suggestion # Return the original suggestion that was actioned.
            else:
                logger.warning(f"Skipping MODIFY_TOOL_CODE suggestion {suggestion.get('suggestion_id', 'N/A')} due to missing/invalid action_details: {action_details}")
        
    logger.debug("No suggestion passed action_details validation or other criteria after sorting by priority_score.")
    return None

if __name__ == '__main__':
    # Setup basic logging for the test run if not already configured by resilience.py or other imports
    if not logging.getLogger().handlers:
        logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    print("--- Testing Reflection Log Analysis Function ---")
    # Mock data for testing select_suggestion_for_autonomous_action
    # Ensure this mock data includes impact_score, risk_score, effort_score, and review_looks_good
    mock_suggestions_for_select_test = [
        { # Valid MODIFY_TOOL_CODE
            "suggestion_id": "MTC001", "action_type": "MODIFY_TOOL_CODE", "priority": "High",
            "impact_score": 4, "risk_score": 1, "effort_score": 2, "review_looks_good": True, # review_looks_good is True
            "action_details": {
                "module_path": "ai_assistant.tools.sample_tool", 
                "function_name": "do_something", 
                "suggested_code_change": "def do_something(new_param):\n  pass"
            }
        },
        { # Valid UPDATE_TOOL_DESCRIPTION
            "suggestion_id": "UTD001", "action_type": "UPDATE_TOOL_DESCRIPTION", "priority": "Medium", 
            "impact_score": 3, "risk_score": 1, "effort_score": 1, "review_looks_good": True,
            "action_details": {"tool_name": "tool_A", "new_description": "New desc for A"}
        },
        { # Invalid MODIFY_TOOL_CODE (missing suggested_code_change)
            "suggestion_id": "MTC002", "action_type": "MODIFY_TOOL_CODE", "priority": "High",
            "impact_score": 4, "risk_score": 2, "effort_score": 2, "review_looks_good": True,
            "action_details": {"module_path": "ai_assistant.tools.another_tool", "function_name": "another_func"}
        },
        { # Valid CREATE_NEW_TOOL
            "suggestion_id": "CNT001", "action_type": "CREATE_NEW_TOOL", "priority": "High",
            "impact_score": 5, "risk_score": 2, "effort_score": 3, "review_looks_good": True,
            "action_details": {"tool_description_prompt": "A tool to do X."}
        },
        { # MODIFY_TOOL_CODE with review_looks_good = False
            "suggestion_id": "MTC003_REJECTED", "action_type": "MODIFY_TOOL_CODE", "priority": "High",
            "impact_score": 5, "risk_score": 1, "effort_score": 1, "review_looks_good": False, 
            "qualitative_review": "Reviewer found potential issues.",
            "action_details": {
                "module_path": "ai_assistant.tools.rejected_tool", 
                "function_name": "rejected_func", 
                "suggested_code_change": "def rejected_func():\n  # risky change\n  pass"
            }
        }
    ]

    from unittest.mock import patch, MagicMock

    # Test 1: Select MODIFY_TOOL_CODE suggestion
    # We need to mock 'apply_code_modification'
    with patch('ai_assistant.learning.evolution.apply_code_modification') as mock_apply_code:
        mock_apply_code.return_value = True # Simulate successful application
        
        # Use a logger that captures messages for assertion
        test_logger = logging.getLogger('ai_assistant.core.autonomous_reflection')
        # If using Python 3.10+, can use assertLogs context manager more easily.
        # For now, simple check of called_once_with for apply_code_modification
        
        selected_mtc = select_suggestion_for_autonomous_action(
            mock_suggestions_for_select_test, 
            supported_action_types=["MODIFY_TOOL_CODE", "UPDATE_TOOL_DESCRIPTION", "CREATE_NEW_TOOL"]
        )
        
        if selected_mtc:
            print(f"Selected suggestion (MTC Test): {selected_mtc.get('suggestion_id')}")
            assert selected_mtc.get('suggestion_id') == "MTC001", f"Expected MTC001, got {selected_mtc.get('suggestion_id')}"
            
            expected_call_params = {
                "module_path": "ai_assistant.tools.sample_tool", 
                "function_name": "do_something", 
                "suggested_code_change": "def do_something(new_param):\n  pass"
            }
            mock_apply_code.assert_called_once_with(expected_call_params)
            # Add log assertion here if capturing logs
        else:
            print("No suggestion selected for MTC test (unexpected).")
            assert False, "Expected MTC001 to be selected and processed."

    # Test 2: MODIFY_TOOL_CODE suggestion fails application
    with patch('ai_assistant.learning.evolution.apply_code_modification') as mock_apply_code_fail:
        mock_apply_code_fail.return_value = False # Simulate failed application
        
        selected_mtc_fail = select_suggestion_for_autonomous_action(
            [mock_suggestions_for_select_test[0]], # Only the MTC001 suggestion
            supported_action_types=["MODIFY_TOOL_CODE"]
        )
        if selected_mtc_fail:
            print(f"Selected suggestion (MTC Fail Test): {selected_mtc_fail.get('suggestion_id')}")
            assert selected_mtc_fail.get('suggestion_id') == "MTC001"
            mock_apply_code_fail.assert_called_once() 
            # Add log assertion here for failure message
        else:
            assert False, "Expected MTC001 to be selected even if application fails, for logging."
            
    # Test 3: Ensure MTC003_REJECTED is not selected due to review_looks_good=False
    with patch('ai_assistant.learning.evolution.apply_code_modification') as mock_apply_code_rejected:
        suggestions_for_rejected_test = [
             mock_suggestions_for_select_test[4], # MTC003_REJECTED (review_looks_good=False)
             mock_suggestions_for_select_test[1]  # UTD001 (review_looks_good=True, lower priority than MTC003 if it were good)
        ]
        selected_rejected = select_suggestion_for_autonomous_action(
            suggestions_for_rejected_test,
            supported_action_types=["MODIFY_TOOL_CODE", "UPDATE_TOOL_DESCRIPTION"]
        )
        if selected_rejected:
            print(f"Selected suggestion (Rejected Test): {selected_rejected.get('suggestion_id')}")
            assert selected_rejected.get('suggestion_id') == "UTD001", "Expected UTD001 to be selected, not the rejected MTC003."
            mock_apply_code_rejected.assert_not_called() # MTC003 should not have been attempted
        else:
            assert False, "Expected UTD001 to be selected in the rejected test."
            
    print("\n--- select_suggestion_for_autonomous_action tests complete ---")

# ### END FILE: ai_assistant/core/autonomous_reflection.py ###

# ### START FILE: ai_assistant/core/background_service.py ###
### START FILE: core/background_service.py ###
# ai_assistant/core/background_service.py
import asyncio
import time
import json
import os # Added
import re
import logging
from typing import Optional, List

from ai_assistant.core.autonomous_reflection import run_self_reflection_cycle
from ai_assistant.tools import tool_system # To get available tools
# Modified: Import the specific curation function and config for interval
from ai_assistant.custom_tools.knowledge_tools import run_periodic_fact_store_curation_async
from ai_assistant.config import is_debug_mode, FACT_CURATION_INTERVAL_SECONDS

# Configure logger for this module
logger = logging.getLogger(__name__)

# Import for project execution task
try:
    from ai_assistant.custom_tools.file_system_tools import BASE_PROJECTS_DIR
    from ai_assistant.custom_tools.project_execution_tools import execute_project_coding_plan
    PROJECT_TOOLS_AVAILABLE = True
except ImportError as e: # pragma: no cover
    print(f"BackgroundService: Warning - Could not import project execution tools. Autonomous project work will be disabled. Error: {e}")
    PROJECT_TOOLS_AVAILABLE = False
    # Define placeholders if imports fail, so the rest of the module doesn't break
    BASE_PROJECTS_DIR = "ai_generated_projects" 
    def read_text_from_file(filepath: str) -> str: return f"Error: Tool not available due to import failure for {filepath}"
    def sanitize_project_name(name: str) -> str: return name
    async def execute_project_coding_plan(project_name: str, base_projects_dir_override: Optional[str] = None) -> str:
        return "Error: execute_project_coding_plan tool not available due to import failure."


# Fallback for config if not defined
try:
    from ai_assistant.config import PROJECT_EXECUTION_INTERVAL_SECONDS
except ImportError: # pragma: no cover
    PROJECT_EXECUTION_INTERVAL_SECONDS = 720 # Default to 12 minutes if not in config

# --- Service State ---
_background_service_active = False
_background_task: Optional[asyncio.Task] = None
_polling_interval_seconds = 300  # For self-reflection # FACT_CURATION_INTERVAL_SECONDS will be used from config
_last_fact_curation_time: float = 0.0
_last_project_execution_scan_time: float = 0.0 # New state for project execution

def sanitize_project_name(name: str) -> str:  # Changed 'project_name' to 'name'
    """
    Sanitizes a project name to create a safe directory name.
    # ... (rest of docstring) ...
    Args:
        name: The raw project name string. # Changed 'project_name' to 'name'
    # ... (rest of function body, ensure 'name' is used instead of 'project_name') ...
    """
    if not name or not name.strip(): # Use 'name'
        return "unnamed_project"

    s_name = name.lower() # Use 'name'
    s_name = re.sub(r'\s+', '_', s_name)
    s_name = re.sub(r'-+', '_', s_name)
    s_name = re.sub(r'[^\w-]', '', s_name)
    s_name = re.sub(r'_+', '_', s_name)

    if not s_name:
        return "unnamed_project"
    
    return s_name[:50]

def write_text_to_file(filepath: str, content: str) -> str: # Changed 'full_filepath' to 'filepath'
    """
    Writes the given text content to the specified file.
    # ... (rest of docstring) ...
    Args:
        filepath: The absolute or relative path to the file. # Changed 'full_filepath' to 'filepath'
    # ... (rest of function body, ensure 'filepath' is used instead of 'full_filepath') ...
    """
    if not filepath or not isinstance(filepath, str): # Use 'filepath'
        return "Error: Filepath must be a non-empty string."
    # ... (ensure all internal uses of 'full_filepath' are changed to 'filepath')
    try:
        dir_path = os.path.dirname(filepath) # Use 'filepath'
        if dir_path: 
            os.makedirs(dir_path, exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f: # Use 'filepath'
            f.write(content)
        return f"Success: Content written to '{filepath}'." # Use 'filepath'
    except IOError as e:
        return f"Error writing to file '{filepath}': {e} (IOError)" # Use 'filepath'
    # ... (and so on for other error messages) ...

def read_text_from_file(filepath: str) -> str: # Changed 'full_filepath' to 'filepath'
    """
    Reads and returns the text content from the specified file.
    # ... (rest of docstring) ...
    Args:
        filepath: The absolute or relative path to the file. # Changed 'full_filepath' to 'filepath'
    # ... (rest of function body, ensure 'filepath' is used instead of 'full_filepath') ...
    """
    if not filepath or not isinstance(filepath, str): # Use 'filepath'
        return "Error: Filepath must be a non-empty string."

    if not os.path.exists(filepath): # Use 'filepath'
         return f"Error: File '{filepath}' not found." # Use 'filepath'
    
    if not os.path.isfile(filepath): # Use 'filepath'
        return f"Error: Path '{filepath}' is not a file." # Use 'filepath'

    try:
        with open(filepath, 'r', encoding='utf-8') as f: # Use 'filepath'
            content = f.read()
        return content
    except IOError as e:
        return f"Error reading file '{filepath}': {e} (IOError)"

# --- Asyncio Version ---
async def _background_loop_async():
    global _last_fact_curation_time, _last_project_execution_scan_time
    print("BackgroundService: Async loop started.")
    _last_fact_curation_time = time.time()
    _last_project_execution_scan_time = time.time()

    next_reflection_run_time = time.time() + _polling_interval_seconds
    next_fact_curation_run_time = time.time() + FACT_CURATION_INTERVAL_SECONDS # Use config value
    next_project_execution_run_time = time.time() + PROJECT_EXECUTION_INTERVAL_SECONDS


    while _background_service_active:
        current_loop_time = time.time()
        
        # --- Self-Reflection Task ---
        if current_loop_time >= next_reflection_run_time:
            current_time_str_reflection = time.strftime('%Y-%m-%d %H:%M:%S') # No need for to_thread for time.strftime
            logger.info(f"BackgroundService: Running self-reflection cycle (current time: {current_time_str_reflection})...")
            try:
                available_tools = await asyncio.to_thread(tool_system.tool_system_instance.list_tools)
                if not available_tools: # pragma: no cover
                    logger.info("BackgroundService: No tools available for reflection cycle. Skipping self-reflection.")
                else:
                    suggestions = await asyncio.to_thread(run_self_reflection_cycle, available_tools=available_tools)
                    if suggestions: # pragma: no cover
                        logger.info(f"BackgroundService: Self-reflection cycle generated {len(suggestions)} suggestions.")
                    elif suggestions == []: # pragma: no cover
                        logger.info("BackgroundService: Self-reflection cycle generated no suggestions.")
                    else: 
                        logger.info("BackgroundService: Self-reflection cycle did not complete or was aborted (e.g. not enough log data).")
            except Exception as e: # pragma: no cover
                logger.error(f"BackgroundService: Error during self-reflection cycle: {e}", exc_info=True)
            next_reflection_run_time = time.time() + _polling_interval_seconds

        # --- LLM-Powered Fact Curation Task ---
        if current_loop_time >= next_fact_curation_run_time:
            current_time_str_curation = time.strftime('%Y-%m-%d %H:%M:%S')
            logger.info(f"BackgroundService: Running LLM fact curation (current time: {current_time_str_curation})...")
            try:
                # Call the dedicated function from knowledge_tools
                curation_success = await run_periodic_fact_store_curation_async()
                if curation_success: # pragma: no cover
                    logger.info("BackgroundService: LLM fact curation process completed successfully.")
                else: # pragma: no cover
                    logger.warning("BackgroundService: LLM fact curation process encountered an issue or made no changes.")
            except Exception as e: # pragma: no cover
                logger.error(f"BackgroundService: Error during LLM fact curation: {e}", exc_info=True)
            _last_fact_curation_time = time.time()
            next_fact_curation_run_time = time.time() + FACT_CURATION_INTERVAL_SECONDS # Use config value
        
        # --- Autonomous Project Coding Task ---
        if PROJECT_TOOLS_AVAILABLE and current_loop_time >= next_project_execution_run_time:
            current_time_str_project_exec = await asyncio.to_thread(time.strftime, '%Y-%m-%d %H:%M:%S')
            print(f"BackgroundService (Async): Scanning for projects with planned tasks (current time: {current_time_str_project_exec})...")
            projects_worked_on_this_cycle = 0
            try:
                if not os.path.isdir(BASE_PROJECTS_DIR): # pragma: no cover
                    logger.info(f"BackgroundService: Projects directory '{BASE_PROJECTS_DIR}' does not exist. Skipping project execution scan.")
                else:
                    for project_sanitized_name in os.listdir(BASE_PROJECTS_DIR):
                        project_dir_path = os.path.join(BASE_PROJECTS_DIR, project_sanitized_name)
                        if os.path.isdir(project_dir_path):
                            manifest_path = os.path.join(project_dir_path, "_ai_project_manifest.json")
                            if os.path.exists(manifest_path):
                                manifest_content_str = read_text_from_file(manifest_path)
                                if manifest_content_str.startswith("Error:"): # pragma: no cover
                                    logger.warning(f"BackgroundService: Error reading manifest for {project_sanitized_name}: {manifest_content_str}")
                                    continue
                                try:
                                    manifest_data = json.loads(manifest_content_str)
                                    # Ensure project_name is derived correctly, it might not be the sanitized name
                                    original_project_name = manifest_data.get("project_name", project_sanitized_name) 
                                    
                                    # Check for planned tasks more accurately
                                    # The manifest schema stores tasks in 'development_tasks'
                                    development_tasks = manifest_data.get("development_tasks", [])
                                    has_planned_tasks = False
                                    if isinstance(development_tasks, list):
                                        for task in development_tasks:
                                            if isinstance(task, dict) and task.get("status") == "planned":
                                                has_planned_tasks = True
                                                break
                                    
                                    if has_planned_tasks:
                                        logger.info(f"BackgroundService: Project '{original_project_name}' has planned tasks. Attempting to execute coding plan.")
                                        # Pass the BASE_PROJECTS_DIR to ensure execute_project_coding_plan uses the correct root
                                        # if it doesn't inherit it via its own imports of file_system_tools.
                                        exec_result = await execute_project_coding_plan(original_project_name, base_projects_dir_override=BASE_PROJECTS_DIR)
                                        logger.info(f"BackgroundService: Result for '{original_project_name}':\n{exec_result}")
                                        projects_worked_on_this_cycle += 1
                                    else:
                                        if is_debug_mode(): # pragma: no cover
                                            logger.debug(f"[DEBUG BACKGROUND_SERVICE] Project '{original_project_name}' has no 'planned' development tasks in its manifest.")
                                except json.JSONDecodeError: # pragma: no cover
                                    logger.error(f"BackgroundService: Error decoding manifest JSON for {project_sanitized_name}.", exc_info=True)
                                except Exception as e_proj_scan: # pragma: no cover
                                    logger.error(f"BackgroundService: Error processing project {project_sanitized_name}: {e_proj_scan}", exc_info=True)
                if projects_worked_on_this_cycle == 0 and is_debug_mode(): # pragma: no cover
                    logger.debug(f"[DEBUG BACKGROUND_SERVICE] No projects found with pending tasks in this scan.")

            except Exception as e: # pragma: no cover
                logger.error(f"BackgroundService: Error during autonomous project execution scan: {e}", exc_info=True)
            _last_project_execution_scan_time = time.time()
            next_project_execution_run_time = time.time() + PROJECT_EXECUTION_INTERVAL_SECONDS
        
        # Determine sleep time until the next event
        time_until_next_reflection = max(0, next_reflection_run_time - time.time())
        time_until_next_curation = max(0, next_fact_curation_run_time - time.time())
        time_until_next_project_exec = max(0, next_project_execution_run_time - time.time()) if PROJECT_TOOLS_AVAILABLE else float('inf')
        
        sleep_duration = min(time_until_next_reflection, time_until_next_curation, time_until_next_project_exec, 10)

        try:
            if is_debug_mode(): # pragma: no cover
                debug_msg_parts = [f"Sleeping for {sleep_duration:.2f}s."]
                debug_msg_parts.append(f"Next reflection in {time_until_next_reflection:.0f}s")
                debug_msg_parts.append(f"next curation in {time_until_next_curation:.0f}s")
                if PROJECT_TOOLS_AVAILABLE:
                    debug_msg_parts.append(f"next project exec scan in {time_until_next_project_exec:.0f}s")
                logger.debug(f"[DEBUG BACKGROUND_SERVICE] {', '.join(debug_msg_parts)}.")
            await asyncio.sleep(sleep_duration)
        except asyncio.CancelledError: # pragma: no cover
            logger.info("BackgroundService: Loop cancelled during sleep.")
            break 
            
    logger.info("BackgroundService: Async loop finished.")

# Renamed and made synchronous as it just creates a task
def start_background_services():
    global _background_service_active, _background_task, _last_fact_curation_time, _last_project_execution_scan_time
    # Ensure is_debug_mode is available or imported if used here

    if _background_service_active and isinstance(_background_task, asyncio.Task) and not _background_task.done():
        logger.info("BackgroundService: Service is already running or starting.") # pragma: no cover
        return
        
    _background_service_active = True
    _last_fact_curation_time = 0.0 
    _last_project_execution_scan_time = 0.0 # Reset this too
    if is_debug_mode():
        logger.info("BackgroundService: Attempting to start service...")
    try:
        loop = asyncio.get_running_loop() 
        _background_task = loop.create_task(_background_loop_async())
        if is_debug_mode():
            logger.info("BackgroundService: Service task created.")
    except RuntimeError: # pragma: no cover
        logger.error("BackgroundService: Asyncio loop not running. Cannot start service this way.")
        _background_service_active = False 
        return
    except Exception as e: # pragma: no cover
        logger.error(f"BackgroundService: Failed to create service task: {e}", exc_info=True)
        _background_service_active = False
        return

# Renamed, remains async
async def stop_background_services():
    global _background_service_active, _background_task
    
    if not _background_service_active or not isinstance(_background_task, asyncio.Task): # pragma: no cover
        logger.info("BackgroundService: Service is not running or task not found.")
        return

    logger.info("BackgroundService: Attempting to stop service...")
    _background_service_active = False 
    
    if _background_task and not _background_task.done(): # pragma: no branch
        _background_task.cancel()
        try:
            await _background_task 
            logger.info("BackgroundService: Service task successfully cancelled and awaited.") # pragma: no cover
        except asyncio.CancelledError: # pragma: no cover
            logger.info("BackgroundService: Service task explicitly cancelled.")
        except Exception as e: # pragma: no cover
            logger.error(f"BackgroundService: Error while awaiting cancelled task: {e}", exc_info=True)
            
    _background_task = None
    logger.info("BackgroundService: Service stop procedure completed.")

def is_background_service_active() -> bool:
    """Checks if the background service is currently active."""
    return _background_service_active

if __name__ == '__main__': # pragma: no cover
    # Minimal __main__ for testing the background service loop structure manually
    # Actual tool imports and functionality would require more setup or mocking
    
    # Mock necessary components if they are not available in this standalone run
    class MockToolSystemInstance:
        def list_tools(self): return {"mock_tool": "A mock tool for testing."}
    
    class MockReflectionModule:
        def run_self_reflection_cycle(self, available_tools):
            logger.info("--- MOCK run_self_reflection_cycle CALLED ---")
            time.sleep(0.1) # Simulate work
            return [{"suggestion_id": "mock_suggestion_main", "text": "Mock reflection suggestion"}]

    class MockKnowledgeToolsModule:
        async def run_periodic_fact_store_curation_async(self): # Matched name
            logger.info("--- MOCK run_periodic_fact_store_curation_async CALLED ---")
            await asyncio.sleep(0.1) # Simulate async work
            return True

    # Apply mocks
    tool_system.tool_system_instance = MockToolSystemInstance()
    run_self_reflection_cycle_orig = run_self_reflection_cycle
    run_periodic_fact_store_curation_async_orig = run_periodic_fact_store_curation_async

    globals()['run_self_reflection_cycle'] = MockReflectionModule().run_self_reflection_cycle
    globals()['run_periodic_fact_store_curation_async'] = MockKnowledgeToolsModule().run_periodic_fact_store_curation_async

    logger.info("--- Background Service Manual Test (via __main__) ---")
    
    async def test_run():
        global _polling_interval_seconds, _fact_curation_interval_seconds
        global PROJECT_EXECUTION_INTERVAL_SECONDS # Ensure this is accessible

        logger.info("Starting background service with very short intervals for testing...")
        _polling_interval_seconds = 3  # Short interval for reflection
        # FACT_CURATION_INTERVAL_SECONDS is now from config, so we'd mock config or set it high for manual test
        # For this manual test, let's assume config.FACT_CURATION_INTERVAL_SECONDS is also short or we override it locally
        # For simplicity, this __main__ test will use the config value.
        PROJECT_EXECUTION_INTERVAL_SECONDS = 5 # Short interval for project execution

        start_background_services() # Call the renamed sync function
        
        logger.info("Background service is running. Main test will sleep for 15 seconds.")
        await asyncio.sleep(15) 
        
        logger.info("\nStopping background service...")
        await stop_background_services() # Call the renamed async function
        logger.info("Background service stopped by test.")

    if os.name == 'nt':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(test_run())
    
    globals()['run_self_reflection_cycle'] = run_self_reflection_cycle_orig
    globals()['run_periodic_fact_store_curation_async'] = run_periodic_fact_store_curation_async_orig

    logger.info("--- Background Service Manual Test Finished ---")
### END FILE: core/background_service.py ###
# ### END FILE: ai_assistant/core/background_service.py ###

# ### START FILE: ai_assistant/core/conversation_intelligence.py ###
### START FILE: ai_assistant/core/conversation_intelligence.py ###
# ai_assistant/core/conversation_intelligence.py
import json
import asyncio
import os
import re
from typing import Dict, Optional, List, Any

from ai_assistant.llm_interface.ollama_client import invoke_ollama_model_async
from ai_assistant.config import get_model_for_task, CONVERSATION_HISTORY_TURNS, is_debug_mode, get_data_dir
from ai_assistant.planning.execution import ExecutionAgent # Assuming ExecutionAgent is the correct type
from ai_assistant.tools.tool_system import ToolSystem # Assuming ToolSystem is the correct type
from .reflection import global_reflection_log
from ai_assistant.memory.event_logger import log_event, get_recent_events
from ai_assistant.custom_tools.knowledge_tools import recall_facts # Added import

from ai_assistant.learning.learning import LearningAgent # Import LearningAgent
TOOL_CONFIRMATION_CONFIG_FILENAME_CI = "tool_confirmation_config.json" # For consistency
TOOL_CONFIRMATION_CONFIG_PATH_CI = os.path.join(get_data_dir(), TOOL_CONFIRMATION_CONFIG_FILENAME_CI)

MISSED_TOOL_OPPORTUNITY_PROMPT_TEMPLATE = """
You are an AI assistant evaluating if a previous user statement could have been addressed by one of your available tools, or if it's a direct response to a question you just asked.

User's Statement: {user_statement}

Recent Conversation History (AI's last turn might be a question):
{conversation_history}

Relevant Learned Facts:
{learned_facts_str}

Available Tools:
{tools_json_string}

**Project Management & Execution Tools to Consider if Relevant:**
If the user's statement relates to starting, developing, or running a software project, pay special attention to these tools if they are listed in "Available tools":
1.  `initiate_ai_project(project_name: str, project_description: str)`:
    * Use if the user wants to start a new software project.
    * `project_name` should be a concise name (e.g., "MyWebApp", "DataAnalyzer").
    * `project_description` should be the user's stated goal or a summary.
2.  `generate_code_for_project_file(project_name: str, filename: str)`:
    * Use if the user wants to generate code for a specific file within an *existing* project.
    * Identify `project_name` and `filename`.
3.  `execute_project_coding_plan(project_name: str)`:
    * Use if the user wants to generate all remaining planned code for an *existing* project.
    * Identify `project_name`.
4. `execute_python_script_in_project(project_name: str, script_filename: str, args: Optional[List[str]], timeout_seconds: int)`:
    * Use to run a specific Python script within an existing project.
    * Positional arguments for this tool are: `project_name` (string), `script_filename` (string), `args_for_the_script` (JSON list of strings), `timeout_seconds` (string representation of an integer).
    * `args_for_the_script` (the third argument to this tool) MUST be a JSON list of strings. If the script itself takes no arguments, pass an empty JSON list `[]` for this third argument.
    * `timeout_seconds` (the fourth argument) should be a string representing an integer (e.g., "60").
    * Example (no script arguments, 30s timeout): `"tool_name": "execute_python_script_in_project", "args": ["game_project", "main.py", [], "30"], "kwargs": {{}}`
    * Example (with script arguments, 60s timeout): `"tool_name": "execute_python_script_in_project", "args": ["data_project", "process.py", ["--input", "data.csv", "--output", "out.txt"], "60"], "kwargs": {{}}`

**Important Decision-Making Rules for Project-Related Tasks:**
1.  **Project Continuation/Updates**: If the user asks to 'update the game', 'continue the project', 'work on the project', 'do it', 'proceed', 'get the project started' or similar, and a specific project (e.g., "hangman", "snake") was recently discussed or initiated (check conversation history and learned facts), interpret this as a request to continue work on that project. The most appropriate tool is likely `execute_project_coding_plan` with the identified `project_name`.
2.  **Project Initiation vs. File Generation**: If the user expresses intent to create a new game, application, or any new software entity (e.g., "create the game X"), and it's not clear that a project for this entity has already been initiated in the conversation or known facts, you **MUST** prioritize suggesting or using the `initiate_ai_project` tool first. Do not suggest `generate_code_for_project_file` for a project that has not been explicitly initiated.
3.  **Argument Format**: The "args" field in your JSON response **MUST be a LIST**. Each element in this list corresponds to a positional argument for the tool.
    *   If a positional argument is expected to be a simple type (string, number, boolean), provide its string representation (e.g., `"value1"`, `"123"`, `"true"`).
    *   If a positional argument is *itself* expected to be a list (like the `args_for_the_script` parameter of `execute_python_script_in_project`), provide it as a JSON list within the main "args" list (e.g., `["project_name", "script.py", ["script_arg1", "script_arg2"], "60"]`). If this list argument is empty, use `[]`.
    The "kwargs" field **MUST be a dictionary**, e.g., `{{"key1": "valueA"}}`. If no keyword arguments are needed, use an empty dictionary `{{}}`.
 
**Clarified Rules for Continuing Project Work:**
1.  **Generating/Completing Code**: If the user asks to 'update the game', 'continue the project', 'work on the project', 'add features', or similar, AND it's implied that there are *pending coding tasks or new files to generate* for a known project (check conversation history and learned facts for project status), suggest `execute_project_coding_plan` with the identified `project_name`. This tool is for generating code based on the project's plan.
2.  **Running or Testing a Project**:
    *   If the user explicitly asks to 'run the game', 'test the project', 'see if it works', 'find the errors by running it', 'run the damn game', or similar for a known project (e.g., "hangman").
    *   OR, if the user implies the project's code generation phase is complete (e.g., `execute_project_coding_plan` recently ran and reported "no files in 'planned' state" or "nothing to do" for "hangman" or a similar game project) AND they now want to check its functionality or find errors.
    *   In these cases, **you MUST strongly prefer** suggesting or using `execute_python_script_in_project`.
    *   You will need to infer the `project_name` (e.g., "hangman") and the main `script_filename` (often "main.py", "app.py", or a name related to the project like "hangman.py"). Check learned facts or conversation history for the project's main script if known. If not known, you might need to ask or make a reasonable guess for `script_filename`.
3.  **Sequential Logic for Fixing Errors**: If `execute_project_coding_plan` was the last tool used for a project and it indicated completion (e.g., "no files in 'planned' state"), and the user then asks to "fix errors" or "see it run", the next logical step is `execute_python_script_in_project` to identify runtime issues. **Do NOT call `execute_project_coding_plan` again in this immediate sequence unless new coding tasks have been explicitly identified.** The goal of "fixing errors" often requires *finding* them first by running the code.

Analyze the user's statement in the context of the conversation history and learned facts.
**Pay close attention to the output of previous tool executions in the conversation history.** For example, if `execute_project_coding_plan` just reported that all tasks are complete for "hangman" (e.g., "Info: No files in 'planned' state found... Nothing to do."), and the user now says "run it" or "fix the errors", `execute_python_script_in_project` is the appropriate next step, not `execute_project_coding_plan` again.


Evaluation Steps:
1.  **Direct Confirmation/Response:** If the AI's last turn in history was a question (e.g., "Would you like me to run tool X?") and the user's current statement is a direct affirmation (e.g., "yes", "sure", "ok") or negation ("no", "don't"), respond with:
    `{{"is_confirmation_response": true, "confirmed_action": true/false, "tool_to_confirm": "tool_X_name_if_applicable"}}`
    If it's a confirmation for a generic question, `tool_to_confirm` can be null.
2.  **Tool Opportunity:** If not a direct confirmation, evaluate if an available tool (respecting the guidelines above) could address the user's statement:
    - Respond with a JSON object: `{{"tool_name": "...", "inferred_args": ["list", "of", "args"], "inferred_kwargs": {{"kwarg_name": "value"}}, "reasoning": "..."}}`
3.  **Tool Confirmation Settings Management:** If the user seems to want to manage tool confirmation settings (e.g., "always ask before searching", "don't ask for search anymore"), suggest 'manage_tool_confirmation_settings' if available:
    `{{"tool_name": "manage_tool_confirmation_settings", "inferred_args": ["action_to_infer", "tool_name_to_manage_if_any"], "inferred_kwargs": {{}}, "reasoning": "User wants to manage tool confirmation settings."}}`
4.  **No Tool/Confirmation:** If none of the above, respond with the exact string "NO_TOOL_RELEVANT".

Respond ONLY with the JSON object or "NO_TOOL_RELEVANT". Do not include other text or markdown.
"""

def _load_requires_confirmation_list_ci() -> List[str]:
    """
    Loads the list of tools that require user confirmation from the JSON configuration file.
    If the file doesn't exist or is invalid, it returns an empty list.
    """
    default_list: List[str] = []
    if not os.path.exists(TOOL_CONFIRMATION_CONFIG_PATH_CI):
        return default_list
    try:
        with open(TOOL_CONFIRMATION_CONFIG_PATH_CI, 'r', encoding='utf-8') as f:
            content = f.read()
            if not content.strip():
                return default_list
            data = json.load(f)
        loaded_list = data.get("requires_confirmation_tools", default_list)
        if not isinstance(loaded_list, list) or not all(isinstance(item, str) for item in loaded_list):
            print(f"Warning: 'requires_confirmation_tools' in '{TOOL_CONFIRMATION_CONFIG_PATH_CI}' is not a list of strings. Using default (empty list).") # pragma: no cover
            return default_list # pragma: no cover
        return loaded_list
    except (json.JSONDecodeError, IOError) as e: # pragma: no cover
        print(f"Warning: Could not load or parse '{TOOL_CONFIRMATION_CONFIG_PATH_CI}'. Defaulting to no tools requiring confirmation. Error: {e}")
        return default_list


async def detect_missed_tool_opportunity(
    user_statement: str,
    available_tools: Dict[str, str],
    executor: ExecutionAgent,
    tool_system_instance: ToolSystem,
    learning_agent: LearningAgent, # Add LearningAgent
    llm_model_name: Optional[str] = None # Keep this last for compatibility if not all callers update immediately
) -> Optional[Dict[str, Any]]:
    """
    Detects if a user's statement could have been addressed by an available tool,
    suggests the tool, or autonomously executes it if appropriate.
    """
    if not user_statement or not available_tools:
        if is_debug_mode(): # pragma: no cover
            print(f"[DEBUG CONV_INTEL] detect_missed_tool_opportunity returning None due to empty user_statement or available_tools.")
        return None

    try:
        tools_json_string = json.dumps(available_tools, indent=2)
    except TypeError: # pragma: no cover
        print("Error: Could not serialize available_tools to JSON for LLM prompt.")
        return None

    recent_events = get_recent_events(limit=CONVERSATION_HISTORY_TURNS * 2)
    formatted_history_lines = []
    for event in reversed(recent_events):
        if event.get("event_type") == "USER_INPUT_RECEIVED":
            formatted_history_lines.append(f"User: {event.get('description', '')}")
        elif event.get("event_type") in ["AI_CONVERSATIONAL_RESPONSE", "AI_TOOL_SUGGESTION_PROMPT", "AI_AUTONOMOUS_TOOL_RESPONSE", "WEEBO_RESPONSE", "AI_TOOL_EXECUTION_RESPONSE", "AI_TOOL_EXECUTION_DECLINED", "AI_TOOL_EXECUTION_FAILURE"]:
            formatted_history_lines.append(f"AI: {event.get('description', '')}")
        if len(formatted_history_lines) >= CONVERSATION_HISTORY_TURNS * 2 : # pragma: no cover
            break
    conversation_history_for_prompt = "\n".join(reversed(formatted_history_lines))
    if not conversation_history_for_prompt:
        conversation_history_for_prompt = "No recent history available."

    # ---- Retrieve and format learned facts ----
    try:
        recalled_facts_list = recall_facts() 
        if recalled_facts_list:
            facts_for_prompt = "\n".join([f"- {fact}" for fact in recalled_facts_list[:5]])
            if len(recalled_facts_list) > 5: # pragma: no cover
                facts_for_prompt += f"\n- ...and {len(recalled_facts_list) - 5} more facts."
        else:
            facts_for_prompt = "No specific facts currently learned that seem relevant."
    except Exception as e_facts: # pragma: no cover
        if is_debug_mode():
            print(f"[DEBUG CONV_INTEL] Error recalling facts for tool detection: {e_facts}")
        facts_for_prompt = "Could not retrieve learned facts at this time."
    # ---- END Fact Retrieval ----
    
    # Escape content that might contain stray {} characters which could break .format()
    escaped_user_statement = user_statement.replace('{', '{{').replace('}', '}}')
    escaped_conversation_history = conversation_history_for_prompt.replace('{', '{{').replace('}', '}}')
    escaped_learned_facts_str = facts_for_prompt.replace('{', '{{').replace('}', '}}')
    # tools_json_string also needs escaping, especially if it becomes "{}" for an empty dict.
    escaped_tools_json_string = tools_json_string.replace('{', '{{').replace('}', '}}')

    try:
        prompt = MISSED_TOOL_OPPORTUNITY_PROMPT_TEMPLATE.format(
            user_statement=escaped_user_statement,
            tools_json_string=escaped_tools_json_string, # Use the escaped version
            conversation_history=escaped_conversation_history,
            learned_facts_str=escaped_learned_facts_str
        )
    except IndexError as e_format: # pragma: no cover
        # This is where the user's error was happening.
        print(f"CRITICAL ERROR formatting MISSED_TOOL_OPPORTUNITY_PROMPT_TEMPLATE: {e_format}")
        print("This usually means there's a stray positional placeholder '{}' in the template string,")
        print("or a named placeholder is missing from the .format() call's keyword arguments.")
        print(f"Template preview (check for stray {{}}):\n{MISSED_TOOL_OPPORTUNITY_PROMPT_TEMPLATE[:1000]}...")
        return None


    if is_debug_mode(): # pragma: no cover
        print(f"[DEBUG CONV_INTEL] Missed tool detection prompt:\n{prompt[:1000]}\n---END PROMPT (TRUNCATED)---")

    model_to_use = llm_model_name if llm_model_name is not None else get_model_for_task("conversation_intelligence")

    if is_debug_mode(): # pragma: no cover
        print(f"[DEBUG CONV_INTEL] About to call invoke_ollama_model_async for tool detection. Model: {model_to_use}")

    llm_response = await invoke_ollama_model_async(prompt, model_name=model_to_use)

    if is_debug_mode(): # pragma: no cover
        print(f"[DEBUG CONV_INTEL] Raw LLM response for missed tool detection:\n'{llm_response}'")

    if not llm_response: # pragma: no cover
        print(f"Warning: Received no response from LLM ({model_to_use}) for missed tool detection.")
        return None

    llm_response = llm_response.strip()

    if llm_response == "NO_TOOL_RELEVANT":
        if is_debug_mode(): # pragma: no cover
            print(f"[DEBUG CONV_INTEL] LLM determined NO_TOOL_RELEVANT.")
        return None

    if llm_response.startswith("```json"):
        llm_response = llm_response.lstrip("```json").rstrip("```").strip()
    elif llm_response.startswith("```"): # pragma: no cover
        llm_response = llm_response.lstrip("```").rstrip("```").strip()

    try:
        parsed_response = json.loads(llm_response)
        if not isinstance(parsed_response, dict): # pragma: no cover
            print(f"Warning: LLM response for tool/confirmation detection was not a JSON dictionary. Response: {llm_response}")
            return None

        if parsed_response.get("is_confirmation_response") is True:
            if is_debug_mode(): # pragma: no cover
                print(f"[DEBUG CONV_INTEL] LLM identified a confirmation response: {parsed_response}")
            return parsed_response

        required_keys = ["tool_name", "inferred_args", "inferred_kwargs", "reasoning"]
        for key in required_keys:
            if key not in parsed_response: # pragma: no cover
                print(f"Warning: LLM response (tool suggestion) JSON is missing key '{key}'. Response: {llm_response}")
                return None
        
        # --- Robust argument parsing ---
        tool_name_detected = parsed_response.get("tool_name")
        raw_inferred_args = parsed_response.get("inferred_args")
        raw_inferred_kwargs = parsed_response.get("inferred_kwargs", {})

        final_args_list = []
        final_kwargs_dict = {}

        if isinstance(raw_inferred_kwargs, dict):
            final_kwargs_dict = {str(k): str(v) for k, v in raw_inferred_kwargs.items()}
        elif raw_inferred_kwargs is not None: # pragma: no cover
             print(f"Warning: 'inferred_kwargs' from LLM for tool '{tool_name_detected}' was not a dict (got {type(raw_inferred_kwargs)}). Using empty dict.")

        if isinstance(raw_inferred_args, list):
            final_args_list = [str(arg) for arg in raw_inferred_args]
        elif isinstance(raw_inferred_args, dict):
            print(f"Warning: 'inferred_args' from LLM was a dictionary for tool '{tool_name_detected}'. Attempting specific extraction. Dict: {raw_inferred_args}")
            if tool_name_detected == "initiate_ai_project":
                project_name_val = raw_inferred_args.get("project_name")
                project_description_val = raw_inferred_args.get("project_description")
                if project_description_val is None and "project_description" in final_kwargs_dict: # Check if it's in kwargs
                    project_description_val = final_kwargs_dict.pop("project_description")
                
                if project_name_val is not None and project_description_val is not None:
                    final_args_list = [str(project_name_val), str(project_description_val)]
                elif project_name_val is not None: # Only name found
                    final_args_list = [str(project_name_val), f"Project description for {project_name_val} (based on user query: {user_statement})"]
                    print(f"Warning: 'project_description' was not fully resolved for '{tool_name_detected}'. Using a placeholder.")
                else: # pragma: no cover
                    print(f"Error: Could not reliably extract 'project_name' and 'project_description' for '{tool_name_detected}' from dict: {raw_inferred_args}")
            elif tool_name_detected == "generate_code_for_project_file":
                project_name_val = raw_inferred_args.get("project_name")
                filename_val = raw_inferred_args.get("filename")
                if project_name_val is not None and filename_val is not None:
                    final_args_list = [str(project_name_val), str(filename_val)]
                else: # pragma: no cover
                    print(f"Warning: Could not determine project_name and filename from inferred_args dict for {tool_name_detected}")
            else: # General fallback if it's a dict for an unknown tool structure
                final_args_list = [str(v) for v in raw_inferred_args.values()]
        elif raw_inferred_args is not None: # Present but not list or dict
             print(f"Warning: 'inferred_args' from LLM for tool '{tool_name_detected}' was not a list or dict (got {type(raw_inferred_args)}). Using empty list.")
        
        parsed_response['inferred_args'] = final_args_list
        parsed_response['inferred_kwargs'] = final_kwargs_dict
        # --- End robust argument parsing ---


        if not isinstance(parsed_response["tool_name"], str) or \
           not isinstance(parsed_response["inferred_args"], list) or \
           not all(isinstance(arg, (str, int, float, bool)) or arg is None for arg in parsed_response["inferred_args"]) or \
           not isinstance(parsed_response["inferred_kwargs"], dict) or \
           not all(isinstance(k, str) and (isinstance(v, (str, int, float, bool)) or v is None) for k, v in parsed_response["inferred_kwargs"].items()) or \
           not isinstance(parsed_response["reasoning"], str): # pragma: no cover
            print(f"Warning: LLM response (tool suggestion) JSON has incorrect types for args/kwargs after processing. Response: {parsed_response}")
            pass # Allow to proceed, but log warning. Execution might fail if types are critical.

        if tool_name_detected not in available_tools and tool_name_detected != "manage_tool_confirmation_settings": # Allow this special tool
            print(f"Warning: LLM suggested tool '{tool_name_detected}' which is not in the available tools list (and not manage_tool_confirmation_settings).") # pragma: no cover
            return None 

        if tool_name_detected == "generate_code_for_project_file":
            project_name_arg_check = final_args_list[0] if final_args_list else None
            if project_name_arg_check:
                from ai_assistant.custom_tools.file_system_tools import sanitize_project_name, BASE_PROJECTS_DIR
                s_name = sanitize_project_name(project_name_arg_check)
                manifest_path_check = os.path.join(BASE_PROJECTS_DIR, s_name, "_ai_project_manifest.json")
                if not os.path.exists(manifest_path_check): # pragma: no cover
                    print(f"CONV_INTEL: Project '{project_name_arg_check}' manifest not found. Proposing 'initiate_ai_project' instead.")
                    desc_for_init = f"Create project '{project_name_arg_check}' based on user query: {user_statement}"
                    new_suggestion_prompt = f"It seems the project '{project_name_arg_check}' hasn't been set up yet. Shall I create it first with the description: '{desc_for_init}'?"
                    return {
                        "is_tool_suggestion": True,
                        "tool_name": "initiate_ai_project",
                        "inferred_args": [project_name_arg_check, desc_for_init],
                        "inferred_kwargs": {},
                        "reasoning": f"Project '{project_name_arg_check}' needs to be initiated first.",
                        "suggestion_prompt": new_suggestion_prompt 
                    }
        
        if tool_name_detected == "get_self_awareness_info_and_converse" and not parsed_response.get("inferred_args"): # pragma: no cover
            if is_debug_mode():
                print(f"[DEBUG CONV_INTEL] Auto-populating 'user_input' for '{tool_name_detected}' with current user_statement: '{user_statement}'")
            parsed_response["inferred_args"] = [user_statement]

        requires_confirmation_tools = _load_requires_confirmation_list_ci()

        if is_debug_mode(): # pragma: no cover
            print(f"[DEBUG CONV_INTEL] LLM suggested tool: {tool_name_detected}. Loaded 'requires confirmation' list: {requires_confirmation_tools}")

        if tool_name_detected not in requires_confirmation_tools:
            if is_debug_mode(): # pragma: no cover
                print(f"[DEBUG CONV_INTEL] Tool '{tool_name_detected}' is NOT in 'requires confirmation' list. Proceeding with autonomous execution.")
            
            inferred_args_tuple = tuple(parsed_response['inferred_args']) # Should be a list now
            inferred_kwargs_dict = parsed_response['inferred_kwargs'] # Should be a dict

            log_event(
                event_type="AUTONOMOUS_TOOL_EXECUTION_INITIATED",
                description=f"Autonomously executing tool '{tool_name_detected}' for user input: '{user_statement}'.",
                source="detect_missed_tool_opportunity",
                metadata={
                    "tool_name": tool_name_detected, "inferred_args": inferred_args_tuple,
                    "inferred_kwargs": inferred_kwargs_dict, "original_user_input": user_statement,
                    "reasoning": parsed_response.get("reasoning", "N/A")
                }
            )
            single_step_plan_auto = [{"tool_name": tool_name_detected, "args": inferred_args_tuple, "kwargs": inferred_kwargs_dict}]
            goal_for_auto_execution = f"Autonomously execute tool '{tool_name_detected}' based on user statement: {user_statement}"

            execution_results_auto_str = "No result or error during execution."
            try:
                if is_debug_mode(): # pragma: no cover
                    print(f"[DEBUG CONV_INTEL] Calling executor.execute_plan for autonomous execution. Goal: '{goal_for_auto_execution}'")
                from ai_assistant.planning.planning import PlannerAgent 
                temp_planner = PlannerAgent()
                execution_results_auto = await executor.execute_plan(
                    goal_description=goal_for_auto_execution,
                    initial_plan=single_step_plan_auto,
                    tool_system=tool_system_instance,
                    planner_agent=temp_planner,
                    learning_agent=learning_agent # Pass learning_agent
                )
                execution_results_auto_str = str(execution_results_auto)

                if is_debug_mode(): # pragma: no cover
                    print(f"[DEBUG CONV_INTEL] Autonomous execution result: {execution_results_auto_str}")

                augmented_history_for_response_gen = (
                    f"{conversation_history_for_prompt}\n"
                    f"{facts_for_prompt}\n" # Also include facts here
                    f"User: {user_statement}\n"
                    f"System: I autonomously ran the tool '{tool_name_detected}'. Result: {execution_results_auto_str[:500]}"
                )
                natural_response_after_tool = await generate_conversational_response(
                    user_input=f"(System note: I just ran '{tool_name_detected}' and got this: {execution_results_auto_str[:200]}. Now, how should I respond to the user's original statement: '{user_statement}'?)",
                    conversation_history=augmented_history_for_response_gen
                )
                return {
                    "autonomously_executed": True, "tool_name": tool_name_detected,
                    "results": execution_results_auto, "original_user_input": user_statement,
                    "conversational_response": natural_response_after_tool
                }
            except Exception as e_auto: # pragma: no cover
                print(f"Error during autonomous execution of '{tool_name_detected}': {e_auto}")
                augmented_history_for_error_response = (
                    f"{conversation_history_for_prompt}\n"
                    f"{facts_for_prompt}\n"
                    f"User: {user_statement}\n"
                    f"System: I tried to autonomously run the tool '{tool_name_detected}' but encountered an error: {str(e_auto)[:200]}"
                )
                natural_error_response = await generate_conversational_response(
                     user_input=f"(System note: I tried to run '{tool_name_detected}' but got an error: {str(e_auto)[:200]}. How should I inform the user about this regarding their original statement: '{user_statement}'?)",
                    conversation_history=augmented_history_for_error_response
                )
                return {
                    "autonomously_executed": False, "error": str(e_auto),
                    "tool_name": tool_name_detected,
                    "conversational_response": natural_error_response
                }
        else: # pragma: no cover
            if is_debug_mode():
                print(f"[DEBUG CONV_INTEL] Tool '{tool_name_detected}' IS in 'requires confirmation' list. Generating suggestion prompt.")
            args_str_manual = ", ".join(map(str, parsed_response['inferred_args']))
            kwargs_str_manual = ", ".join(f"{k}={str(v)}" for k, v in parsed_response['inferred_kwargs'].items())
            suggestion_prompt_text_manual = f"I found a tool called '{parsed_response['tool_name']}' that might help with that. "
            if args_str_manual: suggestion_prompt_text_manual += f"Based on your statement, I've inferred these arguments: [{args_str_manual}]. "
            if kwargs_str_manual: suggestion_prompt_text_manual += f"And these keyword arguments: {{{kwargs_str_manual}}}. "
            if not args_str_manual and not kwargs_str_manual: suggestion_prompt_text_manual += "It doesn't seem to require specific arguments based on your statement. "
            suggestion_prompt_text_manual += "Would you like me to run it?"
            parsed_response["suggestion_prompt"] = suggestion_prompt_text_manual
            parsed_response["is_tool_suggestion"] = True
            return parsed_response

    except json.JSONDecodeError: # pragma: no cover
        print(f"Warning: Failed to parse LLM response as JSON for tool/confirmation detection. Response: {llm_response}")
        return None
    except Exception as e: # pragma: no cover
        print(f"Warning: An unexpected error occurred during LLM response processing for tool/confirmation detection: {e}")
        return None


FORMULATE_TOOL_DESCRIPTION_PROMPT_TEMPLATE = """
You are an AI assistant helping to clarify a user's request for a new tool. Your goal is to transform the user's raw request into a concise and actionable description that can be fed into a code generation system.

User's raw request for a new tool:
"{user_raw_request}"

Based on this request:
1.  Identify the core functionality of the desired tool.
2.  Suggest a potential Python function name (e.g., `summarize_text`, `calculate_area`).
3.  Briefly describe what the tool should do.
4.  Mention its expected inputs (and their likely types if obvious, like string, list of numbers, etc.).
5.  Mention its expected output.

Combine these points into a single, clear, and concise paragraph. This paragraph will be used as a prompt for a code-generating LLM.

Example:
User's raw request: "I need a tool that can take a long article and just give me the main points, maybe like 3-4 bullet points."
Concise Tool Description: "A Python function, possibly named `summarize_text_to_bullets`, that takes a long string of text as input. It should analyze the text and return a list of strings, where each string is a key bullet point summarizing the input text (aim for 3-4 bullet points)."

User's raw request: "Make something to convert Celsius to Fahrenheit for me."
Concise Tool Description: "A Python function, possibly named `celsius_to_fahrenheit`, that takes a float representing a temperature in Celsius as input and returns a float representing the equivalent temperature in Fahrenheit."

Now, formulate a concise tool description for the following user's raw request:
User's raw request: "{user_raw_request}"
Concise Tool Description:
"""

async def formulate_tool_description_from_conversation(user_raw_request: str, llm_model_name: Optional[str] = None) -> Optional[str]:
    """
    Uses an LLM to transform a user's raw request for a new tool into a concise
    description suitable for a code generation system.
    """
    if not user_raw_request: # pragma: no cover
        if is_debug_mode():
            print(f"[DEBUG CONV_INTEL] formulate_tool_description_from_conversation returning None due to empty user_raw_request.")
        return None
    prompt = FORMULATE_TOOL_DESCRIPTION_PROMPT_TEMPLATE.format(user_raw_request=user_raw_request)
    if is_debug_mode(): # pragma: no cover
        print(f"[DEBUG CONV_INTEL] Formulate tool description prompt (first 300 chars):\n{prompt[:300]}...")
    model_to_use = llm_model_name if llm_model_name is not None else get_model_for_task("conversation_intelligence")

    if is_debug_mode(): # pragma: no cover
        print(f"[DEBUG CONV_INTEL] About to call invoke_ollama_model_async for tool formulation. Model: {model_to_use}")

    llm_response = await invoke_ollama_model_async(prompt, model_name=model_to_use)

    if is_debug_mode(): # pragma: no cover
         print(f"[DEBUG CONV_INTEL] Raw LLM response for tool formulation:\n'{llm_response}'")

    if not llm_response: # pragma: no cover
        print(f"Warning: Received no response from LLM ({model_to_use}) for tool description formulation.")
        return None
    cleaned_response = llm_response.strip()
    if cleaned_response.startswith("Concise Tool Description:"): # pragma: no cover
        cleaned_response = cleaned_response[len("Concise Tool Description:"):].strip()
    if not cleaned_response or len(cleaned_response) < 20: # pragma: no cover
        print(f"Warning: LLM response for tool description formulation seems too short or empty. Response: '{llm_response}'")
        return None
    return cleaned_response

CONVERSATIONAL_RESPONSE_PROMPT_TEMPLATE = """
You are a helpful AI assistant. Engage in a natural and friendly conversation with the user.
Consider the recent conversation history provided below to maintain context and coherence.

If you know the user's name is {user_name}, try to use it naturally in your response if appropriate.
Otherwise, address them generally (e.g., "you", "User").

If relevant facts about the user or the world are provided below, incorporate them naturally into your response if they help address the user's input or enrich the conversation. Do not just list the facts.

Relevant facts I know (if any):
{retrieved_facts_str}

Recent Conversation History:
{conversation_history}

User's latest input:
"{user_input}"

Your Conversational Response:
"""

async def generate_conversational_response(user_input: str, conversation_history: str) -> str:
    """
    Generates a conversational response from the AI, considering history and known facts.
    """
    if not user_input: # pragma: no cover
        if is_debug_mode():
            print(f"[DEBUG CONV_INTEL] generate_conversational_response received empty user_input. Returning default.")
        return "Is there something specific you'd like to talk about?"

    user_name = "User" 
    all_recalled_facts: List[str] = []
    try:
        all_recalled_facts = recall_facts()
        name_fact_prefix = "The user's name is "
        name_from_facts = None
        other_facts_for_prompt = []
        for fact in all_recalled_facts:
            if fact.lower().startswith(name_fact_prefix.lower()):
                potential_name = fact[len(name_fact_prefix):].strip()
                if potential_name: 
                    name_from_facts = potential_name
            else:
                other_facts_for_prompt.append(fact)
        if name_from_facts:
            user_name = name_from_facts
            if is_debug_mode(): # pragma: no cover
                print(f"[DEBUG CONV_INTEL] Recalled user name: {user_name}")
        elif is_debug_mode(): # pragma: no cover
            print(f"[DEBUG CONV_INTEL] No specific 'user's name is' fact found in recalled facts.")
        retrieved_facts_str = "\n".join([f"- {fact}" for fact in other_facts_for_prompt[:5]])
        if not retrieved_facts_str:
            retrieved_facts_str = "No specific relevant facts known at this moment."
    except Exception as e: # pragma: no cover
        if is_debug_mode():
            print(f"[DEBUG CONV_INTEL] Error recalling facts: {e}")
        retrieved_facts_str = "Error retrieving some contextual facts."

    prompt = CONVERSATIONAL_RESPONSE_PROMPT_TEMPLATE.format(
        user_input=user_input,
        conversation_history=conversation_history if conversation_history else "No prior conversation history for this turn.",
        user_name=user_name,
        retrieved_facts_str=retrieved_facts_str
    )
    if is_debug_mode(): # pragma: no cover
        print(f"[DEBUG CONV_INTEL] Conversational response prompt (user_name: {user_name}, retrieved_facts: {retrieved_facts_str[:100]}..., first 300 chars of prompt):\n{prompt[:300]}...")

    model_to_use = get_model_for_task("conversation_intelligence")

    if is_debug_mode(): # pragma: no cover
        print(f"[DEBUG CONV_INTEL] About to call invoke_ollama_model_async for conversational response. Model: {model_to_use}")

    llm_response = await invoke_ollama_model_async(prompt, model_name=model_to_use, max_tokens=2048)

    if is_debug_mode(): # pragma: no cover
        print(f"[DEBUG CONV_INTEL] Raw LLM response for conversational response:\n'{llm_response}'")

    if not llm_response: # pragma: no cover
        print(f"Warning: Received no response from LLM ({model_to_use}) for conversational response generation.")
        return "I'm not sure how to respond to that right now. Could you try rephrasing?"

    cleaned_response = llm_response.strip()
    if cleaned_response.startswith("Your Conversational Response:"): # pragma: no cover
        cleaned_response = cleaned_response[len("Your Conversational Response:"):].strip()
    return cleaned_response


if __name__ == '__main__': # pragma: no cover
    async def run_conv_intel_tests():
        print("--- Testing Conversation Intelligence Module (with Mocks & Broader Name/Fact Usage) ---")
        # ... (your existing __main__ test setup and cases) ...
        # (Ensure mocks for invoke_ollama_model_async, recall_facts, get_recent_events, 
        #  _load_requires_confirmation_list_ci, ExecutionAgent, ToolSystem are in place if running this directly)
        pass # Placeholder for actual test calls if this file were run standalone
    
    # To run the tests if this file is executed:
    # asyncio.run(run_conv_intel_tests())
### END FILE: ai_assistant/core/conversation_intelligence.py ###
# ### END FILE: ai_assistant/core/conversation_intelligence.py ###

# ### START FILE: ai_assistant/core/fs_utils.py ###
# ai_assistant/core/fs_utils.py
import os
import logging

logger = logging.getLogger(__name__)

def write_to_file(filepath: str, content: str) -> bool:
    """
    Writes the given content string to the specified filepath.
    Ensures the directory for the filepath exists, creating it if necessary.

    Args:
        filepath: The absolute or relative path to the file.
        content: The string content to write to the file.

    Returns:
        True if writing was successful, False otherwise.
    """
    try:
        # Ensure the directory exists
        dir_path = os.path.dirname(filepath)
        if dir_path: # Check if there is a directory part
            os.makedirs(dir_path, exist_ok=True)

        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        logger.info(f"Successfully wrote content to {filepath}")
        return True
    except IOError as e: # pragma: no cover
        logger.error(f"IOError writing to {filepath}: {e}", exc_info=True)
        return False
    except Exception as e: # pragma: no cover
        logger.error(f"Unexpected error writing to {filepath}: {e}", exc_info=True)
        return False

if __name__ == '__main__': # pragma: no cover
    # Configure basic logging for testing this module directly
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    test_dir = "test_fs_utils_output"
    test_filepath_success = os.path.join(test_dir, "subdir", "test_output.txt")
    test_content = "Hello, world!\nThis is a test file."

    print(f"Attempting to write to: {test_filepath_success}")
    success = write_to_file(test_filepath_success, test_content)

    if success:
        print(f"Successfully wrote to {test_filepath_success}. Verifying content...")
        try:
            with open(test_filepath_success, 'r', encoding='utf-8') as f_read:
                read_content = f_read.read()
            assert read_content == test_content
            print("Content verification successful.")
        except Exception as e_verify:
            print(f"Error verifying content: {e_verify}")
    else:
        print(f"Failed to write to {test_filepath_success}.")

    # Test failure case (e.g., by trying to write to a protected path, though hard to simulate reliably cross-platform)
    # For now, the success case demonstrates directory creation and writing.
    # A more robust test for failure would require mocking os.makedirs or open to raise IOError.

    # Cleanup (optional, comment out to inspect files)
    # import shutil
    # if os.path.exists(test_dir):
    #     print(f"Cleaning up test directory: {test_dir}")
    #     shutil.rmtree(test_dir)
    print(f"Test finished. Inspect '{test_dir}' if cleanup is commented out.")

# ### END FILE: ai_assistant/core/fs_utils.py ###

# ### START FILE: ai_assistant/core/orchestrator.py ###
"""Dynamic orchestrator for handling complex user interactions."""

import re # Added for regex matching of filenames
import os # Added for path joining in context gathering simulation
import asyncio
from typing import Dict, List, Optional, Any, Tuple
from ..planning.planning import PlannerAgent
from ..planning.execution import ExecutionAgent 
from ..memory.event_logger import log_event
from ..core.reflection import global_reflection_log, analyze_last_failure
from ..learning.learning import LearningAgent
from ..tools.tool_system import tool_system_instance
from ..config import is_debug_mode
from ..utils.display_utils import CLIColors, color_text # Added import

class DynamicOrchestrator:
    """
    Orchestrates the dynamic planning and execution of user prompts.
    Handles multi-step tasks, maintains context, and adapts plans based on feedback.
    """

    def __init__(self, 
                 planner: PlannerAgent,
                 executor: ExecutionAgent,
                 learning_agent: LearningAgent):
        self.planner = planner
        self.executor = executor
        self.learning_agent = learning_agent
        self.context: Dict[str, Any] = {}
        self.current_goal: Optional[str] = None
        self.current_plan: Optional[List[Dict[str, Any]]] = None

    def _generate_execution_summary(self, plan: Optional[List[Dict[str, Any]]], results: List[Any]) -> str:
        if not plan: # plan can be None if initial planning failed
            return "\n\nNo actions were planned or taken."

        summary_lines = [
            color_text("\n\nHere's a summary of what I did:", CLIColors.SYSTEM_MESSAGE)
        ]

        if not plan: # Double check, though outer check should catch it
             summary_lines.append("No plan was executed.")
             return "\n".join(summary_lines)

        for i, step in enumerate(plan):
            tool_name = step.get("tool_name", "Unknown Tool")
            args = step.get("args", ())

            outcome_str = ""
            if i < len(results):
                result_item = results[i]
                if isinstance(result_item, Exception):
                    outcome_str = color_text(
                        f"Failed (Error: {type(result_item).__name__}: {str(result_item)[:100]})",
                        CLIColors.FAILURE # Assuming CLIColors.FAILURE exists (e.g., red)
                    )
                elif isinstance(result_item, dict) and result_item.get("error"):
                    outcome_str = color_text(
                        f"Failed (Reported Error: {str(result_item.get('error'))[:100]})",
                        CLIColors.FAILURE
                    )
                elif isinstance(result_item, dict) and result_item.get("ran_successfully") is False:
                    err_detail = result_item.get("stderr", result_item.get("error", "Unknown error from tool"))
                    outcome_str = color_text(
                        f"Failed (Return Code: {result_item.get('return_code')}, Detail: {str(err_detail)[:100]})",
                        CLIColors.FAILURE
                    )
                else:
                    outcome_str = color_text(
                        f"Succeeded (Result: {str(result_item)[:100]}{'...' if len(str(result_item)) > 100 else ''})",
                        CLIColors.SUCCESS # Assuming CLIColors.SUCCESS exists (e.g., green)
                    )
            else:
                outcome_str = color_text("No result recorded for this step.", CLIColors.WARNING) # Assuming CLIColors.WARNING
            
            args_display_parts = [f"'{str(arg_val)[:25]}{'...' if len(str(arg_val)) > 25 else ''}'" if not (isinstance(arg_val, list) or isinstance(arg_val, dict)) else f"{type(arg_val).__name__}(len:{len(arg_val)})" for arg_val in args]
            summary_lines.append(
                f"- Ran '{color_text(tool_name, CLIColors.TOOL_NAME)}' " # Assuming CLIColors.TOOL_NAME
                f"with arguments ({color_text(', '.join(args_display_parts), CLIColors.TOOL_ARGS)}): " # Assuming CLIColors.TOOL_ARGS
                f"{outcome_str}") # outcome_str is already colored
        return "\n".join(summary_lines)

    async def process_prompt(self, prompt: str) -> Tuple[bool, str]:
        """
        Process a user prompt by creating and executing a dynamic plan.
        Returns (success, response_message)
        """
        try:
            self.current_goal = prompt
            available_tools = tool_system_instance.list_tools()

            # Log the start of processing
            log_event(
                event_type="ORCHESTRATOR_START_PROCESSING",
                description=f"Starting to process prompt: {prompt}",
                source="DynamicOrchestrator.process_prompt",
                metadata={"goal": prompt}
            )

            # --- NEW: Contextualization Phase (Simulated) ---
            project_context_summary = None
            project_name_for_context = None
            prompt_lower = prompt.lower()
            
            # Try to find a .py filename in the prompt
            py_file_match = re.search(r"([\w_/-]+\.py)", prompt) # Regex for a .py file (e.g., my_tool.py, utils/helpers.py)

            # Keywords for intent
            project_keywords = ["project", "game", "app", "application", "webapp"]
            tool_action_keywords = ["update", "fix", "modify", "add to", "change", "enhance", "improve", "debug"]
            tool_entity_keywords = ["tool", "function", "script", "file"]

            is_project_request = any(pk in prompt_lower for pk in project_keywords)
            is_tool_action_request = any(tak in prompt_lower for tak in tool_action_keywords)
            is_tool_entity_request = any(tek in prompt_lower for tek in tool_entity_keywords)

            # Path construction helpers
            ai_assistant_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
            simulated_project_base = os.path.join(ai_assistant_dir, "ai_generated_projects") # For multi-file projects
            custom_tools_base = os.path.join(ai_assistant_dir, "custom_tools") # For single tool files

            if py_file_match and (is_tool_action_request or is_tool_entity_request):
                # Likely a request to modify a specific tool file
                tool_filename_from_prompt = py_file_match.group(1)
                # Use os.path.basename to prevent path traversal if user provides a complex path
                safe_tool_basename = os.path.basename(tool_filename_from_prompt)
                
                # Attempt to locate the tool file in known directories (e.g., custom_tools)
                # This could be expanded to search other tool directories if needed.
                abs_path_to_tool_file = os.path.join(custom_tools_base, safe_tool_basename)

                if os.path.exists(abs_path_to_tool_file) and os.path.isfile(abs_path_to_tool_file):
                    project_name_for_context = f"Tool: {safe_tool_basename}"
                    if is_debug_mode():
                        print(f"DynamicOrchestrator: Detected request related to '{project_name_for_context}'. Attempting to gather context from {abs_path_to_tool_file}.")
                    
                    content = await tool_system_instance.execute_tool("read_text_from_file", args=(abs_path_to_tool_file,))
                    if not content.startswith("Error:"):
                        project_context_summary = f"Context for {project_name_for_context}:\n### START FILE: {safe_tool_basename} ###\n{content}\n### END FILE: {safe_tool_basename} ###"
                    else:
                        project_context_summary = f"Context for {project_name_for_context}: Error reading file: {content}"
                else:
                    if is_debug_mode():
                        print(f"DynamicOrchestrator: Tool file '{safe_tool_basename}' (from '{tool_filename_from_prompt}') mentioned, but not found at expected path '{abs_path_to_tool_file}'. No specific tool context loaded.")
            
            # If no specific .py tool file context was loaded, check for project-level context (like hangman)
            if project_context_summary is None and is_project_request and ("hangman game" in prompt_lower and is_tool_action_request):
                project_name_for_context = "myhangmangame" 
                if is_debug_mode():
                    print(f"DynamicOrchestrator: Detected request related to project '{project_name_for_context}'. Attempting to gather context.")
                
                context_parts = [f"Project: {project_name_for_context}\nFile Structure & Content (simplified for example):"]
                
                files_to_read_in_project = {
                    "src/game.py": os.path.join(simulated_project_base, project_name_for_context, "src", "game.py"),
                    "src/graphics.py": os.path.join(simulated_project_base, project_name_for_context, "src", "graphics.py"),
                    "src/user_input.py": os.path.join(simulated_project_base, project_name_for_context, "src", "user_input.py")
                }

                for rel_path, abs_path_to_read in files_to_read_in_project.items():
                    if os.path.exists(abs_path_to_read) and os.path.isfile(abs_path_to_read):
                        content = await tool_system_instance.execute_tool("read_text_from_file", args=(abs_path_to_read,))
                        if not content.startswith("Error:"):
                            context_parts.append(f"\n### START FILE: {rel_path} ###\n{content[:1000]}{'...' if len(content) > 1000 else ''}\n### END FILE: {rel_path} ###")
                        else:
                            context_parts.append(f"\n### FILE: {rel_path} - Error reading: {content} ###")
                    else:
                        context_parts.append(f"\n### FILE: {rel_path} - Not found at {abs_path_to_read} ###")
                project_context_summary = "\n".join(context_parts)

            # --- END NEW: Contextualization Phase ---

            # Create initial plan
            if is_debug_mode():
                print(f"DynamicOrchestrator: Creating initial plan for: {prompt}")

            self.current_plan = await self.planner.create_plan_with_llm(
                goal_description=prompt,
                available_tools=available_tools,
                project_context_summary=project_context_summary, # Pass gathered context
                project_name_for_context=project_name_for_context # Pass project name for context
            )
            if not self.current_plan:
                # If initial plan creation fails, self.current_plan is None or empty
                # _generate_execution_summary will handle None plan
                summary_for_no_plan = self._generate_execution_summary(self.current_plan, [])
                return False, f"Could not create a plan for the given prompt.{summary_for_no_plan}"

            # Execute plan with potential replanning
            if is_debug_mode():
                print(f"DynamicOrchestrator: Executing plan with {len(self.current_plan)} steps")

            # MODIFIED: execute_plan now returns final_plan_attempted and results_of_final_attempt
            final_plan_attempted, results_of_final_attempt = await self.executor.execute_plan(
                prompt,
                self.current_plan, # This is the initial plan
                tool_system_instance,
                self.planner,
                self.learning_agent
            )
            # Update self.current_plan to the plan that was actually run for logging/context
            self.current_plan = final_plan_attempted

            # Process results
            success = True
            if not results_of_final_attempt and final_plan_attempted : # Plan existed but no results
                success = False
            elif not final_plan_attempted and not results_of_final_attempt: # No plan, no results
                success = False # Or True if prompt was trivial and needed no plan? For now, False.
            else: # Has results, check them
                for res_item in results_of_final_attempt:
                    if isinstance(res_item, Exception):
                        success = False
                        break
                    if isinstance(res_item, dict):
                        if res_item.get("error") or res_item.get("ran_successfully") is False:
                            success = False
                            break
            # If the plan was non-empty but results are shorter than plan (e.g. critical failure mid-plan)
            if final_plan_attempted and len(results_of_final_attempt) < len(final_plan_attempted):
                success = False

            # Update context with results
            self.context.update({
                'last_results': results_of_final_attempt,
                'last_success': success,
                'completed_goal': prompt if success else None
            })

            # Log completion
            # self.current_plan is now final_plan_attempted
            num_steps_in_final_plan = len(self.current_plan) if self.current_plan else 0
            if not self.current_plan and not results_of_final_attempt and not final_plan_attempted:
                # This case means initial planning likely failed and returned empty plan
                # and orchestrator didn't return early.
                # Let's ensure num_steps is 0 if self.current_plan is None.
                num_steps_in_final_plan = 0

            log_event(
                event_type="ORCHESTRATOR_COMPLETE_PROCESSING",
                description=f"Completed processing prompt: {prompt}",
                source="DynamicOrchestrator.process_prompt",
                metadata={
                    "goal": prompt,
                    "success": success,
                    "num_steps": num_steps_in_final_plan
                }
            )

            # Format response message
            response = ""
            if success and self.current_plan and len(self.current_plan) == 1:
                tool_name_single = self.current_plan[0].get('tool_name', 'the tool')
                result_single_str = str(results_of_final_attempt[0])[:500] if results_of_final_attempt else "No specific result."
                response = f"Successfully completed the task by running '{tool_name_single}'. Result: {result_single_str}"
                # For single successful step, this response is the summary.
            else:
                if success:
                    response = "Successfully completed the multi-step task. "
                    if results_of_final_attempt:
                        result_str = str(results_of_final_attempt[-1])[:200]
                        response += f"The final step's result: {result_str}"
                    else: # Should not happen if success is true and plan was multi-step
                        response += "No specific result from the final step."
                else: # Failure
                    response = "Could not complete the task. "
                    if results_of_final_attempt:
                        last_error_str = "Details of the steps are in the summary below." # Default
                        for res_item in results_of_final_attempt: # Find first error
                            if isinstance(res_item, Exception):
                                last_error_str = f"An error occurred: {type(res_item).__name__}: {str(res_item)[:200]}"
                                break
                            if isinstance(res_item, dict) and (res_item.get("error") or res_item.get("ran_successfully") is False):
                                err_detail = res_item.get("stderr", res_item.get("error", "Unknown error from tool"))
                                last_error_str = f"A tool reported an error: {str(err_detail)[:200]}"
                                break
                        response += f"{last_error_str}"
                    else: # No results from final attempt, but it failed
                        response += "No specific results or errors from the last attempt."
                # Append detailed summary for multi-step tasks or any failure
                execution_summary = self._generate_execution_summary(self.current_plan, results_of_final_attempt)
                response += execution_summary
            return success, response

        except Exception as e:
            error_msg = f"Error during orchestration: {str(e)}"
            log_event(
                event_type="ORCHESTRATOR_ERROR",
                description=error_msg,
                source="DynamicOrchestrator.process_prompt",
                metadata={"error": str(e), "goal": prompt}
            )
            return False, error_msg

    async def get_current_progress(self) -> Dict[str, Any]:
        """Get the current progress and context of task execution."""
        return {
            'current_goal': self.current_goal,
            'current_plan': self.current_plan,
            'context': self.context,
            'last_success': self.context.get('last_success')
        }

# ### END FILE: ai_assistant/core/orchestrator.py ###

# ### START FILE: ai_assistant/core/project_manager.py ###
import json
import os
import uuid
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional, Union

from ai_assistant.config import get_data_dir
from ai_assistant.utils.display_utils import CLIColors, color_text # For potential direct use or consistency

PROJECTS_FILE_NAME = "projects.json"

def get_projects_file_path() -> str:
    return os.path.join(get_data_dir(), PROJECTS_FILE_NAME)

def _load_projects() -> List[Dict[str, Any]]:
    """Loads projects from the JSON file."""
    filepath = get_projects_file_path()
    if not os.path.exists(filepath):
        return []
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
            if not content:
                return []
            return json.loads(content)
    except (IOError, json.JSONDecodeError) as e:
        print(color_text(f"Error loading projects: {e}", CLIColors.ERROR_MESSAGE))
        return []

def _save_projects(projects: List[Dict[str, Any]]) -> bool:
    """Saves projects to the JSON file."""
    filepath = get_projects_file_path()
    try:
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(projects, f, indent=4)
        return True
    except IOError as e:
        print(color_text(f"Error saving projects: {e}", CLIColors.ERROR_MESSAGE))
        return False

def list_projects() -> List[Dict[str, Any]]:
    """Returns a list of all projects."""
    return _load_projects()

def create_project(name: str, description: Optional[str] = None) -> Optional[Dict[str, Any]]:
    """Creates a new project."""
    projects = _load_projects()
    if any(p['name'].lower() == name.lower() for p in projects):
        print(color_text(f"Project with name '{name}' already exists.", CLIColors.ERROR_MESSAGE))
        return None

    new_project = {
        "project_id": str(uuid.uuid4()),
        "name": name,
        "description": description or "",
        "status": "planning", # Default status
        "created_at": datetime.now(timezone.utc).isoformat(),
        "updated_at": datetime.now(timezone.utc).isoformat(),
        "tasks": []
    }
    projects.append(new_project)
    if _save_projects(projects):
        print(color_text(f"Project '{name}' created successfully with ID: {new_project['project_id']}.", CLIColors.SUCCESS))
        return new_project
    return None

def find_project(identifier: str) -> Optional[Dict[str, Any]]:
    """Finds a project by its ID or name."""
    projects = _load_projects()
    # Try by ID first
    for project in projects:
        if project['project_id'] == identifier:
            return project
    # Then try by name (case-insensitive)
    for project in projects:
        if project['name'].lower() == identifier.lower():
            return project
    return None

def remove_project(identifier: str) -> bool:
    """Removes a project by its ID or name."""
    projects = _load_projects()
    project_to_remove = find_project(identifier)

    if not project_to_remove:
        print(color_text(f"Project '{identifier}' not found.", CLIColors.ERROR_MESSAGE))
        return False

    projects = [p for p in projects if p['project_id'] != project_to_remove['project_id']]
    if _save_projects(projects):
        print(color_text(f"Project '{project_to_remove['name']}' (ID: {project_to_remove['project_id']}) removed.", CLIColors.SUCCESS))
        return True
    return False

def get_project_info(identifier: str) -> Optional[Dict[str, Any]]:
    """Gets detailed information for a specific project."""
    project = find_project(identifier)
    if not project:
        print(color_text(f"Project '{identifier}' not found.", CLIColors.ERROR_MESSAGE))
    return project

def get_project_status(identifier: str) -> Optional[str]:
    """Gets the status of a specific project."""
    project = find_project(identifier)
    return project['status'] if project else None

def update_project_status(identifier: str, new_status: str) -> bool:
    """Updates the status of a project."""
    projects = _load_projects()
    project_found = False
    for project in projects:
        if project['project_id'] == identifier or project['name'].lower() == identifier.lower():
            project['status'] = new_status
            project['updated_at'] = datetime.now(timezone.utc).isoformat()
            project_found = True
            break
    
    if not project_found:
        print(color_text(f"Project '{identifier}' not found for status update.", CLIColors.ERROR_MESSAGE))
        return False

    if _save_projects(projects):
        print(color_text(f"Status of project '{identifier}' updated to '{new_status}'.", CLIColors.SUCCESS))
        return True
    return False


def get_all_projects_summary_status() -> str:
    """Returns a summary string of all project statuses."""
    projects = _load_projects()
    if not projects:
        return "No projects found."
    
    status_counts: Dict[str, int] = {}
    for project in projects:
        status = project.get('status', 'unknown')
        status_counts[status] = status_counts.get(status, 0) + 1
    
    summary_lines = [f"Total Projects: {len(projects)}"]
    for status, count in status_counts.items():
        summary_lines.append(f"  - {status.capitalize()}: {count}")
    return "\n".join(summary_lines)


# ### END FILE: ai_assistant/core/project_manager.py ###

# ### START FILE: ai_assistant/core/refinement.py ###
# Self-Evolving-Agent-feat-chat-history-context/ai_assistant/core/refinement.py
import re
from typing import Dict, Any, Optional

from ai_assistant.llm_interface.ollama_client import invoke_ollama_model_async
from ai_assistant.config import get_model_for_task, is_debug_mode

REFINE_CODE_PROMPT_TEMPLATE = """
You are an AI assistant tasked with refining Python code based on a review.

Original Requirements:
{requirements}

The following Python code was generated to meet these requirements:
```python
{original_code}
```

This code was reviewed, and the review outcome was:
Status: {review_status}
Comments: {review_comments}
Suggestions for Improvement: {review_suggestions}

Your task is to carefully analyze the review feedback (comments and suggestions)
and rewrite the original code to address all the points raised.
The refined code must still meet the original requirements.

Respond ONLY with the complete, new, refined Python code block.
Do not include any explanations, apologies, or markdown formatting like ```python.
"""

class RefinementAgent:
    def __init__(self, llm_model_name: Optional[str] = None):
        """
        Initializes the RefinementAgent.
        Args:
            llm_model_name: Optional name of the LLM model to use for refinement.
                            If None, it will be determined by `get_model_for_task`.
        """
        self.model_name = llm_model_name if llm_model_name else get_model_for_task("code_refinement")
        if not self.model_name:
            # Fallback if "code_refinement" is not defined
            self.model_name = get_model_for_task("code_generation") # Or another capable model
            if not self.model_name:
                print("Warning: No model configured for 'code_refinement' or 'code_generation'. Refinement quality may be affected.")
                # self.model_name = "phi3:latest" # Example, if a known capable model exists
                pass

    async def refine_code(
        self,
        original_code: str,
        requirements: str,
        review_feedback: Dict[str, Any]
    ) -> str:
        """
        Refines the given code based on review feedback.

        Args:
            original_code: The original Python code string.
            requirements: The original requirements for the code.
            review_feedback: A dictionary containing review details like
                             "status", "comments", and "suggestions".

        Returns:
            The refined code string. Returns an empty string if LLM fails to generate
            a response or if the response is empty after cleaning.
        """
        review_status = review_feedback.get("status", "N/A")
        review_comments = review_feedback.get("comments", "No comments provided.")
        review_suggestions = review_feedback.get("suggestions") # Can be None
        if review_suggestions is None or not str(review_suggestions).strip():
            review_suggestions = "No specific suggestions provided."


        prompt = REFINE_CODE_PROMPT_TEMPLATE.format(
            original_code=original_code,
            requirements=requirements,
            review_status=review_status,
            review_comments=review_comments,
            review_suggestions=review_suggestions
        )

        if is_debug_mode():
            print(f"[DEBUG] RefinementAgent: requirements={requirements}")
            print(f"[DEBUG] RefinementAgent: original_code (first 200 chars)={original_code[:200]}")
            print(f"[DEBUG] RefinementAgent: review_status={review_status}")
            print(f"[DEBUG] RefinementAgent: review_comments={review_comments}")
            print(f"[DEBUG] RefinementAgent: review_suggestions={review_suggestions}")

        llm_response_str = await invoke_ollama_model_async(
            prompt,
            model_name=self.model_name,
            temperature=0.4 # Slightly lower for refinement to be less creative than initial gen
        )

        if not llm_response_str or not llm_response_str.strip():
            print("Warning: LLM returned an empty response during code refinement.")
            return "" # Return empty string if no response

        # Clean LLM output (remove markdown fences)
        cleaned_code = re.sub(r"^\s*```python\s*\n?", "", llm_response_str, flags=re.IGNORECASE | re.MULTILINE)
        cleaned_code = re.sub(r"\n?\s*```\s*$", "", cleaned_code, flags=re.IGNORECASE | re.MULTILINE).strip()
        
        if not cleaned_code:
            print("Warning: LLM response was empty after cleaning markdown for code refinement.")
            return ""

        if is_debug_mode():
            print(f"[DEBUG] LLM refinement response (first 200 chars): {llm_response_str[:200]}")

        return cleaned_code

if __name__ == '__main__':
    # Example Usage (requires Ollama server running with a suitable model)
    async def main():
        # Ensure a model for "code_refinement" or "code_generation" is configured
        # For testing, you might explicitly pass a model:
        # refinement_agent = RefinementAgent(llm_model_name="your_local_ollama_code_model:latest")
        refinement_agent = RefinementAgent()

        original_code_sample = """
def calculate_sum(a, b):
    # This function just adds two numbers
    result = a + b
    return result
"""
        requirements_sample = "Create a Python function `calculate_sum` that takes two numerical inputs, `a` and `b`, and returns their sum. The function should also include a docstring explaining its purpose, arguments, and return value. Input `a` must be positive."

        review_feedback_sample = {
            "status": "requires_changes",
            "comments": "The function correctly adds two numbers, but it's missing the docstring. Also, the requirement that 'a' must be positive is not enforced.",
            "suggestions": "1. Add a comprehensive docstring. 2. Add a check at the beginning of the function to ensure 'a' is positive; if not, raise a ValueError."
        }
        
        empty_suggestions_feedback = {
            "status": "requires_changes",
            "comments": "Missing docstring and validation for 'a'.",
            "suggestions": None # Test None suggestions
        }
        
        blank_suggestions_feedback = {
            "status": "requires_changes",
            "comments": "Missing docstring and validation for 'a'.",
            "suggestions": "   " # Test blank suggestions
        }


        print("\n--- Refining Code Sample 1 (with suggestions) ---")
        refined_code1 = await refinement_agent.refine_code(original_code_sample, requirements_sample, review_feedback_sample)
        print("Refined Code 1:")
        print(refined_code1)
        
        print("\n--- Refining Code Sample 2 (with empty suggestions) ---")
        refined_code2 = await refinement_agent.refine_code(original_code_sample, requirements_sample, empty_suggestions_feedback)
        print("Refined Code 2 (should be similar to 1 if LLM is good):")
        print(refined_code2)

        print("\n--- Refining Code Sample 3 (with blank suggestions) ---")
        refined_code3 = await refinement_agent.refine_code(original_code_sample, requirements_sample, blank_suggestions_feedback)
        print("Refined Code 3 (should be similar to 1 if LLM is good):")
        print(refined_code3)

        # Test with LLM returning empty response (mocked if possible, or hope it doesn't happen)
        # To truly test this, you'd mock invoke_ollama_model_async to return "" or None
        print("\n--- Refining Code Sample 4 (expecting empty from LLM - manual test or mock needed) ---")
        # This part is harder to test without mocking the LLM call to return empty
        # For now, assume it might generate something or we'd mock `invoke_ollama_model_async` in a real test suite
        # Example of how you might test a direct empty response from LLM:
        # with patch('ai_assistant.core.refinement.invoke_ollama_model_async', AsyncMock(return_value="")):
        #     refined_code_empty_llm = await refinement_agent.refine_code(original_code_sample, requirements_sample, review_feedback_sample)
        #     print(f"Refined Code (Empty LLM): '{refined_code_empty_llm}'") # Expect empty string


    import asyncio
    asyncio.run(main())

# For potential import into __init__.py or other modules
__all__ = ['RefinementAgent', 'REFINE_CODE_PROMPT_TEMPLATE']

# ### END FILE: ai_assistant/core/refinement.py ###

# ### START FILE: ai_assistant/core/reflection.py ###
# ai_assistant/core/reflection.py
import datetime
import re
import json # For the simplistic check in to_serializable_dict
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional
import traceback # For serializing exception tracebacks
import uuid
import os # For os.path.exists and os.path.getsize
from ai_assistant.config import is_debug_mode

# Import the new functions and filepath from persistent_memory
from ai_assistant.memory.persistent_memory import (
    save_reflection_log_entries,
    load_reflection_log_entries,
    REFLECTION_LOG_FILEPATH # Import the default filepath
)

@dataclass
class ReflectionLogEntry:
    """Represents a single entry in the reflection log."""
    goal_description: str
    plan: List[Dict[str, Any]]
    execution_results: List[Any]
    status: str
    entry_id: str = field(default_factory=lambda: str(uuid.uuid4()))  # Unique ID for each entry
    notes: Optional[str] = ""
    timestamp: datetime.datetime = field(default_factory=lambda: datetime.datetime.now(datetime.timezone.utc))
    error_type: Optional[str] = None
    error_message: Optional[str] = None
    traceback_snippet: Optional[str] = None
    # New fields for self-modification attempts
    is_self_modification_attempt: bool = False
    source_suggestion_id: Optional[str] = None
    modification_type: Optional[str] = None # E.g., "MODIFY_TOOL_CODE", "UPDATE_TOOL_DESCRIPTION"
    modification_details: Optional[Dict[str, Any]] = None # E.g., {"module_path": ..., "function_name": ..., "suggested_code_change": ...}
    post_modification_test_passed: Optional[bool] = None
    post_modification_test_details: Optional[Dict[str, Any]] = None # E.g., {"passed": True/False, "stdout": ..., "stderr": ..., "notes": ...}
    commit_info: Optional[Dict[str, Any]] = None # E.g., {"commit_message": ..., "commit_hash": ...}


    def to_serializable_dict(self) -> Dict[str, Any]:
        """Converts the entry to a dictionary suitable for JSON serialization."""
        serializable_results = []
        for res in self.execution_results:
            if isinstance(res, Exception):
                # CHANGE 1: Store a structured dictionary for exceptions
                serializable_results.append({
                    "_is_error_representation_": True,  # Clear marker
                    "error_type_name": type(res).__name__,
                    "error_message_str": str(res),
                    # Optionally add traceback for individual step errors if needed:
                    # "error_traceback_snippet": traceback.format_exception_only(type(res), res)[-1].strip()
                })
            else:
                try:
                    json.dumps(res) # Check if directly serializable
                    serializable_results.append(res)
                except (TypeError, OverflowError):
                    serializable_results.append(str(res)) # Fallback to string

        return {
            "goal_description": self.goal_description,
            "plan": self.plan,
            "execution_results": serializable_results, # Contains structured dicts for errors
            "status": self.status,
            "entry_id": self.entry_id,
            "notes": self.notes,
            "timestamp": self.timestamp.isoformat(),
            "error_type": self.error_type,
            "error_message": self.error_message,
            "traceback_snippet": self.traceback_snippet,
            # New fields
            "is_self_modification_attempt": self.is_self_modification_attempt,
            "source_suggestion_id": self.source_suggestion_id,
            "modification_type": self.modification_type,
            "modification_details": self.modification_details,
            "post_modification_test_passed": self.post_modification_test_passed,
            "post_modification_test_details": self.post_modification_test_details,
            "commit_info": self.commit_info,
        }

    @classmethod
    def from_serializable_dict(cls, data: Dict[str, Any]) -> 'ReflectionLogEntry':
        """Creates a ReflectionLogEntry from a dictionary (e.g., loaded from JSON)."""
        timestamp_str = data.get("timestamp")
        timestamp_obj = None
        if timestamp_str:
            try:
                timestamp_obj = datetime.datetime.fromisoformat(timestamp_str)
            except ValueError:
                print(f"Warning: Could not parse timestamp '{timestamp_str}'. Using current UTC time for this entry.")
                timestamp_obj = datetime.datetime.now(datetime.timezone.utc)
        else:
            timestamp_obj = datetime.datetime.now(datetime.timezone.utc)

        return cls(
            goal_description=data.get("goal_description", ""),
            plan=data.get("plan", []),
            execution_results=data.get("execution_results", []), # Will contain dicts for errors
            status=data.get("status", "UNKNOWN"),
            entry_id=data.get("entry_id", str(uuid.uuid4())),  # Generate new ID if not present
            notes=data.get("notes", ""),
            timestamp=timestamp_obj,
            error_type=data.get("error_type"),
            error_message=data.get("error_message"),
            traceback_snippet=data.get("traceback_snippet"),
            # New fields
            is_self_modification_attempt=data.get("is_self_modification_attempt", False),
            source_suggestion_id=data.get("source_suggestion_id"),
            modification_type=data.get("modification_type"),
            modification_details=data.get("modification_details"),
            post_modification_test_passed=data.get("post_modification_test_passed"),
            post_modification_test_details=data.get("post_modification_test_details"),
            commit_info=data.get("commit_info"),
        )

    def to_formatted_string(self) -> str:
        """Formats the log entry into a human-readable string."""
        formatted_plan_steps = []
        if self.plan:
            for i, step in enumerate(self.plan):
                formatted_plan_steps.append(
                    f"    Step {i+1}: Tool: {step.get('tool_name', 'N/A')}, "
                    f"Args: {step.get('args', 'N/A')}, Kwargs: {step.get('kwargs', 'N/A')}"
                )
        else:
            formatted_plan_steps.append("    No plan executed.")
        formatted_plan = "\n".join(formatted_plan_steps)

        formatted_results_steps = []
        if self.execution_results:
            for i, res_item in enumerate(self.execution_results):
                res_str = ""
                if isinstance(res_item, dict) and res_item.get("_is_error_representation_"):
                    res_str = f"Error: {res_item.get('error_type_name')}: {res_item.get('error_message_str')}"
                else:
                    res_str = str(res_item)

                if len(res_str) > 150:
                    res_str = res_str[:147] + "..."
                formatted_results_steps.append(f"    Step {i+1} Result: {res_str}")
        else:
            formatted_results_steps.append("    No results recorded.")
        formatted_results = "\n".join(formatted_results_steps)

        notes_str = f"Notes: {self.notes}\n" if self.notes else ""

        error_details_str = ""
        if self.error_type:
            error_details_str = (
                f"First Critical Error:\n"
                f"  Type: {self.error_type}\n"
                f"  Message: {self.error_message}\n"
            )
            if self.traceback_snippet:
                error_details_str += f"  Traceback Snippet:\n{self.traceback_snippet}\n"

        timestamp_display = self.timestamp.strftime('%Y-%m-%d %H:%M:%S UTC')
        if self.timestamp.tzinfo:
            timestamp_display = self.timestamp.strftime('%Y-%m-%d %H:%M:%S %Z')

        # Start building output_parts list
        output_parts = [
            "--------------------------------------------------",
            f"Timestamp: {timestamp_display}",
            f"Goal: {self.goal_description}",
            f"Status: {self.status}"
        ]
        if notes_str: # Only add if notes_str is not empty
            output_parts.append(notes_str.strip()) # Remove trailing newline from notes_str if it exists
        if error_details_str: # Only add if error_details_str is not empty
            output_parts.append(error_details_str.strip()) # Remove trailing newline
        output_parts.extend([
            f"Plan:\n{formatted_plan}",
            f"Results (individual steps):\n{formatted_results}"
        ])

        if self.is_self_modification_attempt:
            output_parts.append("--- Self-Modification Attempt Details ---")
            if self.source_suggestion_id:
                output_parts.append(f"Source Suggestion ID: {self.source_suggestion_id}")
            if self.modification_type:
                output_parts.append(f"Modification Type: {self.modification_type}")
            if self.modification_details:
                try:
                    details_str = json.dumps(self.modification_details, indent=2, sort_keys=True)
                    # For very long code changes, maybe just show keys or a summary
                    if len(details_str) > 500: 
                        details_str = f"Keys: {list(self.modification_details.keys())} (Details too long to display fully)"
                except TypeError:
                    details_str = str(self.modification_details) # Fallback
                output_parts.append(f"Modification Details: {details_str}")

            test_passed_str = "N/A"
            if self.post_modification_test_passed is True:
                test_passed_str = "True"
            elif self.post_modification_test_passed is False:
                test_passed_str = "False"
            output_parts.append(f"Post-Modification Test Passed: {test_passed_str}")

            if self.post_modification_test_details:
                test_details_summary = self.post_modification_test_details.get("notes", "No specific notes.")
                if len(test_details_summary) > 200 : test_details_summary = test_details_summary[:197] + "..."
                output_parts.append(f"Post-Modification Test Details: {test_details_summary}")
            
            if self.commit_info:
                commit_message_summary = self.commit_info.get("commit_message", "N/A")
                if len(commit_message_summary) > 200 : commit_message_summary = commit_message_summary[:197] + "..."
                output_parts.append(f"Commit Info: {commit_message_summary}")
            
            output_parts.append("---------------------------------------")
        
        output_parts.append("--------------------------------------------------")
        return "\n".join(output_parts)


class ReflectionLog:
    """Manages a log of reflection entries with persistence."""
    def __init__(self, filepath: str = REFLECTION_LOG_FILEPATH):
        self.filepath: str = filepath
        self.log_entries: List[ReflectionLogEntry] = []
        self.load_log()

    def load_log(self):
        """Loads reflection log entries from the persistent file."""
        if is_debug_mode():
            print(f"ReflectionLog: Loading log from '{self.filepath}'...")
        loaded_entry_dicts = load_reflection_log_entries(self.filepath)
        temp_entries = []
        for entry_data in loaded_entry_dicts:
            if not isinstance(entry_data, dict):
                # This is a warning, so it should probably always print or use logger.warning
                print(f"ReflectionLog: Warning - Skipping non-dictionary item in loaded log data: {str(entry_data)[:100]}...")
                continue
            try:
                temp_entries.append(ReflectionLogEntry.from_serializable_dict(entry_data))
            except Exception as e:
                print(f"ReflectionLog: Error deserializing entry data: '{str(entry_data)[:100]}...'. Error: {e}. Skipping.")
        self.log_entries = temp_entries
        if is_debug_mode():
            print(f"ReflectionLog: Loaded {len(self.log_entries)} entries from '{self.filepath}'.")
        if not self.log_entries and os.path.exists(self.filepath) and os.path.getsize(self.filepath) > 0:
             print(f"ReflectionLog: Warning - File '{self.filepath}' exists and is not empty, but no valid log entries were loaded. The file might be corrupted or in an old format.")

    def save_log(self):
        """Saves the current reflection log entries to the persistent file."""
        if is_debug_mode():
            print(f"ReflectionLog: Attempting to save {len(self.log_entries)} log entries to '{self.filepath}'...")
        entries_to_save = []
        for entry in self.log_entries:
            try:
                entries_to_save.append(entry.to_serializable_dict())
            except Exception as e:
                print(f"ReflectionLog: Error serializing entry for goal '{entry.goal_description}'. Error: {e}. Skipping this entry for save.")

        if save_reflection_log_entries(self.filepath, entries_to_save):
            if is_debug_mode():
                print(f"ReflectionLog: Successfully saved {len(entries_to_save)} entries to '{self.filepath}'.")
        else:
            print(f"ReflectionLog: Failed to save log entries to '{self.filepath}'.")

    def add_entry(self, entry: ReflectionLogEntry):
        """Appends a new entry to the in-memory log."""
        self.log_entries.append(entry)
        # Save the log immediately after adding an entry
        self.save_log()

    def get_entries(self, limit: int = 10) -> List[ReflectionLogEntry]:
        """Returns the last 'limit' entries from the in-memory log."""
        if limit <= 0:
            return []
        return self.log_entries[-limit:]

    def log_execution(
        self,
        goal_description: str,
        plan: List[Dict[str, Any]],
        execution_results: List[Any],
        overall_success: bool,
        notes: str = "",
        first_error_type: Optional[str] = None,
        first_error_message: Optional[str] = None,
        first_traceback_snippet: Optional[str] = None,
        status_override: Optional[str] = None,
        # New parameters for self-modification
        is_self_modification_attempt: bool = False,
        source_suggestion_id: Optional[str] = None,
        modification_type: Optional[str] = None,
        modification_details: Optional[Dict[str, Any]] = None,
        post_modification_test_passed: Optional[bool] = None,
        post_modification_test_details: Optional[Dict[str, Any]] = None,
        commit_info: Optional[Dict[str, Any]] = None
    ) -> ReflectionLogEntry: # Add return type
        current_status: str
        if status_override:
            current_status = status_override
        else:
            current_status = "FAILURE"
            if overall_success:
                current_status = "SUCCESS"
            else:
                if plan:
                    successful_steps = 0
                    # CHANGE 2: Check for our error representation in status determination
                    for result in execution_results:
                        is_error_representation = isinstance(result, dict) and result.get("_is_error_representation_")
                        if not isinstance(result, Exception) and not is_error_representation:
                            successful_steps += 1

                    if successful_steps == len(plan) and plan:
                        current_status = "SUCCESS"
                        if not overall_success:
                            if not notes: notes += " "
                            notes += "(Note: overall_success was False but all plan steps completed without error representations)."
                    elif successful_steps > 0 and successful_steps < len(plan):
                        current_status = "PARTIAL_SUCCESS"
                    else:
                        current_status = "FAILURE"
                # else current_status remains "FAILURE"

            if not plan and not execution_results:
                current_status = "SUCCESS" if overall_success else "EMPTY_FAILURE"
                if not notes: notes += " "
                notes += "No plan or results. Marked as " + ("successful." if overall_success else "unsuccessful empty goal.")
            elif overall_success and not execution_results and plan:
                current_status = "ANOMALY_SUCCESS_NO_RESULTS"
                if not notes: notes += " "
                notes += "Marked SUCCESS but no results for non-empty plan."
            elif not plan and execution_results:
                current_status = "ANOMALY_RESULTS_NO_PLAN"
                if not notes: notes += " "
                notes += "Results present without a plan."

        entry = ReflectionLogEntry(
            goal_description=goal_description,
            plan=plan,
            execution_results=execution_results,
            status=current_status,
            notes=notes.strip(),
            error_type=first_error_type,
            error_message=first_error_message,
            traceback_snippet=first_traceback_snippet,
            # Pass new fields
            is_self_modification_attempt=is_self_modification_attempt,
            source_suggestion_id=source_suggestion_id,
            modification_type=modification_type,
            modification_details=modification_details,
            post_modification_test_passed=post_modification_test_passed,
            post_modification_test_details=post_modification_test_details,
            commit_info=commit_info
        )
        self.add_entry(entry)
        return entry # Return the created entry

global_reflection_log = ReflectionLog()

_STOP_WORDS = set([
    "a", "an", "the", "is", "are", "was", "were", "be", "been", "being",
    "have", "has", "had", "do", "does", "did", "will", "would", "should", "can",
    "could", "may", "might", "must", "and", "or", "but", "if", "because", "as",
    "until", "while", "of", "at", "by", "for", "with", "about", "against",
    "between", "into", "through", "during", "before", "after", "above", "below",
    "to", "from", "up", "down", "in", "out", "on", "off", "over", "under",
    "again", "further", "then", "once", "here", "there", "when", "where", "why",
    "how", "all", "any", "both", "each", "few", "more", "most", "other", "some",
    "such", "no", "nor", "not", "only", "own", "same", "so", "than", "too", "very",
    "s", "t", "can", "will", "just", "don", "should", "now", "tool", "function",
    "input", "inputs", "output", "outputs", "args", "str", "int", "float", "list",
    "returns", "calculates", "performs", "executes", "given", "specified"
])

def _extract_keywords(text: str) -> set:
    if not text:
        return set()
    words = re.findall(r'\b\w+\b', text.lower())
    return {word for word in words if word not in _STOP_WORDS and len(word) > 2}

LLM_FAILURE_ANALYSIS_PROMPT = """The AI assistant encountered an issue in its last operation. Please analyze the details and provide insights.
Original Goal: {goal}
Attempted Plan:
{plan_str}
Execution Status: {status}
Error Type: {error_type}
Error Message: {error_message}
Traceback Snippet (if available):
{traceback_snippet}
Available Tools (Name: Description):
{tools_json_str}
Based on this information:
1. Explain the likely cause of the failure in simple terms.
2. If applicable, suggest how the specific tool(s) that failed in the plan might be used differently (e.g., different arguments, fixing input) to achieve their part of the goal.
3. Suggest any alternative tools or sequences of tools from the 'Available Tools' list that could be used to achieve the original goal.
Please structure your response clearly (e.g., using markdown headings for "Cause", "Suggestions for Failed Tool", "Alternative Approaches").
Response:
"""

def analyze_last_failure(tool_registry: Dict[str, str], ollama_model_name: Optional[str] = None) -> Optional[str]:
    from ai_assistant.llm_interface.ollama_client import invoke_ollama_model
    from ai_assistant.config import get_model_for_task

    model_to_use = ollama_model_name if ollama_model_name is not None else get_model_for_task("reflection")

    if not global_reflection_log.log_entries:
        return "No actions logged yet to analyze."
    last_entry = global_reflection_log.log_entries[-1]

    is_truly_failed_for_analysis = False
    if last_entry.status != "SUCCESS" and last_entry.error_type:
        is_truly_failed_for_analysis = True
    if not is_truly_failed_for_analysis:
        return "No critical failure (with error details) found in the last action to analyze with LLM."

    try:
        plan_str = json.dumps(last_entry.plan, indent=2) if last_entry.plan else "No plan was executed or plan was empty."
    except TypeError:
        plan_str = str(last_entry.plan) + " (Note: Plan contained non-serializable data)"
    try:
        tools_json_str = json.dumps(tool_registry, indent=2)
    except TypeError:
        tools_json_str = str(tool_registry) + " (Note: Tool registry contained non-serializable data)"

    formatted_prompt = LLM_FAILURE_ANALYSIS_PROMPT.format(
        goal=last_entry.goal_description,
        plan_str=plan_str,
        status=last_entry.status,
        error_type=last_entry.error_type or "N/A",
        error_message=str(last_entry.error_message) if last_entry.error_message else "N/A",
        traceback_snippet=last_entry.traceback_snippet or "N/A",
        tools_json_str=tools_json_str
    )
    if is_debug_mode():
        print(f"\nReflectionAnalysis: Sending failure analysis prompt to LLM (model: {model_to_use})...")
    llm_analysis = invoke_ollama_model(formatted_prompt, model_name=model_to_use)

    if llm_analysis and llm_analysis.strip():
        return f"--- LLM Failure Analysis ---\n{llm_analysis.strip()}"
    else:
        # Fallback logic for analyze_last_failure
        if is_debug_mode():
            print("ReflectionAnalysis: LLM analysis was unsuccessful or returned empty. Fallback analysis follows.")
        failed_tool_name: Optional[str] = None
        # Try to find the failed tool by checking execution_results for our error dict
        if last_entry.plan and last_entry.execution_results and len(last_entry.plan) == len(last_entry.execution_results):
            for i, res_item in enumerate(last_entry.execution_results):
                if isinstance(res_item, dict) and res_item.get("_is_error_representation_"):
                    if i < len(last_entry.plan):
                        step_info = last_entry.plan[i]
                        if isinstance(step_info, dict):
                            failed_tool_name = step_info.get('tool_name')
                            break
        # If not found, try from error message (less reliable)
        if not failed_tool_name and last_entry.error_message:
            match = re.search(r"Tool '([^']*)'", str(last_entry.error_message))
            if match:
                failed_tool_name = match.group(1)

        error_msg_display = str(last_entry.error_message) if last_entry.error_message else last_entry.error_type
        if failed_tool_name and failed_tool_name in tool_registry:
            failed_tool_keywords = _extract_keywords(tool_registry.get(failed_tool_name, "") + " " + failed_tool_name)
            alternatives = []
            if failed_tool_keywords:
                for name, description in tool_registry.items():
                    if name == failed_tool_name: continue
                    common_keywords = failed_tool_keywords.intersection(_extract_keywords(description + " " + name))
                    if common_keywords:
                        alternatives.append({"name": name, "description": description, "score": len(common_keywords), "common": list(common_keywords)})
                alternatives.sort(key=lambda x: x["score"], reverse=True)
            if alternatives:
                suggestion_str = f"LLM analysis failed. Fallback keyword analysis for tool '{failed_tool_name}' (Error: {error_msg_display}):\n"
                suggestion_str += "Possible alternatives:\n"
                for alt in alternatives[:2]:
                    suggestion_str += f"  - '{alt['name']}': {alt['description']} (Common: {alt['common']})\n"
                return suggestion_str.strip()
            else:
                return f"LLM analysis failed. Fallback: Tool '{failed_tool_name}' failed (Error: {error_msg_display}). No keyword-based alternatives found."
        else:
            return f"LLM analysis failed. Fallback: The operation failed (Error: {error_msg_display}). Review the plan and tool descriptions for alternatives."

def get_learnings_from_reflections(max_entries: int = 50) -> List[str]:
    entries = global_reflection_log.get_entries(limit=max_entries)
    if not entries:
        return ["No reflection log entries found to derive learnings."]

    learning_points: List[str] = []
    for entry in entries:
        timestamp_str = entry.timestamp.strftime('%Y-%m-%d %H:%M')

        if entry.status == "SUCCESS" and entry.notes and "Succeeded on retry" in entry.notes:
            speculative_tool_mention = ""
            if entry.plan and len(entry.plan) == 1:
                 speculative_tool_mention = f" (tool '{entry.plan[0].get('tool_name', 'Unknown Tool')}' was involved)"
            learning = (
                f"Learning [{timestamp_str}]: For goal '{entry.goal_description}', "
                f"a successful outcome was achieved after initial difficulties (retries were needed){speculative_tool_mention}. "
                f"This suggests that the involved tool(s) might be sensitive to transient issues or require specific argument tuning."
            )
            learning_points.append(learning)

        elif entry.status != "SUCCESS" and entry.error_type:
            first_failed_tool_name: Optional[str] = None
            specific_error_message_for_learning: str = entry.error_message or entry.error_type or "Unknown error"

            # CHANGE 3: Use the structured error representation to find the failed tool and its message
            if entry.plan and entry.execution_results and len(entry.plan) == len(entry.execution_results):
                for i, res_item in enumerate(entry.execution_results):
                    if isinstance(res_item, dict) and res_item.get("_is_error_representation_"):
                        if i < len(entry.plan):
                            step_info = entry.plan[i]
                            if isinstance(step_info, dict):
                                first_failed_tool_name = step_info.get('tool_name', 'Unknown Tool in plan')
                                specific_error_message_for_learning = f"Type: {res_item.get('error_type_name')}, Message: {res_item.get('error_message_str')}"
                        break # Focus on the first error found in execution_results

            if first_failed_tool_name and first_failed_tool_name != 'Unknown Tool in plan':
                learning = (
                    f"Learning [{timestamp_str}]: For goal '{entry.goal_description}', "
                    f"tool '{first_failed_tool_name}' likely failed with error details: '{specific_error_message_for_learning}'. "
                    f"Consider reviewing its usage or alternatives."
                )
            else: # General failure or error not tied to a specific tool in execution_results
                learning = (
                     f"Learning [{timestamp_str}]: Goal '{entry.goal_description}' resulted in status '{entry.status}' "
                     f"with primary error: '{specific_error_message_for_learning}'. "
                     f"This might indicate an issue in planning, pre-execution setup, or a tool failure not detailed in step results."
                )
            learning_points.append(learning)

    if not learning_points:
        return ["No specific learning points identified from the recent activity based on current criteria."]
    return learning_points

# Original __main__ block is removed to prevent accidental execution with test data
# and because it would need significant updates to test persistence.
# ### END FILE: ai_assistant/core/reflection.py ###

# ### START FILE: ai_assistant/core/reviewer.py ###
# Self-Evolving-Agent-feat-chat-history-context/ai_assistant/core/reviewer.py
import json
from typing import Optional, Dict, Any

from ai_assistant.llm_interface.ollama_client import invoke_ollama_model_async
from ai_assistant.config import get_model_for_task

REVIEW_CODE_PROMPT_TEMPLATE = """
You are a meticulous AI code reviewer. Your task is to review the provided code based on the given requirements and related tests (if any).

**Code to Review:**
```
{code_to_review}
```

**Original Requirements:**
{original_requirements}

**Related Tests (if provided):**
```
{related_tests}
```

**Review Criteria:**
1.  **Adherence to Original Requirements**: Does the code meet all specified requirements? Are there any deviations or missed functionalities?
2.  **Correctness & Potential Bugs**: Are there any logical errors, potential bugs, or edge cases not handled?
3.  **Alignment with Related Tests**: If tests are provided, how well would the code likely pass them? Does the code address the scenarios covered by the tests?
4.  **Clarity, Readability, and Maintainability**: Is the code clear, well-documented (if applicable), and easy to understand? Are variable names descriptive? (Provide a brief assessment).

**Output Structure:**
You *MUST* respond with a single JSON object. Do not include any other text or explanations before or after the JSON object.
The JSON object must contain the following keys:
-   `"status"`: String - One of "approved", "requires_changes", or "rejected".
    -   "approved": Code meets requirements, seems correct, and is well-written.
    -   "requires_changes": Code is largely on track but has issues (e.g., missed requirements, minor bugs, clarity issues) that could be fixed.
    -   "rejected": Code is fundamentally flawed, significantly deviates from requirements, or has critical errors.
-   `"comments"`: String - A detailed textual summary of your review findings, explaining the reasoning for the status. Be specific.
-   `"suggestions"`: String (Optional) - If status is "requires_changes", provide specific, actionable suggestions for improvement. If status is "approved" or "rejected", this can be an empty string or omitted.

**Example JSON Output for "requires_changes":**
```json
{{
  "status": "requires_changes",
  "comments": "The code correctly implements the addition feature but misses the requirement to handle negative numbers. The variable names 'a' and 'b' could be more descriptive. The provided tests only cover positive integers, so test coverage for negative inputs is missing.",
  "suggestions": "1. Add a check for negative input values and clarify how they should be handled based on requirements (e.g., return an error, use absolute values). 2. Rename variable 'a' to 'first_number' and 'b' to 'second_number' for better readability. 3. Consider adding test cases for negative inputs and zero values."
}}
```

**Example JSON Output for "approved":**
```json
{{
  "status": "approved",
  "comments": "The code perfectly meets all specified requirements, including handling of edge cases discussed. It is clear, readable, and the provided tests cover the main functionality effectively.",
  "suggestions": ""
}}
```

Now, please review the provided code.
"""

class ReviewerAgent:
    def __init__(self, llm_model_name: Optional[str] = None):
        """
        Initializes the ReviewerAgent.
        Args:
            llm_model_name: Optional name of the LLM model to use for reviews.
                            If None, it will be determined by `get_model_for_task`.
        """
        self.llm_model_name = llm_model_name if llm_model_name else get_model_for_task("code_reviewer")
        if not self.llm_model_name:
            # Fallback if "code_reviewer" is not defined
            self.llm_model_name = get_model_for_task("general_purpose_llm") # Or another capable model like "code_generation"
            if not self.llm_model_name:
                print("Warning: No model configured for 'code_reviewer' or 'general_purpose_llm'. Review quality may be affected. Consider using a general code-aware model.")
                # As a last resort, one might hardcode a known capable model name if absolutely necessary,
                # but relying on get_model_for_task and config is preferred.
                # self.llm_model_name = "phi3:latest" # Example if ollama has this by default
                pass


    async def review_code(
        self,
        code_to_review: str,
        original_requirements: str,
        related_tests: Optional[str] = None,
        attempt_number: int = 1  # New parameter
    ) -> Dict[str, Any]:
        """
        Reviews the given code against original requirements and related tests.

        Args:
            code_to_review: The source code string to be reviewed.
            original_requirements: A string describing the original requirements for the code.
            related_tests: An optional string containing related test cases or descriptions.
            attempt_number: The attempt number for this review cycle.

        Returns:
            A dictionary containing the review status, comments, and suggestions.
            In case of errors during review (e.g., LLM failure, JSON parsing issues),
            it returns a dictionary with status "error" and relevant comments.
        """
        if not code_to_review:
            return {
                "status": "error",
                "comments": "No code provided for review.",
                "suggestions": ""
            }
        if not original_requirements:
            return {
                "status": "error",
                "comments": "Original requirements were not provided for the review.",
                "suggestions": ""
            }

        tests_for_prompt = related_tests if related_tests and related_tests.strip() else "No specific tests provided for review context."

        prompt = REVIEW_CODE_PROMPT_TEMPLATE.format(
            code_to_review=code_to_review,
            original_requirements=original_requirements,
            related_tests=tests_for_prompt
        )
        
        # Optional: Use attempt_number in a print statement for clarity during execution
        # Limiting length of requirements in print for brevity
        print(f"ReviewerAgent: Reviewing code for '{original_requirements[:70].replace('\n', ' ')}...' (Attempt #{attempt_number})")

        llm_response_str = "" # Initialize for error reporting
        try:
            llm_response_str = await invoke_ollama_model_async(
                prompt,
                model_name=self.llm_model_name,
                temperature=0.2
            )

            if not llm_response_str or not llm_response_str.strip():
                return {
                    "status": "error",
                    "comments": "LLM returned an empty response.",
                    "suggestions": ""
                }

            # Clean and parse the response
            cleaned_response_str = llm_response_str.strip()
            if cleaned_response_str.startswith("```json"):
                cleaned_response_str = cleaned_response_str[len("```json"):].strip()
                if cleaned_response_str.endswith("```"):
                    cleaned_response_str = cleaned_response_str[:-len("```")].strip()
            elif cleaned_response_str.startswith("```"):
                cleaned_response_str = cleaned_response_str[len("```"):].strip()
                if cleaned_response_str.endswith("```"):
                    cleaned_response_str = cleaned_response_str[:-len("```")].strip()
            
            review_data = json.loads(cleaned_response_str)
            if not review_data:
                return {
                    "status": "error",
                    "comments": "Parsed review data is empty",
                    "suggestions": ""
                }

            # Validate required keys and data types
            if not isinstance(review_data, dict) or "status" not in review_data or "comments" not in review_data:
                error_comment = "LLM response JSON is missing required keys ('status', 'comments') or is not a dictionary. Raw response: " + cleaned_response_str
                return {
                    "status": "error",
                    "comments": error_comment,
                    "suggestions": ""
                }
            
            # Ensure suggestions key exists and has a valid value
            if "suggestions" not in review_data or review_data["suggestions"] is None:
                review_data["suggestions"] = ""
            
            # Validate status value
            valid_statuses = ["approved", "requires_changes", "rejected"]
            if not review_data.get("status") or review_data["status"] not in valid_statuses:
                error_comment = f"LLM response JSON has an invalid 'status' value: {review_data.get('status')}. Expected one of {valid_statuses}. Raw response: " + cleaned_response_str
                return {
                    "status": "error",
                    "comments": error_comment,
                    "suggestions": review_data.get("suggestions") or "" # Ensure suggestions is never None
                }

            # Ensure all dictionary values are not None before returning
            review_data["status"] = review_data["status"] or "error"
            review_data["comments"] = review_data["comments"] or "No comments provided"
            review_data["suggestions"] = review_data["suggestions"] or ""

            return review_data

        except json.JSONDecodeError as e:
            response_preview = llm_response_str[:500] if llm_response_str else "(empty response)"
            error_msg = f"Failed to parse LLM response as JSON. Error: {e}. Raw response snippet: '{response_preview}...'"
            return {
                "status": "error",
                "comments": error_msg,
                "suggestions": ""
            }
        except Exception as e:
            # Catch any other unexpected errors during LLM call or processing
            response_preview = llm_response_str[:500] if llm_response_str else "(empty response)"
            return {
                "status": "error",
                "comments": f"An unexpected error occurred during code review: {e}. Raw LLM response snippet: '{response_preview}...'",
                "suggestions": ""
            }

def review_reflection_suggestion(suggestion: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """
    (Placeholder) Reviews a single reflection-generated improvement suggestion.
    This function needs to be fully implemented to provide qualitative review.
    """
    print(f"INFO: Placeholder review_reflection_suggestion called for suggestion ID: {suggestion.get('suggestion_id', 'N/A')}")
    # For now, let's assume the review is positive but indicates it's a placeholder
    return {
        "review_looks_good": True, # Default to True for placeholder
        "qualitative_review": "Placeholder review: Suggestion logged. Actual review logic not yet implemented.",
        "confidence_score": 0.5, # Neutral confidence
        "suggested_modifications": ""
    }

if __name__ == '__main__':
    # Example Usage (requires Ollama server running with a suitable model)
    async def main():
        # Ensure you have a model configured for "code_reviewer" or "general_purpose_llm"
        # For testing, you might explicitly pass a known model if config isn't set up:
        # reviewer = ReviewerAgent(llm_model_name="your_local_ollama_code_model:latest") 
        reviewer = ReviewerAgent() 

        # Test Case 1: Code that needs changes
        code1 = """
def add(a, b):
    # This function adds two numbers
    return a + b
"""
        reqs1 = "Create a Python function called 'add' that takes two numbers and returns their sum. It should also handle string inputs by trying to convert them to numbers if they represent valid numbers."
        tests1 = """
Test 1: add(5, 10) should return 15
Test 2: add("5", "10") should return 15
Test 3: add(-1, 1) should return 0
Test 4: add("abc", "10") should ideally raise an error or return a specific value indicating failure.
"""
        print("\n--- Reviewing Code 1 (Needs Changes) ---")
        review1 = await reviewer.review_code(code1, reqs1, tests1, attempt_number=1)
        print(json.dumps(review1, indent=2))

        # Test Case 2: Good code
        code2 = """
def multiply(x: int, y: int) -> int:
    \"\"\"Multiplies two integers and returns the result.
    Handles positive and negative integers.
    \"\"\"
    return x * y
"""
        reqs2 = "Create a Python function 'multiply' that takes two integers, x and y, and returns their product. Include type hints and a docstring. Ensure it works for positive and negative integers."
        tests2 = "Test: multiply(3, 4) == 12; multiply(-2, 5) == -10; multiply(0, 100) == 0"
        print("\n--- Reviewing Code 2 (Good Code) ---")
        review2 = await reviewer.review_code(code2, reqs2, tests2, attempt_number=1)
        print(json.dumps(review2, indent=2))

        # Test Case 3: Flawed code (e.g. NameError)
        code3 = "def divide(a,b): return a/c # obvious error: c is not defined"
        reqs3 = "Function to divide number a by number b."
        print("\n--- Reviewing Code 3 (Flawed Code) ---")
        review3 = await reviewer.review_code(code3, reqs3, attempt_number=1) # No tests
        print(json.dumps(review3, indent=2))
        
        # Test Case 4: Empty code
        print("\n--- Reviewing Code 4 (Empty Code) ---")
        review4 = await reviewer.review_code("", reqs1, attempt_number=1) # Using reqs1 for consistency
        print(json.dumps(review4, indent=2))
        
        # Test Case 5: No requirements
        print("\n--- Reviewing Code 5 (No Requirements) ---")
        review5 = await reviewer.review_code(code1, "", attempt_number=1)
        print(json.dumps(review5, indent=2))

    import asyncio
    asyncio.run(main())

# To make this module's contents (like ReviewerAgent) easily importable
__all__ = ['ReviewerAgent', 'REVIEW_CODE_PROMPT_TEMPLATE']

# ### END FILE: ai_assistant/core/reviewer.py ###

# ### START FILE: ai_assistant/core/self_modification.py ###
# Code for the AI assistant's self-modification capabilities.
import importlib
import inspect
from typing import Optional
import ast
import os
import shutil
import logging
import sys

# Configure logger for this module
logger = logging.getLogger(__name__)
if not logger.handlers: # Avoid adding multiple handlers if script is reloaded/run multiple times
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')


def get_function_source_code(module_path: str, function_name: str) -> Optional[str]:
    """
    Retrieves the source code of a specified function within a given module.

    Args:
        module_path: The Python module path (e.g., "ai_assistant.communication.cli").
        function_name: The name of the function.

    Returns:
        The source code of the function as a string, or None if an error occurs.
    """
    try:
        module = importlib.import_module(module_path)
    except ModuleNotFoundError:
        print(f"Error: Module '{module_path}' not found.")
        return None
    except Exception as e: # pragma: no cover
        print(f"Error importing module '{module_path}': {e}")
        return None

    try:
        function_obj = getattr(module, function_name)
    except AttributeError:
        print(f"Error: Function '{function_name}' not found in module '{module_path}'.")
        return None
    except Exception as e: # pragma: no cover
        print(f"Error getting attribute '{function_name}' from module '{module_path}': {e}")
        return None

    try:
        source_code = inspect.getsource(function_obj)
        return source_code
    except TypeError: # pragma: no cover
        print(f"Error: Source code for '{function_name}' in '{module_path}' is not available (e.g., C extension, built-in).")
        return None
    except OSError: # pragma: no cover
        print(f"Error: Source file for '{module_path}' likely not found, cannot get source for '{function_name}'.")
        return None
    except Exception as e: # pragma: no cover
        print(f"Error getting source code for '{function_name}' in '{module_path}': {e}")
        return None

def edit_function_source_code(module_path: str, function_name: str, new_code_string: str, project_root_path: str) -> str:
    """
    Edits the source code of a specified function within a given module file using AST.

    Args:
        module_path: The Python module path (e.g., "ai_assistant.custom_tools.my_extra_tools").
        function_name: The name of the function to modify.
        new_code_string: A string containing the new, complete source code for the function.
        project_root_path: The absolute path to the root of the project.

    Returns:
        A success message if the modification was successful, or an error message string if not.
    """
    file_path = "" # Initialize to ensure it's defined for logging in case of early error
    try:
        # Construct the full path to the module file
        relative_module_file_path = os.path.join(*module_path.split('.')) + ".py"
        file_path = os.path.join(project_root_path, relative_module_file_path)
        
        if not os.path.exists(file_path):
            err_msg = f"Error: Module file not found at '{file_path}' derived from module path '{module_path}'."
            logger.error(err_msg)
            return err_msg

        backup_file_path = file_path + ".bak"
        shutil.copy2(file_path, backup_file_path)
        logger.info(f"Backup of '{file_path}' created at '{backup_file_path}'.")

        with open(file_path, 'r', encoding='utf-8') as f:
            original_source = f.read()

        original_ast = ast.parse(original_source, filename=file_path)

        try:
            new_function_ast_module = ast.parse(new_code_string)
        except SyntaxError as e_new_code_syn: # pragma: no cover
            err_msg = f"SyntaxError in new_code_string: {e_new_code_syn.msg} (line {e_new_code_syn.lineno}, offset {e_new_code_syn.offset})"
            logger.error(f"{err_msg} - New code: \n{new_code_string}")
            return err_msg
        
        if not new_function_ast_module.body or not isinstance(new_function_ast_module.body[0], ast.FunctionDef):
            err_msg = "Error: new_code_string does not seem to be a valid single function definition."
            logger.error(err_msg)
            return err_msg
        
        new_function_node = new_function_ast_module.body[0]

        if new_function_node.name != function_name:
            logger.warning(
                f"The new code defines a function named '{new_function_node.name}', "
                f"but the target function name is '{function_name}'. "
                f"The function name in the new code will be used for replacement, effectively renaming the function."
            )
            # If strict name matching is required, this should return an error.
            # For now, we proceed with replacing the old named function with the new one (which might have a new name).
            # The function being replaced is identified by 'function_name'.

        function_found_and_replaced = False
        new_body = []
        for node in original_ast.body:
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)) and node.name == function_name:
                new_body.append(new_function_node)
                function_found_and_replaced = True
                logger.info(f"Function '{function_name}' found in '{file_path}' and marked for replacement with '{new_function_node.name}'.")
            else:
                new_body.append(node)
        
        if not function_found_and_replaced:
            err_msg = f"Error: Function '{function_name}' not found in module '{module_path}' (file '{file_path}')."
            logger.error(err_msg)
            return err_msg

        original_ast.body = new_body

        try:
            new_source_code = ast.unparse(original_ast)
        except AttributeError: # pragma: no cover
            err_msg = "Error: ast.unparse is not available. Python 3.9+ is required."
            logger.error(err_msg)
            return err_msg
        except Exception as e_unparse: # pragma: no cover
            err_msg = f"Error unparsing modified AST for '{file_path}': {e_unparse}"
            logger.error(err_msg, exc_info=True)
            return err_msg

        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(new_source_code)
        
        logger.info(f"Successfully modified function '{function_name}' (replaced with '{new_function_node.name}') in module '{module_path}' (file '{file_path}').")
        return f"Function '{function_name}' (replaced with '{new_function_node.name}') in module '{module_path}' updated successfully."

    except FileNotFoundError: # pragma: no cover
        err_msg = f"Error: File not found for module path '{module_path}' (expected at '{file_path}')."
        logger.error(err_msg)
        return err_msg
    except SyntaxError as e_syn: # pragma: no cover
        err_msg = f"SyntaxError during AST parsing. File: '{e_syn.filename}', Line: {e_syn.lineno}, Offset: {e_syn.offset}, Message: {e_syn.msg}"
        logger.error(err_msg, exc_info=True)
        return f"SyntaxError: {err_msg}"
    except Exception as e: # pragma: no cover
        err_msg = f"An unexpected error occurred in edit_function_source_code: {type(e).__name__}: {e}"
        logger.error(err_msg, exc_info=True)
        return err_msg

def get_backup_function_source_code(module_path: str, function_name: str) -> Optional[str]:
    """
    Retrieves the source code of a specified function from its backup (.bak) file.

    Args:
        module_path: The Python module path (e.g., "ai_assistant.custom_tools.my_extra_tools").
        function_name: The name of the function to retrieve from the backup.

    Returns:
        The source code of the function as a string if found in the backup, otherwise None.
    """
    file_path_py = os.path.join(*module_path.split('.')) + ".py"
    backup_file_path = file_path_py + ".bak"

    if not os.path.exists(backup_file_path):
        print(f"Warning: Backup file '{backup_file_path}' not found for module '{module_path}'.")
        return None

    try:
        with open(backup_file_path, 'r', encoding='utf-8') as f:
            backup_source = f.read()

        backup_ast = ast.parse(backup_source, filename=backup_file_path)

        for node in backup_ast.body:
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)) and node.name == function_name:
                try:
                    return ast.unparse(node)
                except AttributeError: # pragma: no cover
                    print("Error: ast.unparse not available. Python 3.9+ required.")
                    return None

        print(f"Warning: Function '{function_name}' not found in backup file '{backup_file_path}'.")
        return None

    except FileNotFoundError: # pragma: no cover
        print(f"Error: Backup file '{backup_file_path}' disappeared unexpectedly.")
        return None
    except SyntaxError as e_syn: # pragma: no cover
        print(f"SyntaxError parsing backup file '{backup_file_path}': {e_syn}")
        return None
    except Exception as e: # pragma: no cover
        print(f"Unexpected error retrieving function from backup '{backup_file_path}': {e}")
        return None

if __name__ == '__main__': # pragma: no cover
    # --- Setup for Tests ---
    TEST_DIR = "test_ai_assistant_ws_self_modification" # Unique name for this test suite
    CORE_DIR_SM = os.path.join(TEST_DIR, "ai_assistant", "core")
    CUSTOM_TOOLS_DIR_SM = os.path.join(TEST_DIR, "ai_assistant", "custom_tools")

    # Clean up old test directory if it exists
    if os.path.exists(TEST_DIR):
        shutil.rmtree(TEST_DIR)
    
    os.makedirs(CORE_DIR_SM, exist_ok=True)
    os.makedirs(CUSTOM_TOOLS_DIR_SM, exist_ok=True)

    # Create dummy __init__.py files to make them packages
    with open(os.path.join(TEST_DIR, "ai_assistant", "__init__.py"), "w") as f: f.write("")
    with open(os.path.join(CORE_DIR_SM, "__init__.py"), "w") as f: f.write("")
    with open(os.path.join(CUSTOM_TOOLS_DIR_SM, "__init__.py"), "w") as f: f.write("")


    core_test_module_filename = "test_module_core.py"
    core_test_module_path_str = os.path.join(CORE_DIR_SM, core_test_module_filename)
    original_core_function_one_code = (
        "def core_function_one():\n"
        "    print('This is core_function_one original')\n"
        "    return 1"
    )
    with open(core_test_module_path_str, "w", encoding="utf-8") as f:
        f.write("import os\n\n")
        f.write(original_core_function_one_code + "\n\n")
        f.write("def core_function_two(x, y):\n")
        f.write("    print('This is core_function_two original')\n")
        f.write("    return x + y\n")

    custom_test_module_filename = "test_module_custom.py"
    custom_test_module_path_str = os.path.join(CUSTOM_TOOLS_DIR_SM, custom_test_module_filename)
    with open(custom_test_module_path_str, "w", encoding="utf-8") as f:
        f.write("def custom_tool_alpha(message: str):\n")
        f.write("    '''This is the original custom_tool_alpha docstring.'''\n")
        f.write("    print(f'Original custom_tool_alpha: {message}')\n")
        f.write("    return f'Received: {message}'\n")

    # Add TEST_DIR to sys.path so that modules within it can be imported
    original_sys_path = list(sys.path)
    sys.path.insert(0, os.path.abspath(TEST_DIR)) # Add test_dir to path

    # --- Test Cases for edit_function_source_code ---
    print("\n--- Testing edit_function_source_code ---")
    module_path_core = f"ai_assistant.core.{core_test_module_filename[:-3]}"

    print("\nTest E1: Successful edit of 'core_function_one'")
    new_code_core_one = (
        "def core_function_one():\n"
        "    print('This is core_function_one MODIFIED')\n"
        "    # Added a comment\n"
        "    return 100"
    )
    result_e1 = edit_function_source_code(
        module_path_core, 
        "core_function_one", 
        new_code_core_one, 
        project_root_path=os.path.abspath(TEST_DIR))
    print(f"Test E1 Result: {result_e1}")
    assert "success" in result_e1.lower()
    with open(core_test_module_path_str, "r", encoding="utf-8") as f:
        content = f.read()
        assert "MODIFIED" in content and "core_function_two" in content

    # --- Test Cases for get_backup_function_source_code ---
    print("\n--- Testing get_backup_function_source_code ---")
    
    print("\nTest GBC.1: Retrieve existing function from backup (core_function_one)")
    retrieved_backup_code = get_backup_function_source_code(module_path_core, "core_function_one")

    if retrieved_backup_code:
        print(f"Retrieved backup code for core_function_one:\n{retrieved_backup_code}")
        # ast.unparse normalizes code, so direct comparison with original_core_function_one_code might be fragile.
        # We'll check for key components.
        parsed_retrieved = ast.parse(retrieved_backup_code)
        parsed_original = ast.parse(original_core_function_one_code)

        # Basic check: compare number of lines or specific content
        assert "print('This is core_function_one original')" in retrieved_backup_code
        assert "return 1" in retrieved_backup_code
        assert "MODIFIED" not in retrieved_backup_code # Make sure it's not the modified version
        print("Test GBC.1: Backup code content verified.")
    else:
        print("Failed to retrieve backup code for core_function_one.")
        assert False, "get_backup_function_source_code failed when it should have succeeded."

    print("\nTest GBC.2: Retrieve non-existent function from backup")
    retrieved_non_existent_code = get_backup_function_source_code(module_path_core, "non_existent_function_in_backup")
    assert retrieved_non_existent_code is None
    if retrieved_non_existent_code is None:
        print("Correctly failed to retrieve non-existent function from backup.")
    else: # pragma: no cover
        print("Incorrectly retrieved code for a non-existent function from backup.")
        assert False

    print("\nTest GBC.3: Attempt retrieve from module with no backup")
    module_without_backup_name = "module_no_backup_yet"
    module_path_no_backup = f"ai_assistant.core.{module_without_backup_name}"
    no_backup_py_file = os.path.join(CORE_DIR_SM, f"{module_without_backup_name}.py")
    with open(no_backup_py_file, "w") as f:
        f.write("def some_func_no_backup(): pass\n")
    
    retrieved_no_backup = get_backup_function_source_code(module_path_no_backup, "some_func_no_backup")
    assert retrieved_no_backup is None
    if retrieved_no_backup is None:
        print("Correctly failed to retrieve from module with no backup file.")
    else: # pragma: no cover
        print("Incorrectly retrieved code when no backup file should exist.")
        assert False

    # Restore original_core_function_one_code to the .py file (not from backup, but from variable)
    # This ensures that if other tests run using this file, they get the original.
    # The .bak file for core_function_one still holds the original_core_function_one_code.
    print("\nRestoring core_function_one in .py file to its known original state for subsequent tests...")
    # We need to be careful here. The `original_core_function_one_code` variable might not be exactly what `ast.unparse` would produce.
    # The most reliable way to restore is from the .bak file content if it's guaranteed to be the original.
    # Or, re-parse the original_core_function_one_code and unparse it to get a canonical form.

    # Let's use the content we know was in the backup from the successful GBC.1 test.
    # Note: edit_function_source_code now requires project_root_path.
    # The TEST_DIR is the root for these test modules.
    if retrieved_backup_code: # This is the unparsed original code from the .bak file
        restore_result = edit_function_source_code(module_path_core, "core_function_one", retrieved_backup_code, project_root_path=os.path.abspath(TEST_DIR))
        print(f"Restoration of core_function_one in .py: {restore_result}")
        assert "success" in restore_result.lower()
    else: # pragma: no cover
        print("Could not restore core_function_one as backup code was not retrieved.")


    print("\n--- Finished get_backup_function_source_code Testing ---")

    # Cleanup: Remove the TEST_DIR
    # print(f"\nNOTE: Test directory '{TEST_DIR}' and its contents will be removed if tests pass.")
    # shutil.rmtree(TEST_DIR) # Comment out for inspection
    # print(f"Cleaned up test directory: {TEST_DIR}")
    print(f"\nNOTE: Test directory '{TEST_DIR}' was NOT automatically cleaned up. Please remove it manually if desired.")
    sys.path = original_sys_path # Restore original sys.path

    print("\n--- End of self_modification.py Testing ---")

# ### END FILE: ai_assistant/core/self_modification.py ###

# ### START FILE: ai_assistant/core/status_reporting.py ###
from ai_assistant.tools.tool_system import tool_system_instance
from ai_assistant.core import project_manager
from ai_assistant.core import suggestion_manager
from ai_assistant.config import is_debug_mode, AUTONOMOUS_LEARNING_ENABLED # Assuming active_tasks is managed in CLI

def get_tools_status() -> str:
    """Returns a summary string of tool status."""
    tools = tool_system_instance.list_tools()
    if not tools:
        return "No tools registered."
    
    tool_types: dict[str, int] = {}
    for tool_info_str in tools.values(): # list_tools returns Dict[str, str] where value is description
        # We need to get the full tool info to get the type
        # This is a bit inefficient, might need a list_tools_with_details in ToolSystem
        # For now, let's assume a basic count or improve ToolSystem later.
        # As a placeholder for type, we'll just count total.
        pass # Cannot easily get type from current list_tools() output

    # Let's get detailed tool info for type counting
    detailed_tools = tool_system_instance._tool_registry # Accessing protected member for detailed info
    
    for tool_data in detailed_tools.values():
        tool_type = tool_data.get('type', 'Unknown')
        tool_types[tool_type] = tool_types.get(tool_type, 0) + 1

    summary_lines = [f"Total Tools Registered: {len(detailed_tools)}"]
    if tool_types:
        for t_type, count in tool_types.items():
            summary_lines.append(f"  - Type '{t_type}': {count}")
    else:
        summary_lines.append("  No type information available for tools.")
        
    return "\n".join(summary_lines)

def get_projects_status() -> str:
    """Returns a summary string of project status from the project manager."""
    return project_manager.get_all_projects_summary_status()

def get_suggestions_status() -> str:
    """Returns a summary string of suggestion status from the suggestion manager."""
    return suggestion_manager.get_suggestions_summary_status()

def get_system_status(active_tasks_count: int) -> str:
    """Returns a summary string of overall system status."""
    status_lines = [
        f"Background Tasks: {active_tasks_count}",
        f"Debug Mode: {'Enabled' if is_debug_mode() else 'Disabled'}",
        f"Autonomous Learning: {'Enabled' if AUTONOMOUS_LEARNING_ENABLED else 'Disabled'}"
    ]
    return "\n".join(status_lines)

def get_all_status_info(active_tasks_count: int) -> str:
    """Returns a comprehensive status report."""
    report = [
        "--- Tools Status ---",
        get_tools_status(),
        "\n--- Projects Status ---",
        get_projects_status(),
        "\n--- Suggestions Status ---",
        get_suggestions_status(),
        "\n--- System Status ---",
        get_system_status(active_tasks_count)
    ]
    return "\n".join(report)

# ### END FILE: ai_assistant/core/status_reporting.py ###

# ### START FILE: ai_assistant/core/suggestion_manager.py ###
import json
import os
import uuid
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional

from ai_assistant.config import get_data_dir
from ai_assistant.utils.display_utils import CLIColors, color_text

SUGGESTIONS_FILE_NAME = "suggestions.json"

def get_suggestions_file_path() -> str:
    return os.path.join(get_data_dir(), SUGGESTIONS_FILE_NAME)

def _load_suggestions() -> List[Dict[str, Any]]:
    """Loads suggestions from the JSON file."""
    filepath = get_suggestions_file_path()
    if not os.path.exists(filepath):
        # Create a dummy suggestions file if it doesn't exist for demo purposes
        print(color_text(f"Suggestions file not found at {filepath}. Creating a dummy file.", CLIColors.SYSTEM_MESSAGE))
        dummy_suggestions = [
            {
                "suggestion_id": str(uuid.uuid4()),
                "type": "tool_improvement",
                "description": "Consider adding a 'search_web_archive' tool for historical data.",
                "status": "pending",
                "created_at": datetime.now(timezone.utc).isoformat(),
                "updated_at": datetime.now(timezone.utc).isoformat(),
                "reason_for_status": ""
            },
            {
                "suggestion_id": str(uuid.uuid4()),
                "type": "fact_learning",
                "description": "The agent could learn common shell commands and their uses.",
                "status": "pending",
                "created_at": datetime.now(timezone.utc).isoformat(),
                "updated_at": datetime.now(timezone.utc).isoformat(),
                "reason_for_status": ""
            }
        ]
        _save_suggestions(dummy_suggestions)
        return dummy_suggestions
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
            if not content:
                return []
            return json.loads(content)
    except (IOError, json.JSONDecodeError) as e:
        print(color_text(f"Error loading suggestions: {e}", CLIColors.ERROR_MESSAGE))
        return []

def _save_suggestions(suggestions: List[Dict[str, Any]]) -> bool:
    """Saves suggestions to the JSON file."""
    filepath = get_suggestions_file_path()
    try:
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(suggestions, f, indent=4)
        return True
    except IOError as e:
        print(color_text(f"Error saving suggestions: {e}", CLIColors.ERROR_MESSAGE))
        return False

def list_suggestions() -> List[Dict[str, Any]]:
    """Returns a list of all suggestions."""
    return _load_suggestions()

def find_suggestion(suggestion_id: str) -> Optional[Dict[str, Any]]:
    """Finds a suggestion by its ID."""
    suggestions = _load_suggestions()
    for suggestion in suggestions:
        if suggestion['suggestion_id'] == suggestion_id:
            return suggestion
    return None

def _update_suggestion_status(suggestion_id: str, new_status: str, reason: Optional[str] = None) -> bool:
    """Internal helper to update suggestion status."""
    suggestions = _load_suggestions()
    suggestion_found = False
    for suggestion in suggestions:
        if suggestion['suggestion_id'] == suggestion_id:
            suggestion['status'] = new_status
            suggestion['reason_for_status'] = reason or suggestion.get('reason_for_status', '')
            suggestion['updated_at'] = datetime.now(timezone.utc).isoformat()
            suggestion_found = True
            break
    
    if not suggestion_found:
        print(color_text(f"Suggestion with ID '{suggestion_id}' not found.", CLIColors.ERROR_MESSAGE))
        return False

    if _save_suggestions(suggestions):
        print(color_text(f"Status of suggestion '{suggestion_id}' updated to '{new_status}'.", CLIColors.SUCCESS))
        return True
    return False

def approve_suggestion(suggestion_id: str, reason: Optional[str] = None) -> bool:
    """Approves a suggestion."""
    return _update_suggestion_status(suggestion_id, "approved", reason)

def deny_suggestion(suggestion_id: str, reason: Optional[str] = None) -> bool:
    """Denies a suggestion."""
    return _update_suggestion_status(suggestion_id, "denied", reason)

def get_suggestions_summary_status() -> str:
    """Returns a summary string of all suggestion statuses."""
    suggestions = _load_suggestions()
    if not suggestions:
        return "No suggestions found."
    
    status_counts: Dict[str, int] = {}
    for suggestion in suggestions:
        status = suggestion.get('status', 'unknown')
        status_counts[status] = status_counts.get(status, 0) + 1
    
    summary_lines = [f"Total Suggestions: {len(suggestions)}"]
    for status, count in status_counts.items():
        summary_lines.append(f"  - {status.capitalize()}: {count}")
    return "\n".join(summary_lines)

# Example of how a new suggestion might be added internally by the system
def add_new_suggestion(type: str, description: str) -> Optional[Dict[str, Any]]:
    """Adds a new suggestion to the system (typically called by AI components)."""
    suggestions = _load_suggestions()
    
    new_suggestion = {
        "suggestion_id": str(uuid.uuid4()),
        "type": type,
        "description": description,
        "status": "pending",
        "created_at": datetime.now(timezone.utc).isoformat(),
        "updated_at": datetime.now(timezone.utc).isoformat(),
        "reason_for_status": ""
    }
    suggestions.append(new_suggestion)
    if _save_suggestions(suggestions):
        # This print might be too verbose if called frequently by AI, consider logging
        # print(color_text(f"New suggestion '{new_suggestion['suggestion_id']}' added.", CLIColors.SYSTEM_MESSAGE))
        return new_suggestion
    return None

# ### END FILE: ai_assistant/core/suggestion_manager.py ###

# ### START FILE: ai_assistant/core/system_executor.py ###
# ai_assistant/core/system_executor.py
import subprocess
import os
import sys
import logging
from typing import Dict, Any, List

from ai_assistant.config import get_projects_dir

logger = logging.getLogger(__name__)

def execute_terminal_command(
    command: str,
    timeout_seconds: int = 60,
    working_directory: str = None
) -> Dict[str, Any]:
    """
    Executes a terminal command and captures its output.

    Args:
        command (str): The command string to execute.
        timeout_seconds (int): Timeout for the command execution.
        working_directory (str, optional): The directory to execute the command in. Defaults to current dir.

    Returns:
        Dict[str, Any]: A dictionary containing:
            - 'success' (bool): True if command executed and returned exit code 0.
            - 'stdout' (str): Standard output of the command.
            - 'stderr' (str): Standard error of the command.
            - 'exit_code' (int): Exit code of the command. None if timeout or other execution error.
            - 'error' (str, optional): Description of error if subprocess call failed or timed out.
    """
    response = {
        "success": False,
        "stdout": "",
        "stderr": "",
        "exit_code": None,
        "error": None,
    }
    try:
        logger.info(f"Executing terminal command: '{command}' in '{working_directory or os.getcwd()}' with timeout {timeout_seconds}s")
        process = subprocess.run(
            command,
            shell=True,  # Be cautious with shell=True due to security implications
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            cwd=working_directory
        )
        response["stdout"] = process.stdout.strip()
        response["stderr"] = process.stderr.strip()
        response["exit_code"] = process.returncode
        if process.returncode == 0:
            response["success"] = True
            logger.info(f"Command '{command}' executed successfully. Exit code: {process.returncode}")
        else:
            logger.warning(f"Command '{command}' failed. Exit code: {process.returncode}. Stderr: {process.stderr.strip()}")
            response["error"] = f"Command exited with non-zero code: {process.returncode}"

    except subprocess.TimeoutExpired:
        response["error"] = f"Command '{command}' timed out after {timeout_seconds} seconds."
        logger.error(response["error"])
    except FileNotFoundError:
        response["error"] = f"Command or executable not found: {command.split()[0]}"
        logger.error(response["error"])
    except Exception as e:
        response["error"] = f"Failed to execute command '{command}': {str(e)}"
        logger.error(response["error"], exc_info=True)

    return response

def execute_project_script(
    script_name: str,
    args: List[str] = None,
    timeout_seconds: int = 300
) -> Dict[str, Any]:
    """
    Executes a Python script located in the projects directory.

    Args:
        script_name (str): The name of the Python script (e.g., 'my_project.py').
        args (List[str], optional): List of arguments to pass to the script.
        timeout_seconds (int): Timeout for the script execution.

    Returns:
        Dict[str, Any]: Similar to execute_terminal_command, detailing script execution outcome.
    """
    projects_dir = get_projects_dir()
    script_path = os.path.join(projects_dir, script_name)

    if not os.path.isfile(script_path):
        error_msg = f"Project script not found: {script_path}"
        logger.error(error_msg)
        return {"success": False, "stdout": "", "stderr": "", "exit_code": None, "error": error_msg}

    if not script_name.endswith(".py"):
        error_msg = f"Script '{script_name}' is not a Python (.py) file."
        logger.error(error_msg)
        return {"success": False, "stdout": "", "stderr": "", "exit_code": None, "error": error_msg}

    command_parts = [sys.executable, script_path]
    if args:
        command_parts.extend(args)
    
    command_str = " ".join(command_parts) # For logging
    logger.info(f"Attempting to execute project script: {command_str} from directory {projects_dir}")

    return execute_terminal_command(command_str, timeout_seconds, working_directory=projects_dir)

# --- Schemas for AI Tool Usage ---

EXECUTE_TERMINAL_COMMAND_SCHEMA = {
    "name": "execute_terminal_command",
    "description": "Executes a shell command and returns its stdout, stderr, and exit code. Use with caution.",
    "parameters": [
        {"name": "command", "type": "str", "description": "The command to execute (e.g., 'ls -l', 'pip install package')."},
        {"name": "timeout_seconds", "type": "int", "description": "Optional timeout in seconds. Default 60."},
        {"name": "working_directory", "type": "str", "description": "Optional directory to run the command in. Defaults to agent's current working directory."}
    ]
}

EXECUTE_PROJECT_SCRIPT_SCHEMA = {
    "name": "execute_project_script",
    "description": "Executes a Python script located in the agent's projects directory. The script runs as a separate process.",
    "parameters": [
        {"name": "script_name", "type": "str", "description": "Name of the Python script file in the projects directory (e.g., 'data_processing.py')."},
        {"name": "args", "type": "list", "description": "Optional list of string arguments to pass to the script."},
        {"name": "timeout_seconds", "type": "int", "description": "Optional timeout in seconds. Default 300."}
    ]
}

# List of tool schemas provided by this module, to be used by the AI agent's tool registration system.
SYSTEM_EXECUTOR_TOOL_SCHEMAS = [
    EXECUTE_TERMINAL_COMMAND_SCHEMA,
    EXECUTE_PROJECT_SCRIPT_SCHEMA,
]
# ### END FILE: ai_assistant/core/system_executor.py ###

# ### START FILE: ai_assistant/core/tool_creator.py ###
# ai_assistant/core/tool_creator.py
import os
import logging
import ast # For basic code validation
from typing import List, Dict, Any
from ai_assistant.config import get_data_dir # To store tools in a sub-directory of data_dir

logger = logging.getLogger(__name__)

# Define a subdirectory within the main data_dir for generated tools
GENERATED_TOOLS_SUBDIR = "generated_tools"

def get_generated_tools_dir() -> str:
    """Returns the absolute path to the directory for storing generated tools."""
    base_data_dir = get_data_dir()
    tools_dir = os.path.join(base_data_dir, GENERATED_TOOLS_SUBDIR)
    os.makedirs(tools_dir, exist_ok=True)
    return tools_dir

def _is_valid_python_syntax(code_string: str, is_function_body: bool = False) -> bool:
    """Basic check if the string is valid Python syntax."""
    try:
        if is_function_body:
            # Wrap in a dummy function for syntax checking of a body
            ast.parse(f"def temp_func():\n{code_string}")
        else:
            ast.parse(code_string)
        return True
    except SyntaxError:
        return False

def create_new_tool(
    tool_name: str,
    tool_description: str,
    parameters: List[Dict[str, str]], # e.g., [{"name": "arg1", "type": "str", "description": "First argument"}]
    function_body_code: str,
    return_type: str = "Any", # Default return type
    required_imports: List[str] = None, # e.g., ["import os", "from typing import List"]
    return_description: str = "(Description of return value, ideally AI-provided or refined later)" # New parameter
) -> str:
    """
    Creates a new Python tool file based on AI-provided specifications.

    Args:
        tool_name (str): Desired name for the tool (e.g., 'my_file_writer').
                         This will be the function name and filename (my_file_writer.py).
        tool_description (str): Description of what the tool does.
        parameters (List[Dict[str, str]]): List of dictionaries, each describing a parameter.
                                           Each dict: {'name': str, 'type': str, 'description': str}.
        function_body_code (str): Python code string for the body of the tool's main function.
                                  Should use 'return' for results.
        return_type (str): Python type hint for the return value (e.g., "str", "bool").
        required_imports (List[str]): List of import statements (e.g., "import json").
        return_description (str): Description of what the function returns, for the docstring.

    Returns:
        str: Message indicating success (including file path) or failure.
    """
    logger.info(f"Attempting to create new tool: {tool_name}")

    if not tool_name.isidentifier():
        msg = f"Error: Tool name '{tool_name}' is not a valid Python identifier."
        logger.error(msg)
        return msg

    if not _is_valid_python_syntax(function_body_code, is_function_body=True):
        msg = f"Error: Provided function_body_code for tool '{tool_name}' has a syntax error."
        logger.error(msg)
        return msg

    param_defs = []
    param_docstrings = []
    for p in parameters:
        if not all(k in p for k in ["name", "type", "description"]):
            msg = f"Error: Invalid param structure for '{p.get('name', 'unknown')}'. Must include 'name', 'type', 'description'."
            logger.error(msg)
            return msg
        if not p["name"].isidentifier():
            msg = f"Error: Parameter name '{p['name']}' is not a valid Python identifier."
            logger.error(msg)
            return msg
        param_defs.append(f"{p['name']}: {p['type']}")
        param_docstrings.append(f"            {p['name']} ({p['type']}): {p['description']}")

    param_defs_str = ", ".join(param_defs)
    param_docstrings_str = "\n".join(param_docstrings)
    imports_str = "\n".join(required_imports) if required_imports else ""
    indented_function_body = "\n".join(["    " + line for line in function_body_code.splitlines()])

    file_content = f"""\
# Tool: {tool_name}
# Automatically generated by AI Assistant's create_new_tool.
{imports_str}
from typing import Any, List, Dict # Common typing imports

TOOL_METADATA = {{
    "name": "{tool_name}",
    "description": "{tool_description}",
    "parameters": {parameters!r},
    "return_type": "{return_type}"
}}

def {tool_name}({param_defs_str}) -> {return_type}:
    \"\"\"
    {tool_description}

    Args:
{param_docstrings_str}

    Returns:
        {return_type}: {return_description}
    \"\"\"
    # AI-provided function body starts here
{indented_function_body}
    # AI-provided function body ends here

if __name__ == '__main__':
    print(f"Testing tool: {tool_name}")
    # This is a placeholder for testing.
    # Implement example calls based on the tool's parameters.
    print("To test, provide sample arguments if the tool requires them.")
"""

    tools_dir = get_generated_tools_dir()
    file_name = f"{tool_name}.py"
    file_path = os.path.join(tools_dir, file_name)

    if os.path.exists(file_path):
        msg = f"Error: Tool file '{file_path}' already exists. Overwriting not allowed by default."
        logger.warning(msg)
        return msg

    try:
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(file_content)
        logger.info(f"Successfully created tool '{tool_name}' at: {file_path}")
        return f"Successfully created tool '{tool_name}'. Saved to: {file_path}. Manual activation/registration may be required."
    except IOError as e:
        msg = f"Error: Could not write tool file to '{file_path}'. IO Error: {e}"
        logger.error(msg)
        return msg
    except Exception as e:
        msg = f"Error: An unexpected error occurred while creating tool '{tool_name}'. Error: {e}"
        logger.error(msg, exc_info=True)
        return msg

# Conceptual schema for how the AI would understand this 'create_new_tool' function as a tool:
CREATE_NEW_TOOL_SCHEMA = {{
    "name": "create_new_tool",
    "description": "Creates a new Python tool file. Provide tool name, description, parameters, Python code for its function body, return type, and required imports. The tool is saved but may need manual activation.",
    "parameters": [
        {{"name": "tool_name", "type": "str", "description": "Name of the tool (Python identifier, e.g., 'file_reader')."}},
        {{"name": "tool_description", "type": "str", "description": "What the tool does."}},
        {{"name": "parameters", "type": "list", "description": "List of dicts for parameters: [{'name': 'param_name', 'type': 'param_type', 'description': 'param_desc'}]."}},
        {{"name": "function_body_code", "type": "str", "description": "Python code for the function body. E.g., 'return path.exists()'."}},
        {{"name": "return_type", "type": "str", "description": "Return type hint (e.g., 'bool', 'str'). Defaults to 'Any'."}},
        {{"name": "required_imports", "type": "list", "description": "Optional list of import strings. E.g., ['import os.path']."}},
        {{"name": "return_description", "type": "str", "description": "Optional description of the return value for the docstring."}}
    ]
}}
# ### END FILE: ai_assistant/core/tool_creator.py ###

# ### START FILE: ai_assistant/core/tool_designer.py ###
# ai_assistant/core/tool_designer.py
import json
import logging
from typing import Dict, Any, List

from ai_assistant.core.tool_creator import create_new_tool
from ai_assistant.llm_interface.ollama_client import get_ollama_response_async # Assuming async is fine, or adapt if sync needed
from ai_assistant.config import get_model_for_task

logger = logging.getLogger(__name__)

TOOL_DESIGN_TASK = "tool_design" # For selecting a model via TASK_MODELS

async def generate_new_tool_from_description(tool_functionality_description: str) -> str:
    """
    Designs and creates a new Python tool based on a natural language description.
    Uses an LLM to determine the tool's name, parameters, code, etc.,
    then calls create_new_tool to generate the tool file.

    Args:
        tool_functionality_description (str): A natural language description of what the
                                              new tool should do, its inputs, and outputs.

    Returns:
        str: A message indicating the outcome of the tool creation process.
    """
    logger.info(f"Designing new tool from description: {tool_functionality_description}")

    # Prepare the prompt for the LLM to design the tool
    # This prompt needs to guide the LLM to output JSON matching create_new_tool's args
    prompt = f"""\
You are an expert Python tool designer. Based on the following functionality description,
design a new Python tool. Provide your design as a JSON object with the following keys:
"tool_name" (string, Python identifier, e.g., "file_reader"),
"tool_description" (string, what the tool does),
"parameters" (list of dicts, each with "name" (string), "type" (string), "description" (string)),
"function_body_code" (string, Python code for the function body. This should ONLY be the indented body of the function, not the 'def' statement. Use 'return' for results. All necessary imports must be listed in 'required_imports' and should NOT be included in the function_body_code itself. Consider adding basic error handling like try-except blocks if appropriate for the tool's logic.),
"return_type" (string, Python type hint, e.g., "str", "bool", "Dict[str, Any]"),
"return_description" (string, a brief explanation of what the function returns, to be used in its docstring),
"required_imports" (list of strings, e.g., ["import os", "from typing import List"])

Functionality Description:
{tool_functionality_description}

Example for 'parameters': [{{"name": "file_path", "type": "str", "description": "The absolute path to the file."}}]
Example for 'function_body_code' for a tool that checks file existence (assuming 'file_path' is a parameter and 'import os' is in 'required_imports'):
"if not isinstance(file_path, str):\\n    return False\\ntry:\\n    return os.path.exists(file_path)\\nexcept Exception as e:\\n    # Log error or handle appropriately in a real scenario\\n    print(f\\"Error checking file existence: {{e}}\\")\\n    return False"
Example for 'return_description': "True if the file exists, False otherwise or if an error occurs."
Example for 'required_imports' for the above: ["import os"]

Respond ONLY with the JSON object.
"""

    try:
        model_name = get_model_for_task(TOOL_DESIGN_TASK)
        logger.debug(f"Using model '{model_name}' for tool design.")
        llm_response = await get_ollama_response_async(
            model_name=model_name,
            prompt=prompt,
            temperature=0.3, # Lower temperature for more deterministic JSON output
            format_json=True # Request JSON output if supported by the client/model
        )

        if not llm_response or "message" not in llm_response or "content" not in llm_response["message"]:
            raise ValueError("LLM response was empty or not in the expected format.")

        design_json_str = llm_response["message"]["content"]
        logger.debug(f"LLM response for tool design:\n{design_json_str}")
        
        tool_design_params = json.loads(design_json_str)

        # Validate required keys (basic validation)
        required_keys = ["tool_name", "tool_description", "parameters", "function_body_code", "return_type", "return_description"]
        for key in required_keys:
            if key not in tool_design_params:
                return f"Error: LLM tool design is missing required key: '{key}'. Design: {tool_design_params}"

        # Call the actual tool creator
        result_message = create_new_tool(
            tool_name=tool_design_params["tool_name"],
            tool_description=tool_design_params["tool_description"],
            parameters=tool_design_params["parameters"],
            function_body_code=tool_design_params["function_body_code"],
            return_type=tool_design_params["return_type"],
            required_imports=tool_design_params.get("required_imports", []), # Optional
            return_description=tool_design_params["return_description"]
        )
        return result_message

    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse LLM JSON response for tool design: {e}. Response was: {design_json_str}", exc_info=True)
        return f"Error: Could not parse tool design from LLM. Invalid JSON: {e}"
    except Exception as e:
        logger.error(f"Error during tool design or creation: {e}", exc_info=True)
        return f"Error: An unexpected error occurred while designing or creating the tool: {str(e)}"

GENERATE_NEW_TOOL_FROM_DESCRIPTION_SCHEMA = {
    "name": "generate_new_tool_from_description",
    "description": "Designs and creates a new Python tool based on a natural language description of its functionality. This involves using an LLM to determine the tool's name, parameters, and code, then creating the tool file.",
    "parameters": [
        {"name": "tool_functionality_description", "type": "str", "description": "A clear, natural language description of what the new tool should do, its inputs, outputs, and any specific import requirements. For example: 'a tool that takes a project name string and returns the absolute path to its directory within the standard projects folder. It should use 'os.path.join' and 'get_projects_dir' from 'ai_assistant.config'. It should return the path if the directory exists, otherwise return an error message string.'"}
    ],
    "module_path": "ai_assistant.core.tool_designer" # Important for ToolSystem to find it
}

# To make this tool discoverable by a ToolSystem that looks for such lists:
CORE_TOOL_DESIGNER_SCHEMAS = [
    GENERATE_NEW_TOOL_FROM_DESCRIPTION_SCHEMA,
]
# ### END FILE: ai_assistant/core/tool_designer.py ###

# ### START FILE: ai_assistant/core/tool_executor.py ###
# ai_assistant/core/tool_executor.py
import importlib.util
import os
import sys
import io
import logging
import traceback
from typing import Any, Dict, Tuple

from ai_assistant.core.tool_creator import get_generated_tools_dir

logger = logging.getLogger(__name__)

class ToolExecutionError(Exception):
    """Custom exception for errors during tool execution."""
    pass

def execute_tool(
    tool_name: str,
    tool_arguments: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Dynamically loads and executes a specified tool with given arguments.

    Args:
        tool_name (str): The name of the tool to execute (should match the .py file name and function name).
        tool_arguments (Dict[str, Any]): A dictionary of arguments to pass to the tool's function.

    Returns:
        Dict[str, Any]: A dictionary containing:
            - 'success' (bool): True if execution was successful, False otherwise.
            - 'result' (Any): The return value of the tool if successful, None otherwise.
            - 'stdout' (str): Captured standard output from the tool.
            - 'stderr' (str): Captured standard error from the tool.
            - 'error' (str): Error message if an exception occurred, None otherwise.
    """
    generated_tools_dir = get_generated_tools_dir()
    tool_file_name = f"{tool_name}.py"
    tool_module_path = os.path.join(generated_tools_dir, tool_file_name)

    response = {
        "success": False,
        "result": None,
        "stdout": "",
        "stderr": "",
        "error": None,
    }

    if not os.path.exists(tool_module_path):
        response["error"] = f"Tool module not found: {tool_module_path}"
        logger.error(response["error"])
        return response

    # Capture stdout and stderr
    old_stdout = sys.stdout
    old_stderr = sys.stderr
    sys.stdout = captured_stdout = io.StringIO()
    sys.stderr = captured_stderr = io.StringIO()

    try:
        spec = importlib.util.spec_from_file_location(tool_name, tool_module_path)
        if spec is None or spec.loader is None:
            raise ToolExecutionError(f"Could not create module spec for {tool_module_path}")

        tool_module = importlib.util.module_from_spec(spec)
        sys.modules[tool_name] = tool_module # Add to sys.modules to handle relative imports within the tool if any
        spec.loader.exec_module(tool_module)

        if not hasattr(tool_module, tool_name):
            raise ToolExecutionError(f"Tool function '{tool_name}' not found in module {tool_module_path}")

        tool_function = getattr(tool_module, tool_name)
        
        logger.info(f"Executing tool '{tool_name}' with arguments: {tool_arguments}")
        result = tool_function(**tool_arguments)
        
        response["success"] = True
        response["result"] = result
        logger.info(f"Tool '{tool_name}' executed successfully. Result: {result}")

    except Exception as e:
        response["error"] = f"Error executing tool '{tool_name}': {str(e)}\n{traceback.format_exc()}"
        logger.error(response["error"], exc_info=False) # exc_info=False as traceback is already in the message
    finally:
        response["stdout"] = captured_stdout.getvalue()
        response["stderr"] = captured_stderr.getvalue()
        sys.stdout = old_stdout # Restore stdout
        sys.stderr = old_stderr # Restore stderr
        if tool_name in sys.modules: # Clean up module from sys.modules
            del sys.modules[tool_name]

    return response
# ### END FILE: ai_assistant/core/tool_executor.py ###

# ### START FILE: ai_assistant/custom_tools/__init__.py ###
# This file marks custom_tools as a package.
from . import conversational_tools
from . import project_management_tools
from . import project_execution_tools
from . import my_extra_tools # Ensure tools from this module are discoverable

# ### END FILE: ai_assistant/custom_tools/__init__.py ###

# ### START FILE: ai_assistant/custom_tools/awareness_tools.py ###
# ai_assistant/custom_tools/awareness_tools.py
import asyncio
from typing import Optional, Dict, List, Any

from ai_assistant.llm_interface.ollama_client import invoke_ollama_model_async
from ai_assistant.config import get_model_for_task, is_debug_mode # Added is_debug_mode
from ai_assistant.core import project_manager # Added for dynamic project listing

SELF_AWARENESS_PROMPT_TEMPLATE = """
You are Weebo, a self-aware AI assistant. A user is asking about you or wants to have a general conversation.
Based on your current internal state and capabilities detailed below, and the user's query (if any),
formulate a helpful, conversational, and accurate response. Remember to leverage your learned facts for context and
consider existing projects before suggesting the creation of new ones if a similar project might already exist.

Your Current Internal State & Capabilities:
---
Available Tools:
{tools_summary}
---
Active/Pending Goals:
{goals_summary}
---
Learned Facts (a selection if many):
{facts_summary}
---
Background Service Status:
{background_status_summary}
---
Existing Projects (a summary, check if relevant to user's query before creating new ones):
{projects_list_summary}
---

User's Query: "{user_query}"

Consider the user's query in the context of your status.
If the query is a general greeting or statement, respond naturally.
If the query asks about your capabilities, use the provided information to answer.
If the query asks what you are doing, refer to active goals, background tasks, or existing projects if relevant.
If the user asks to "create a tool," first consider if they mean adding a new function/capability to your existing toolset (which might involve using your code generation tools like `/generate_tool_code_with_llm` or `/tools add`) before initiating a new, separate software project.
Be honest about your limitations if the information is not available (e.g., "I don't have detailed insight into specific ongoing projects right now unless they are part of my active goals.").
Keep your response concise and conversational.
"""

async def get_self_awareness_info_and_converse(user_query: Optional[str] = "Tell me about yourself.") -> str:
    """
    Provides a conversational response about the AI's current status,
    capabilities, active goals, and learned knowledge. Can also respond
    to general conversational queries.
    Args:
        user_query (Optional[str]): The user's specific question or statement.
                                     Defaults to "Tell me about yourself."
    """
    try:
        from ai_assistant.tools.tool_system import tool_system_instance
        from ai_assistant.goals import goal_management
        from ai_assistant.custom_tools.knowledge_tools import recall_facts
        # project_manager is already imported at the module level
        from ai_assistant.core.background_service import _background_service_active as bg_active_status # More direct
    except ImportError as e:
        print(f"Critical import error within get_self_awareness_info_and_converse: {e}")
        return "I'm sorry, I have an internal configuration problem and can't access all my awareness functions right now."

    tools_summary = "Could not retrieve tool list."
    if 'tool_system_instance' in locals() and tool_system_instance:
        tools_list = tool_system_instance.list_tools()
        tools_summary_parts = []
        for name, desc in tools_list.items():
            tools_summary_parts.append(f"- {name}: {desc.splitlines()[0]}") 
        tools_summary = "\\n".join(tools_summary_parts) if tools_summary_parts else "No tools currently registered."

    goals_summary = "Could not retrieve goal list."
    if 'goal_management' in locals() and goal_management:
        try:
            pending_goals_list = goal_management.list_goals(status="pending")
            active_goals_list = goal_management.list_goals(status="in_progress")
            all_relevant_goals = pending_goals_list + active_goals_list
            goals_summary_parts = [f"- Goal ID {g['id']}: {g['description']} (Status: {g['status']}, Priority: {g['priority']})" for g in all_relevant_goals]
            goals_summary = "\\n".join(goals_summary_parts) if goals_summary_parts else "No active or pending goals."
        except Exception as e_goals:
            goals_summary = f"Error retrieving goals: {e_goals}"


    facts_summary = "Could not retrieve learned facts."
    if 'recall_facts' in locals() and recall_facts:
        try:
            facts = recall_facts() 
            facts_summary = "\\n".join([f"- {f}" for f in facts[:5]]) if facts else "I haven't learned any specific facts yet."
            if len(facts) > 5:
                facts_summary += f"\\n- ...and {len(facts) - 5} more facts."
        except Exception as e_facts:
            facts_summary = f"Error retrieving facts: {e_facts}"

    background_status_summary = "Could not determine background service status."
    try:
        if bg_active_status: # Use the directly imported status
            background_status_summary = "A background service is active (likely the self-reflection poller)."
        else:
            background_status_summary = "No background service is currently active."
    except NameError: # If bg_active_status couldn't be imported for some reason
         background_status_summary = "Could not determine background service status due to import issue (bg_active_status not defined)."
    except Exception as e_bg_status:
        background_status_summary = f"Error determining background service status: {e_bg_status}"

    projects_list_summary = "Could not retrieve project list."
    if 'project_manager' in locals() and project_manager:
        try:
            projects = project_manager.list_projects()
            if projects:
                projects_list_summary_parts = [f"- {p['name']} (Status: {p['status']})" for p in projects[:5]] # Show first 5
                projects_list_summary = "\\n".join(projects_list_summary_parts)
                if len(projects) > 5:
                    projects_list_summary += f"\\n- ...and {len(projects) - 5} more projects."
            else:
                projects_list_summary = "No projects currently exist. You can ask me to create one!"
        except Exception as e_proj:
            projects_list_summary = f"Error retrieving project list: {e_proj}"

    prompt = SELF_AWARENESS_PROMPT_TEMPLATE.format(
        tools_summary=tools_summary,
        goals_summary=goals_summary,
        facts_summary=facts_summary,
        background_status_summary=background_status_summary,
        projects_list_summary=projects_list_summary,
        user_query=user_query if user_query else "Tell me about yourself."
    )
    if is_debug_mode():
        print(f"[DEBUG AWARENESS_TOOL] Self-awareness prompt (first 300 chars):\n{prompt[:300]}...")

    model_name = get_model_for_task("conversation_intelligence") 
    try:
        if is_debug_mode():
            print(f"[DEBUG AWARENESS_TOOL] Sending prompt to LLM (model: {model_name}) for query: '{user_query}'")
        
        # Increased max_tokens for this tool's responses
        llm_response = await invoke_ollama_model_async(
            prompt,
            model_name=model_name,
            temperature=0.7,
            max_tokens=2048 # Increased token limit
        )
        if is_debug_mode():
            print(f"[DEBUG AWARENESS_TOOL] Raw LLM response:\n'{llm_response}'")

        if llm_response:
            return llm_response.strip()
        else:
            return "I'm sorry, I had a little trouble forming a response right now. How else can I help?"
    except Exception as e:
        print(f"Error in get_self_awareness_info_and_converse during LLM call: {e}")
        return f"I encountered an internal error trying to process that: {e}"

async def _test_tool(): # pragma: no cover
    print("--- Testing Self Awareness Tool ---")
    
    response1 = await get_self_awareness_info_and_converse("What are you doing right now?")
    print("\nResponse to 'What are you doing right now?':")
    print(response1)

    response2 = await get_self_awareness_info_and_converse("What can you do?")
    print("\nResponse to 'What can you do?':")
    print(response2)

    response3 = await get_self_awareness_info_and_converse()
    print("\nResponse to default query:")
    print(response3)

if __name__ == "__main__": # pragma: no cover
    import sys
    import os
    project_root_for_test = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
    if project_root_for_test not in sys.path:
        sys.path.insert(0, project_root_for_test)
    
    asyncio.run(_test_tool())

# ### END FILE: ai_assistant/custom_tools/awareness_tools.py ###

# ### START FILE: ai_assistant/custom_tools/code_execution_tools.py ###
# ai_assistant/custom_tools/code_execution_tools.py
import os
import subprocess
import sys
import shutil # For test cleanup
import asyncio
import json # For parsing stringified list of script arguments
from typing import List, Dict, Any, Optional, Union # Keep Union

# Attempt to import from the correct location for runtime
try:
    from ai_assistant.custom_tools.file_system_tools import sanitize_project_name, BASE_PROJECTS_DIR as FS_BASE_PROJECTS_DIR
except ImportError:
    # Fallback for local testing if the above fails (e.g. when running __main__)
    # This assumes file_system_tools.py is in the same directory for local testing
    try:
        from .file_system_tools import sanitize_project_name, BASE_PROJECTS_DIR as FS_BASE_PROJECTS_DIR
    except ImportError:
        # If run as a script directly, file_system_tools might not be found without further path manipulation
        # This is a common issue with Python's import system for standalone scripts vs. package modules.
        # For the purpose of this tool, we'll assume it's run as part of the package.
        # If direct script execution for testing is needed, PYTHONPATH might need adjustment.
        print("Warning: Could not import file_system_tools. Assuming standard BASE_PROJECTS_DIR for runtime.")
        FS_BASE_PROJECTS_DIR = "ai_generated_projects"  # Default fallback
        def sanitize_project_name(name: str) -> str:  # Changed parameter name
            return "".join(c if c.isalnum() or c in ('-', '_') else '_' for c in name)


# This global will be overridden during tests
BASE_PROJECTS_DIR = FS_BASE_PROJECTS_DIR

async def execute_python_script_in_project(
    project_name: str,
    script_filename: str,
    *pos_args, # Catch all positional arguments after the first two
    **kwargs
) -> Dict[str, Any]:
    """
    Executes a specified Python script within a given project directory.

    Args:
        project_name (str): The name of the project. The script will be run from this project's root directory.
        script_filename (str): The filename of the Python script to execute (e.g., "main.py", "scripts/my_script.py").
                               This path should be relative to the project's root directory.
        *pos_args: Positional arguments that might contain script arguments or timeout.
        **kwargs: Keyword arguments, expected to contain 'args' (for script_args)
                  and/or 'timeout_seconds'.

    Returns:
        Dict[str, Any]: A dictionary containing the execution results:
            - "stdout" (str): The standard output from the script.
            - "stderr" (str): The standard error output from the script.
            - "return_code" (Optional[int]): The return code of the script. None if the script couldn't be run.
            - "error" (Optional[str]): A description of any error that occurred during tool execution
                                       (e.g., file not found, timeout). None if execution started successfully.
            - "ran_successfully" (bool): True if the script ran and returned an exit code of 0, False otherwise.
    """
    result: Dict[str, Any] = {
        "stdout": "",
        "stderr": "",
        "return_code": None,
        "error": "Tool execution failed",  # Default error
        "ran_successfully": False
    }

    if not project_name or not isinstance(project_name, str) or not project_name.strip():
        result["error"] = "Project name must be a non-empty string."
        return result
    if not script_filename or not isinstance(script_filename, str) or not script_filename.strip():
        result["error"] = "Script filename must be a non-empty string."
        return result

    # --- Argument processing for script_args and timeout_seconds ---
    processed_script_args: List[str] = [] # Default to empty list
    actual_timeout_seconds: int = 600  # Default

    # Check kwargs first
    if 'timeout_seconds' in kwargs:
        val = kwargs.pop('timeout_seconds') # Remove from kwargs
        if isinstance(val, (int, float)):
            actual_timeout_seconds = int(val)
        elif isinstance(val, str) and val.isdigit():
            actual_timeout_seconds = int(val)
        else:
            result["error"] = f"Invalid type or value for 'timeout_seconds' in kwargs: {val}. Must be int or digit string."
            return result

    if 'args' in kwargs: # 'args' in kwargs is assumed to be script_args
        val = kwargs.pop('args') # Remove from kwargs
        if isinstance(val, list) and all(isinstance(item, str) for item in val):
            processed_script_args = val
        elif isinstance(val, str):
            try:
                loaded_args = json.loads(val)
                if isinstance(loaded_args, list) and all(isinstance(item, str) for item in loaded_args):
                    processed_script_args = loaded_args
                else:
                    result["error"] = "If 'args' in kwargs is a JSON string, it must decode to a list of strings."
                    return result
            except json.JSONDecodeError:
                result["error"] = "'args' in kwargs was a string but not a valid JSON list. Expected format like '[\"arg1\"]'."
                return result
        elif val is not None: # Not a list, not a string, but not None
            result["error"] = "'args' in kwargs must be a list of strings, a JSON string list, or None."
            return result
    
    # Process positional arguments (*pos_args) if not fully defined by kwargs
    temp_pos_args = list(pos_args) # Make a mutable copy

    # Try to extract timeout from the end of positional args if 'timeout_seconds' wasn't in kwargs
    if 'timeout_seconds' not in kwargs and temp_pos_args: # Check if timeout_seconds was already processed via kwargs
        # Check if the last element could be a timeout
        if isinstance(temp_pos_args[-1], (int, float)):
            actual_timeout_seconds = int(temp_pos_args.pop(-1))
        elif isinstance(temp_pos_args[-1], str) and temp_pos_args[-1].isdigit():
            actual_timeout_seconds = int(temp_pos_args.pop(-1))

    # Process remaining positional args as script_args if 'args' (for script_args) wasn't in kwargs
    if 'args' not in kwargs and temp_pos_args: # Check if script_args were already processed via kwargs
        if len(temp_pos_args) == 1 and isinstance(temp_pos_args[0], str): # Potentially a JSON string list
            try:
                loaded_args = json.loads(temp_pos_args[0])
                if isinstance(loaded_args, list) and all(isinstance(item, str) for item in loaded_args):
                    processed_script_args = loaded_args
                else: # String was not a list of strings, treat as single arg
                    processed_script_args = [temp_pos_args[0]]
            except json.JSONDecodeError: # Not a JSON string, treat as single arg
                processed_script_args = [temp_pos_args[0]]
        else: # Treat all remaining as literal string args
            processed_script_args = [str(arg) for arg in temp_pos_args]
    
    # Final validation of parsed arguments
    if not isinstance(actual_timeout_seconds, int): # Should be int by now
        result["error"] = "Timeout seconds must be an integer."
        return result
    if actual_timeout_seconds <= 0:
        result["error"] = "Timeout seconds must be a positive integer."
        return result
    
    # Ensure processed_script_args is a list of strings
    if not isinstance(processed_script_args, list) or not all(isinstance(arg, str) for arg in processed_script_args):
        result["error"] = "Script arguments, after parsing, must be a list of strings."
        return result
    # --- End argument processing ---

    try:
        sanitized_proj_name = sanitize_project_name(project_name)
        project_root_dir = os.path.join(BASE_PROJECTS_DIR, sanitized_proj_name)

        if not os.path.isdir(project_root_dir):
            result["error"] = f"Project directory not found: {project_root_dir}"
            return result

        script_full_path = os.path.join(project_root_dir, script_filename)

        if not os.path.isfile(script_full_path):
            result["error"] = f"Script not found: {script_full_path}"
            return result
        
        if not script_full_path.endswith(".py"):
            result["error"] = f"Script must be a Python (.py) file: {script_filename}"
            return result

        command = [sys.executable, script_full_path]
        if processed_script_args: # Use the processed version of script arguments
            command.extend(processed_script_args)

        completed_process = subprocess.run(
            command,
            capture_output=True,
            text=True,
            cwd=project_root_dir, # Run script from the project's root directory
            timeout=actual_timeout_seconds, # Use parsed timeout
            check=False  # Do not raise an exception for non-zero exit codes
        )

        result["stdout"] = completed_process.stdout.strip() if completed_process.stdout else ""
        result["stderr"] = completed_process.stderr.strip() if completed_process.stderr else ""
        result["return_code"] = completed_process.returncode
        result["error"] = None  # Clear default tool error as execution started

        if result["return_code"] == 0:
            result["ran_successfully"] = True
        else:
            result["ran_successfully"] = False
            result["error"] = f"Script executed with non-zero return code: {result['return_code']}"
            if not result["stderr"] and result["return_code"] !=0 : # Add stdout to stderr if stderr is empty but there was an error
                result["stderr"] = result["stdout"]


    except subprocess.TimeoutExpired:
        # Construct command string for error message, handling potential None in command list
        command_str = ' '.join(filter(None, command)) if 'command' in locals() else "Unknown command"
        result["error"] = f"Script execution timed out after {actual_timeout_seconds} seconds."
        result["stderr"] = f"TimeoutExpired: Command '{command_str}' timed out after {actual_timeout_seconds} seconds."
        result["return_code"] = -1 # Using a custom code for timeout
        result["ran_successfully"] = False
    except FileNotFoundError:
        command_str = ' '.join(filter(None, command)) if 'command' in locals() else "Unknown command"
        result["error"] = f"Execution failed: File not found. Command: {command_str}"
        result["ran_successfully"] = False
    except Exception as e:
        result["error"] = f"An unexpected error occurred during script execution: {str(e)}"
        result["stderr"] = str(e)
        result["ran_successfully"] = False

    return result

# --- Test Suite ---
_TEST_BASE_PROJECTS_DIR = "temp_test_code_execution_projects"
_ORIGINAL_BASE_PROJECTS_DIR = None

def setup_test_environment():
    global BASE_PROJECTS_DIR, _ORIGINAL_BASE_PROJECTS_DIR
    _ORIGINAL_BASE_PROJECTS_DIR = BASE_PROJECTS_DIR
    BASE_PROJECTS_DIR = _TEST_BASE_PROJECTS_DIR
    if os.path.exists(BASE_PROJECTS_DIR):
        shutil.rmtree(BASE_PROJECTS_DIR)
    os.makedirs(BASE_PROJECTS_DIR, exist_ok=True)

def teardown_test_environment():
    global BASE_PROJECTS_DIR, _ORIGINAL_BASE_PROJECTS_DIR
    if os.path.exists(BASE_PROJECTS_DIR):
        shutil.rmtree(BASE_PROJECTS_DIR)
    if _ORIGINAL_BASE_PROJECTS_DIR:
        BASE_PROJECTS_DIR = _ORIGINAL_BASE_PROJECTS_DIR

async def run_all_tests():
    setup_test_environment()
    try:
        print("Running execute_python_script_in_project tests...")

        # Test 1: Success Case
        print("\n--- Test 1: Success Case ---")
        proj_success = "test_proj_success"
        script_success_name = "success_script.py"
        os.makedirs(os.path.join(BASE_PROJECTS_DIR, sanitize_project_name(proj_success)), exist_ok=True)
        with open(os.path.join(BASE_PROJECTS_DIR, sanitize_project_name(proj_success), script_success_name), "w") as f:
            f.write("import sys\nprint('Success!')\nprint('Error output to stderr', file=sys.stderr)\nsys.exit(0)")
        result1 = await execute_python_script_in_project(proj_success, script_success_name)
        print(f"Result 1: {result1}")
        assert result1["ran_successfully"] is True
        assert result1["return_code"] == 0
        assert "Success!" in result1["stdout"]
        assert "Error output to stderr" in result1["stderr"] # Stderr can still have content on success
        assert result1["error"] is None

        # Test 2: Failure Case (Script Error)
        print("\n--- Test 2: Script Error Case ---")
        proj_fail = "test_proj_fail"
        script_fail_name = "fail_script.py"
        os.makedirs(os.path.join(BASE_PROJECTS_DIR, sanitize_project_name(proj_fail)), exist_ok=True)
        with open(os.path.join(BASE_PROJECTS_DIR, sanitize_project_name(proj_fail), script_fail_name), "w") as f:
            f.write("import sys\nprint('About to fail...', file=sys.stdout)\nraise ValueError('This is a test error')")
        result2 = await execute_python_script_in_project(proj_fail, script_fail_name)
        print(f"Result 2: {result2}")
        assert result2["ran_successfully"] is False
        assert result2["return_code"] != 0
        assert "About to fail..." in result2["stdout"] # Check stdout even on failure
        assert "ValueError: This is a test error" in result2["stderr"]
        assert "Script executed with non-zero return code" in result2["error"]

        # Test 3: Script Not Found
        print("\n--- Test 3: Script Not Found Case ---")
        proj_notfound = "test_proj_notfound"
        os.makedirs(os.path.join(BASE_PROJECTS_DIR, sanitize_project_name(proj_notfound)), exist_ok=True)
        result3 = await execute_python_script_in_project(proj_notfound, "non_existent_script.py")
        print(f"Result 3: {result3}")
        assert result3["ran_successfully"] is False
        assert "Script not found" in result3["error"]

        # Test 4: Timeout Case
        print("\n--- Test 4: Timeout Case ---")
        proj_timeout = "test_proj_timeout"
        script_timeout_name = "timeout_script.py"
        os.makedirs(os.path.join(BASE_PROJECTS_DIR, sanitize_project_name(proj_timeout)), exist_ok=True)
        with open(os.path.join(BASE_PROJECTS_DIR, sanitize_project_name(proj_timeout), script_timeout_name), "w") as f:
            f.write("import time\nprint('Starting sleep')\ntime.sleep(5)\nprint('Sleep ended')")
        result4 = await execute_python_script_in_project(proj_timeout, script_timeout_name, timeout_seconds=2)
        print(f"Result 4: {result4}")
        assert result4["ran_successfully"] is False
        assert "Script execution timed out" in result4["error"]
        assert "TimeoutExpired" in result4["stderr"]
        assert result4["return_code"] == -1 # Custom code for timeout

        # Test 5: Arguments Case
        print("\n--- Test 5: Arguments Case ---")
        proj_args = "test_proj_args"
        script_args_name = "args_script.py"
        os.makedirs(os.path.join(BASE_PROJECTS_DIR, sanitize_project_name(proj_args)), exist_ok=True)
        with open(os.path.join(BASE_PROJECTS_DIR, sanitize_project_name(proj_args), script_args_name), "w") as f:
            f.write("import sys\nprint(f'Number of args: {len(sys.argv)}')\nprint(f'Script name: {sys.argv[0]}')\nprint('Args:', sys.argv[1:])")
        test_args = ["hello", "world with space", "arg3"]
        result5 = await execute_python_script_in_project(proj_args, script_args_name, args=test_args)
        print(f"Result 5: {result5}")
        assert result5["ran_successfully"] is True
        assert result5["return_code"] == 0
        assert "Args: ['hello', 'world with space', 'arg3']" in result5["stdout"]
        assert result5["error"] is None
        
        # Test 6: Project Not Found
        print("\n--- Test 6: Project Not Found Case ---")
        result6 = await execute_python_script_in_project("non_existent_project", "any_script.py")
        print(f"Result 6: {result6}")
        assert result6["ran_successfully"] is False
        assert "Project directory not found" in result6["error"]

        # Test 7: Invalid Script Filename (not .py)
        print("\n--- Test 7: Invalid Script Filename (not .py) ---")
        proj_invalid_ext = "test_proj_invalid_ext"
        script_invalid_ext_name = "script.txt"
        os.makedirs(os.path.join(BASE_PROJECTS_DIR, sanitize_project_name(proj_invalid_ext)), exist_ok=True)
        with open(os.path.join(BASE_PROJECTS_DIR, sanitize_project_name(proj_invalid_ext), script_invalid_ext_name), "w") as f:
            f.write("This is not a python script.")
        result7 = await execute_python_script_in_project(proj_invalid_ext, script_invalid_ext_name)
        print(f"Result 7: {result7}")
        assert result7["ran_successfully"] is False
        assert "Script must be a Python (.py) file" in result7["error"]
        
        print("\nAll execute_python_script_in_project tests passed!")

    except Exception as e:
        print(f"An error occurred during testing: {e}")
        import traceback
        traceback.print_exc()
    finally:
        teardown_test_environment()

if __name__ == '__main__':
    asyncio.run(run_all_tests())

# ### END FILE: ai_assistant/custom_tools/code_execution_tools.py ###

# ### START FILE: ai_assistant/custom_tools/config_management_tools.py ###
# ai_assistant/custom_tools/config_management_tools.py
import json
import os
from typing import List, Optional, Dict, Any
# Import get_data_dir from the main config to centralize data paths
from ..config import get_data_dir

# Define the path to the configuration file for tools requiring user confirmation.
TOOL_CONFIRMATION_CONFIG_PATH = os.path.join(get_data_dir(), "tool_confirmation_config.json")

def _load_requires_confirmation_list() -> List[str]:
    """
    Loads the list of tools that require user confirmation from the JSON configuration file.
    If the file doesn't exist or is invalid, it returns an empty list (all tools auto-approved by default).
    """
    default_list: List[str] = [] # Default is an empty list, meaning all tools are auto-approved
    if not os.path.exists(TOOL_CONFIRMATION_CONFIG_PATH):
        return default_list
    try:
        with open(TOOL_CONFIRMATION_CONFIG_PATH, 'r', encoding='utf-8') as f:
            content = f.read()
            if not content.strip(): # Handle empty file
                return default_list
            data = json.loads(content)
        # Ensure the loaded data is a list of strings.
        loaded_list = data.get("requires_confirmation_tools", default_list)
        if not isinstance(loaded_list, list) or not all(isinstance(item, str) for item in loaded_list):
            print(f"Warning: 'requires_confirmation_tools' in '{TOOL_CONFIRMATION_CONFIG_PATH}' is not a list of strings. Using default (empty list).")
            return default_list
        return loaded_list
    except (json.JSONDecodeError, IOError) as e:
        print(f"Warning: Could not load or parse {TOOL_CONFIRMATION_CONFIG_PATH}. Using default (empty list). Error: {e}")
        return default_list

def _save_requires_confirmation_list(tools_list: List[str]) -> bool:
    """
    Saves the provided list of tools requiring confirmation to the JSON configuration file.
    Creates the directory if it doesn't exist.
    """
    try:
        # Ensure the 'data' directory exists.
        os.makedirs(os.path.dirname(TOOL_CONFIRMATION_CONFIG_PATH), exist_ok=True)
        with open(TOOL_CONFIRMATION_CONFIG_PATH, 'w', encoding='utf-8') as f:
            json.dump({"requires_confirmation_tools": tools_list}, f, indent=2)
        return True
    except IOError as e:
        print(f"Error: Could not write to {TOOL_CONFIRMATION_CONFIG_PATH}. Error: {e}")
        return False
    except Exception as e:
        print(f"An unexpected error occurred while saving to {TOOL_CONFIRMATION_CONFIG_PATH}. Error: {e}")
        return False


def manage_tool_confirmation_settings(action: str, tool_name: Optional[str] = None) -> str:
    """
    Manages the list of tools that require explicit user confirmation before execution.
    Other tools are considered auto-approved by default.

    This tool allows adding, removing, or listing tools in the 'requires confirmation' configuration.
    The configuration is stored in 'data/tool_confirmation_config.json'.

    Args:
        action (str): The action to perform. Valid actions are:
                      "add": Adds a specified tool_name to the 'requires confirmation' list.
                             (This makes the tool NOT auto-approved).
                      "remove": Removes a specified tool_name from the 'requires confirmation' list.
                                (This makes the tool auto-approved again).
                      "list": Lists all tools currently requiring user confirmation.
                      "add_all": Adds all known, non-system-internal tools to the 'requires confirmation' list.
                                 (Makes all tools require confirmation).
                      "remove_all": Clears the 'requires confirmation' list.
                                    (Makes all tools auto-approved).
        tool_name (Optional[str]): The name of the tool to add or remove.
                                   This is required for "add" and "remove" actions.

    Returns:
        str: A message confirming the action taken or an error message if the action failed.
    """
    current_confirmation_list = _load_requires_confirmation_list()
    action = action.lower().strip() # Normalize action string

    # Import ToolSystem here to access all registered tools for validation and "add_all"
    try:
        from ai_assistant.tools.tool_system import tool_system_instance
        all_known_tools_dict = tool_system_instance.list_tools() 
        all_known_tool_names = list(all_known_tools_dict.keys())
        # Filter out system_internal tools for "add_all"
        non_system_internal_tool_names = [
            name for name, data in tool_system_instance._tool_registry.items() # Access internal for type
            if data.get('type') != 'system_internal'
        ]

    except ImportError:
        print("Error: Could not import ToolSystem to validate tool names for manage_tool_confirmation_settings.")
        all_known_tool_names = None
        non_system_internal_tool_names = None


    if action == "list":
        if not current_confirmation_list:
            return "No tools currently require user confirmation (all tools are auto-approved by default)."
        return "Tools currently requiring user confirmation: " + ", ".join(sorted(current_confirmation_list))

    if action == "add_all":
        if non_system_internal_tool_names is not None:
            updated_list = sorted(list(set(non_system_internal_tool_names))) # Add all non-system tools, ensure uniqueness and sort
            if _save_requires_confirmation_list(updated_list):
                return f"All {len(updated_list)} known non-system-internal tools now require user confirmation."
            else:
                return "Error: Failed to set all tools to require confirmation due to a save error."
        else:
            return "Error: Cannot perform 'add_all' as the list of all system tools could not be retrieved."


    if action == "remove_all":
        if not current_confirmation_list: # Already empty
            return "No tools require user confirmation (all tools are already auto-approved by default)."
        if _save_requires_confirmation_list([]): # Save an empty list
            return "All tools removed from the 'requires confirmation' list. All tools are now auto-approved."
        else:
            return "Error: Failed to clear the 'requires confirmation' list due to a save error."

    # Actions "add" and "remove" require a tool_name
    if not tool_name:
        return f"Error: 'tool_name' is required for the action '{action}'."

    tool_name = tool_name.strip() # Clean the tool_name input

    # Validate tool_name against known tools if possible
    if all_known_tool_names and tool_name not in all_known_tool_names:
        return f"Error: Tool '{tool_name}' is not a recognized tool in the system. Cannot '{action}' it."

    if action == "add": # Add to "requires confirmation" list
        if tool_name not in current_confirmation_list:
            current_confirmation_list.append(tool_name)
            if _save_requires_confirmation_list(sorted(current_confirmation_list)):
                return f"Tool '{tool_name}' now requires user confirmation (added to 'requires confirmation' list)."
            else:
                current_confirmation_list.remove(tool_name) # Revert in-memory change
                return f"Error: Failed to save updated 'requires confirmation' list after attempting to add '{tool_name}'."
        else:
            return f"Tool '{tool_name}' already requires user confirmation. No action taken."
    elif action == "remove": # Remove from "requires confirmation" list
        if tool_name in current_confirmation_list:
            current_confirmation_list.remove(tool_name)
            if _save_requires_confirmation_list(sorted(current_confirmation_list)):
                return f"Tool '{tool_name}' no longer requires user confirmation (removed from 'requires confirmation' list; now auto-approved)."
            else:
                current_confirmation_list.append(tool_name) # Revert in-memory change
                return f"Error: Failed to save updated 'requires confirmation' list after attempting to remove '{tool_name}'."
        else:
            return f"Tool '{tool_name}' does not require user confirmation (it's already auto-approved). Cannot remove from list."
    else:
        return f"Error: Unknown action '{action}'. Valid actions are 'add', 'remove', 'list', 'add_all', 'remove_all'."

if __name__ == '__main__': # pragma: no cover
    print("--- Testing Tool Confirmation Settings (manage_tool_confirmation_settings) ---")

    original_config_content = None
    if os.path.exists(TOOL_CONFIRMATION_CONFIG_PATH):
        with open(TOOL_CONFIRMATION_CONFIG_PATH, 'r', encoding='utf-8') as f_orig:
            original_config_content = f_orig.read()
        os.remove(TOOL_CONFIRMATION_CONFIG_PATH)

    class MockToolSystemInstanceForConfigTest:
        def __init__(self):
            self._tool_registry = {
                "search_duckduckgo": {"description": "Searches the web.", "type": "custom_discovered"},
                "get_self_awareness_info_and_converse": {"description": "Provides info about the AI.", "type": "custom_discovered"},
                "another_example_tool": {"description": "Does something else.", "type": "custom_discovered"},
                "project_tool_A": {"description": "Manages project A.", "type": "custom_discovered"},
                "system_update_tool_metadata": {"description": "Internal system tool.", "type": "system_internal"}
            }
        def list_tools(self) -> Dict[str, str]:
            return {name: data["description"] for name, data in self._tool_registry.items()}

    from ai_assistant.tools import tool_system as ts_main_module
    original_main_ts_instance = ts_main_module.tool_system_instance
    ts_main_module.tool_system_instance = MockToolSystemInstanceForConfigTest()


    print("\nInitial state (should be default: all tools auto-approved):")
    print(f"List: {manage_tool_confirmation_settings(action='list')}") 

    print("\nAdding tools to 'requires confirmation':")
    print(f"Add 'get_self_awareness_info_and_converse': {manage_tool_confirmation_settings(action='add', tool_name='get_self_awareness_info_and_converse')}")
    print(f"List after add: {manage_tool_confirmation_settings(action='list')}")
    assert "get_self_awareness_info_and_converse" in _load_requires_confirmation_list()

    print(f"\nAdd 'non_existent_tool': {manage_tool_confirmation_settings(action='add', tool_name='non_existent_tool')}") 
    assert "non_existent_tool" not in _load_requires_confirmation_list()

    print(f"\nAdd 'get_self_awareness_info_and_converse' again: {manage_tool_confirmation_settings(action='add', tool_name='get_self_awareness_info_and_converse')}")
    print(f"List after adding existing: {manage_tool_confirmation_settings(action='list')}")

    print("\nRemoving tools from 'requires confirmation' (making them auto-approved):")
    print(f"Remove 'get_self_awareness_info_and_converse': {manage_tool_confirmation_settings(action='remove', tool_name='get_self_awareness_info_and_converse')}")
    print(f"List after remove: {manage_tool_confirmation_settings(action='list')}")
    assert "get_self_awareness_info_and_converse" not in _load_requires_confirmation_list()

    print(f"\nRemove 'get_self_awareness_info_and_converse' again: {manage_tool_confirmation_settings(action='remove', tool_name='get_self_awareness_info_and_converse')}")

    print("\nTesting 'add_all' (make all non-system tools require confirmation) and 'remove_all' (make all tools auto-approved):")
    print(f"Add 'search_duckduckgo' to require confirmation: {manage_tool_confirmation_settings(action='add', tool_name='search_duckduckgo')}")
    print(f"Current list: {manage_tool_confirmation_settings(action='list')}")
    
    print(f"\n'add_all' action (make all non-system tools require confirmation):")
    print(manage_tool_confirmation_settings(action='add_all'))
    list_after_add_all = _load_requires_confirmation_list()
    print(f"List after 'add_all': {manage_tool_confirmation_settings(action='list')}")
    assert len(list_after_add_all) == 4 # search_duckduckgo, get_self_awareness..., another_example..., project_tool_A
    assert "system_update_tool_metadata" not in list_after_add_all


    print(f"\n'remove_all' action (make all tools auto-approved):")
    print(manage_tool_confirmation_settings(action='remove_all'))
    print(f"List after 'remove_all': {manage_tool_confirmation_settings(action='list')}")
    assert not _load_requires_confirmation_list() 

    ts_main_module.tool_system_instance = original_main_ts_instance

    if original_config_content is not None:
        with open(TOOL_CONFIRMATION_CONFIG_PATH, 'w', encoding='utf-8') as f_restore:
            f_restore.write(original_config_content)
        print(f"\nRestored original content to {TOOL_CONFIRMATION_CONFIG_PATH}")
    elif os.path.exists(TOOL_CONFIRMATION_CONFIG_PATH):
        os.remove(TOOL_CONFIRMATION_CONFIG_PATH)
        print(f"\nRemoved test config file {TOOL_CONFIRMATION_CONFIG_PATH}")
    
    data_dir = os.path.dirname(TOOL_CONFIRMATION_CONFIG_PATH)
    if os.path.exists(data_dir) and not os.listdir(data_dir):
        os.rmdir(data_dir)
        print(f"Removed empty data directory: {data_dir}")


    print("\n--- Tool Confirmation Settings Tests Finished ---")

# ### END FILE: ai_assistant/custom_tools/config_management_tools.py ###

# ### START FILE: ai_assistant/custom_tools/conversational_tools.py ###
# Self-Evolving-Agent-feat-chat-history-context/ai_assistant/custom_tools/conversational_tools.py
# This file can be empty or contain other conversational tools.
# The get_self_awareness_info_and_converse tool has been consolidated into awareness_tools.py

# ### END FILE: ai_assistant/custom_tools/conversational_tools.py ###

# ### START FILE: ai_assistant/custom_tools/file_system_tools.py ###
# ai_assistant/custom_tools/file_system_tools.py
import os
import re
from typing import Union # For return type hints if using dicts for errors, though task specifies string returns

# Import get_data_dir from the main config to centralize data paths for some things,
# but ai_generated_projects will be directly under ai_assistant.
from ..config import get_data_dir # Keep for other potential data uses if any

# Module-level constant for the base directory where projects will be created.
# Changed: Now places ai_generated_projects directly under the 'ai_assistant' package directory.
ai_assistant_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
BASE_PROJECTS_DIR = os.path.join(ai_assistant_dir, "ai_generated_projects")

def sanitize_project_name(name: str) -> str:
    """
    Sanitizes a project name to create a safe directory name.

    - Converts to lowercase.
    - Replaces spaces and multiple hyphens with a single underscore.
    - Removes characters that are not alphanumeric, underscores, or hyphens.
    - Ensures it's not empty (defaults to "unnamed_project").
    - Limits length to a maximum of 50 characters.

    Args:
        name: The raw project name string.

    Returns:
        A sanitized string suitable for use as a directory name.
    """
    if not name or not name.strip():
        return "unnamed_project"

    s_name = name.lower()
    s_name = re.sub(r'\s+', '_', s_name)  # Replace spaces with underscores
    s_name = re.sub(r'-+', '_', s_name)   # Replace one or more hyphens with a single underscore
    s_name = re.sub(r'[^\w-]', '', s_name) # Remove non-alphanumeric characters (keeps underscores and hyphens)
    s_name = re.sub(r'_+', '_', s_name)   # Replace multiple underscores with a single one

    if not s_name: # If sanitization results in an empty string (e.g., "!!!")
        return "unnamed_project"
    
    return s_name[:50] # Limit length

def create_project_directory(project_name: str) -> str:
    """
    Creates a new project directory under the BASE_PROJECTS_DIR.

    Args:
        project_name: The desired name for the project. This will be sanitized.

    Returns:
        A string indicating success or failure, including the path or an error message.
    """
    if not project_name or not isinstance(project_name, str):
        return "Error: Project name must be a non-empty string."

    sanitized_name = sanitize_project_name(project_name)
    full_path = os.path.join(BASE_PROJECTS_DIR, sanitized_name)

    if os.path.exists(full_path):
        return f"Error: Project directory '{full_path}' already exists."
    
    try:
        os.makedirs(full_path, exist_ok=True) # exist_ok=True also creates BASE_PROJECTS_DIR if needed
        return f"Success: Project directory '{full_path}' created."
    except OSError as e:
        return f"Error creating project directory '{full_path}': {e}"
    except Exception as e:
        return f"An unexpected error occurred while creating project directory '{full_path}': {e}"

def write_text_to_file(full_filepath: str, content: str) -> str:
    """
    Writes the given text content to the specified file.
    Ensures the directory for the file exists before writing.

    Args:
        full_filepath: The absolute or relative path to the file.
        content: The string content to write to the file.

    Returns:
        A string indicating success or an error message.
    """
    if not full_filepath or not isinstance(full_filepath, str):
        return "Error: Filepath must be a non-empty string."
    if not isinstance(content, str):
        return "Error: Content must be a string."

    try:
        dir_path = os.path.dirname(full_filepath)
        if dir_path: # If there's a directory part
            os.makedirs(dir_path, exist_ok=True)
        
        with open(full_filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        return f"Success: Content written to '{full_filepath}'."
    except OSError as e:
        return f"Error writing to file '{full_filepath}': {e}"
    except Exception as e:
        return f"An unexpected error occurred while writing to file '{full_filepath}': {e}"

def read_text_from_file(full_filepath: str) -> str:
    """
    Reads and returns the text content from the specified file.

    Args:
        full_filepath: The absolute or relative path to the file.

    Returns:
        The content of the file as a string, or an error message string if reading fails.
    """
    if not full_filepath or not isinstance(full_filepath, str):
        return "Error: Filepath must be a non-empty string."

    if not os.path.exists(full_filepath):
        return f"Error: File '{full_filepath}' not found."
    
    if not os.path.isfile(full_filepath): # Check if it's actually a file
        return f"Error: Path '{full_filepath}' is not a file."

    try:
        with open(full_filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        return content
    except OSError as e:
        return f"Error reading file '{full_filepath}': {e}"
    except Exception as e:
        return f"An unexpected error occurred while reading file '{full_filepath}': {e}"

if __name__ == '__main__':
    import shutil # For cleaning up test directories

    print("--- Testing File System Tools ---")

    # --- Test sanitize_project_name ---
    print("\n--- Testing sanitize_project_name ---")
    test_names = {
        "My Awesome Hangman Game!": "my_awesome_hangman_game",
        "Project with spaces": "project_with_spaces",
        "project-with-hyphens": "project_with_hyphens",
        "Project_With_Underscores": "project_with_underscores",
        "Th!s h@s $pec!@l ch@r$": "thshs_pecl_chr",
        "  leading and trailing spaces  ": "leading_and_trailing_spaces",
        "---multiple---hyphens---": "multiple_hyphens",
        "__": "unnamed_project", # Becomes empty after initial sanitization
        "": "unnamed_project",
        "a"*60: "a"*50, # Length limit test
        "Valid-Name_123": "valid-name_123" # Test with allowed special chars
    }
    for original, expected in test_names.items():
        sanitized = sanitize_project_name(original)
        print(f"Original: '{original}' -> Sanitized: '{sanitized}' (Expected: '{expected}')")
        assert sanitized == expected, f"Sanitization failed for '{original}'"
    print("sanitize_project_name tests passed.")

    # --- Test create_project_directory ---
    print("\n--- Testing create_project_directory ---")
    project1_name = "My Test Project Alpha"
    sanitized_p1_name = sanitize_project_name(project1_name)
    expected_p1_path = os.path.join(BASE_PROJECTS_DIR, sanitized_p1_name)

    # Test creating a new directory
    result_create1 = create_project_directory(project1_name)
    print(result_create1)
    assert "Success" in result_create1 and expected_p1_path in result_create1
    assert os.path.exists(expected_p1_path) and os.path.isdir(expected_p1_path)
    print(f"Verified directory '{expected_p1_path}' exists.")

    # Test attempting to create an existing directory
    result_create_existing = create_project_directory(project1_name)
    print(result_create_existing)
    assert "Error" in result_create_existing and "already exists" in result_create_existing
    print("Attempt to create existing directory handled correctly.")
    
    # Test with empty project name
    result_empty_name = create_project_directory("")
    print(result_empty_name)
    assert "Error" in result_empty_name or "unnamed_project" in sanitize_project_name("") # depends on if error is before or after sanitize
    if "Success" in result_empty_name : # if it allows unnamed_project
        assert os.path.exists(os.path.join(BASE_PROJECTS_DIR, "unnamed_project"))
    print("Empty project name test handled.")

    print("create_project_directory tests passed.")

    # --- Test write_text_to_file and read_text_from_file ---
    print("\n--- Testing write_text_to_file and read_text_from_file ---")
    test_file_content = "Hello, this is a test file.\nIt has multiple lines.\nEnd of test."
    test_file_path = os.path.join(expected_p1_path, "test_file.txt") # Place inside created project
    
    # Test writing a new file
    result_write = write_text_to_file(test_file_path, test_file_content)
    print(result_write)
    assert "Success" in result_write and test_file_path in result_write
    assert os.path.exists(test_file_path)
    print(f"Verified file '{test_file_path}' was created.")

    # Test reading the file
    read_content = read_text_from_file(test_file_path)
    # print(f"Read content: '{read_content}'") # Can be noisy
    assert read_content == test_file_content
    print(f"Verified content of '{test_file_path}' matches.")

    # Test overwriting an existing file
    overwrite_content = "This is new content for overwriting."
    result_overwrite = write_text_to_file(test_file_path, overwrite_content)
    print(result_overwrite)
    assert "Success" in result_overwrite
    read_overwritten_content = read_text_from_file(test_file_path)
    assert read_overwritten_content == overwrite_content
    print(f"Verified file '{test_file_path}' was overwritten successfully.")

    # Test reading a non-existent file
    non_existent_file_path = os.path.join(expected_p1_path, "non_existent.txt")
    result_read_non_existent = read_text_from_file(non_existent_file_path)
    print(result_read_non_existent)
    assert "Error" in result_read_non_existent and "not found" in result_read_non_existent
    print("Attempt to read non-existent file handled correctly.")

    # Test writing to an invalid path (e.g. if perms were an issue, or path too long - hard to test robustly here)
    # For now, just test with empty filepath
    result_write_empty_path = write_text_to_file("", "content")
    print(result_write_empty_path)
    assert "Error" in result_write_empty_path
    print("Attempt to write to empty filepath handled.")

    # Test reading from an invalid path
    result_read_empty_path = read_text_from_file("")
    print(result_read_empty_path)
    assert "Error" in result_read_empty_path
    print("Attempt to read from empty filepath handled.")

    print("write_text_to_file and read_text_from_file tests passed.")

    # --- Cleanup ---
    print("\n--- Cleaning up test directories and files ---")
    if os.path.exists(BASE_PROJECTS_DIR):
        try:
            shutil.rmtree(BASE_PROJECTS_DIR)
            print(f"Removed base test directory: '{BASE_PROJECTS_DIR}'")
        except OSError as e:
            print(f"Error removing base test directory '{BASE_PROJECTS_DIR}': {e}")
    else:
        print(f"Base test directory '{BASE_PROJECTS_DIR}' was not created or already cleaned up.")
    
    print("\n--- All File System Tools Tests Finished ---")

# ### END FILE: ai_assistant/custom_tools/file_system_tools.py ###

# ### START FILE: ai_assistant/custom_tools/generated/__init__.py ###
# This file makes Python treat the 'generated' directory as a package.
# ### END FILE: ai_assistant/custom_tools/generated/__init__.py ###

# ### START FILE: ai_assistant/custom_tools/git_tools.py ###
"""
Tools for interacting with Git repositories, such as pushing AI-generated commits
after user approval.
"""
import logging
import os
import shutil
import subprocess
from typing import Optional

# Configure logger for this module
logger = logging.getLogger(__name__)
if not logger.handlers: # Avoid adding multiple handlers if script is reloaded/run multiple times
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

def push_ai_generated_commits(
    project_root: str, 
    branch_name: Optional[str] = None, 
    remote_name: str = "origin"
) -> str:
    """
    Pushes locally committed AI self-modifications to a remote repository 
    after user confirmation. 
    
    Note: The user confirmation mechanism itself is outside this specific tool's 
    direct implementation; this tool is called *after* the user has already 
    given their approval to push.

    Args:
        project_root (str): Absolute path to the local Git repository.
        branch_name (Optional[str]): The specific branch to push. 
                                     If None, pushes the current active branch.
        remote_name (str): The name of the remote to push to (default: "origin").

    Returns:
        A string indicating success (including branch and remote) or an error message.
    """
    # --- Placeholder Logic ---
    current_branch_name = branch_name if branch_name else "current (determined automatically)"
    log_message = (
        f"Placeholder: `push_ai_generated_commits` called for project '{project_root}', "
        f"branch '{current_branch_name}', remote '{remote_name}'. "
        "Actual push logic not implemented."
    )
    logger.info(log_message)
    
    return (
        f"Placeholder: Successfully 'pushed' branch '{current_branch_name}' "
        f"to remote '{remote_name}' in project '{project_root}'."
    )

# --- Detailed Design Comments for Future Implementation ---
#
# 1.  User Confirmation:
#     - This tool should ONLY be called AFTER explicit user confirmation has been 
#       received through the primary agent interface (e.g., chat). The agent must
#       present the planned push operation (what branch to what remote) and get
#       a "yes" or equivalent from the user.
#
# 2.  Prerequisites:
#     - Git Command-Line Tool: Verify `git` is available using `shutil.which("git")`.
#       If not found, return an error: "Git command not found. Please ensure Git is installed and in PATH."
#     - Valid Git Repository: Check if `project_root` is a Git repository by verifying
#       `os.path.isdir(os.path.join(project_root, ".git"))`.
#       If not, return an error: f"Project root '{project_root}' is not a valid Git repository."
#     - Remote Existence (Optional but Recommended): Check if `remote_name` exists using 
#       `git remote get-url <remote_name>`. If it fails, it might indicate the remote
#       is not configured. This check can be tricky as `get-url` might fail for other
#       reasons too. A simpler check is `git remote show <remote_name>`, though its output
#       parsing is more involved. For an initial version, one might skip this and let `git push` fail.
#
# 3.  Branch Handling:
#     - Determine Target Branch:
#       target_branch_to_push = branch_name
#       if target_branch_to_push is None:
#           try:
#               result = subprocess.run(
#                   [git_path, "rev-parse", "--abbrev-ref", "HEAD"],
#                   cwd=project_root, capture_output=True, text=True, check=True, timeout=5
#               )
#               target_branch_to_push = result.stdout.strip()
#               if not target_branch_to_push or target_branch_to_push == "HEAD":
#                   # HEAD means detached state, which is problematic for pushing.
#                   return "Error: Could not determine current branch or in detached HEAD state. Please specify a branch."
#           except Exception as e:
#               logger.error(f"Error determining current branch: {e}")
#               return f"Error determining current branch: {e}"
#     - Active Branch Check (Consideration):
#       Strict version: Only allow pushing the currently active branch.
#       Flexible version: If `branch_name` is provided and is not the current branch,
#                         either (a) fail with a message, or (b) (more complex, riskier)
#                         attempt `git checkout <branch_name>` first. Option (a) is safer.
#                         For now, assume we push `target_branch_to_push` regardless of current
#                         active branch, but `git push` might require it to be the current one
#                         or use a refspec like `HEAD:<target_branch_to_push>`.
#                         The command `git push <remote_name> <target_branch_to_push>` usually works
#                         for local branches that exist.
#
# 4.  Safety Checks (Before Push):
#     - Uncommitted Changes:
#       `status_result = subprocess.run([git_path, "status", "--porcelain"], ...)`
#       If `status_result.stdout` is not empty, there are uncommitted changes or untracked files.
#       Return an error: "Working directory is not clean. Please commit or stash changes before pushing."
#     - Local Branch Behind Remote:
#       `log_result = subprocess.run([git_path, "log", f"@{u}.."], ...)` 
#       (or `git rev-list --count @{u}..HEAD`)
#       If this shows any commits, the local branch has commits not on the remote tracking branch,
#       which is fine. The opposite (`git log ..@{u}`) checks if remote is ahead.
#       A more direct check for "behind" status:
#       `git fetch <remote_name>` (to update remote refs)
#       `status_short_result = subprocess.run([git_path, "status", "-sb"], ...)`
#       Parse output for `[behind X]` pattern. If found, warn:
#       "Local branch is behind remote. Please pull first to avoid potential conflicts."
#       Alternatively, allow push with `--force-with-lease` but this is very risky for AI.
#       Simplest initial approach: Warn if behind, or proceed with push and let Git handle conflicts.
#       A safer initial approach is to fail if behind:
#       `git fetch <remote_name> <target_branch_to_push>` (fetches specific branch)
#       `rev_list_behind = subprocess.run([git_path, "rev-list", "--count", f"HEAD..{remote_name}/{target_branch_to_push}"], ...)`
#       `rev_list_ahead = subprocess.run([git_path, "rev-list", "--count", f"{remote_name}/{target_branch_to_push}..HEAD"], ...)`
#       If `rev_list_behind` > 0, then remote is ahead (local is behind). Fail or warn.
#
# 5.  Git Push Command (using `subprocess.run`):
#     - Command: `[git_path, "push", remote_name, target_branch_to_push]`
#     - `push_result = subprocess.run(command, cwd=project_root, capture_output=True, text=True, timeout=60)`
#     - Check `push_result.returncode`.
#     - Log `push_result.stdout` and `push_result.stderr`.
#     - Potential errors to handle:
#       - Authentication failures (Git will prompt if not configured; subprocess will hang or fail depending on config).
#         This implies credentials must be pre-configured (SSH keys, credential helper).
#       - Network issues.
#       - Rejections (e.g., non-fast-forward if local branch was behind and not rebased/merged).
#         This is where the safety check for being behind is important.
#
# 6.  Output/Return:
#     - Success: `f"Successfully pushed branch '{target_branch_to_push}' to remote '{remote_name}' in project '{project_root}'. Git output:\n{push_result.stdout}\n{push_result.stderr}"`
#     - Failure: `f"Failed to push branch ... Error: {push_result.stderr or 'Unknown error'}"`
#
# 7.  Security:
#     - Git operations involving remotes can require credentials. The environment where the
#       agent runs needs to be configured appropriately (e.g., SSH keys added to the
#       ssh-agent, Git credential helper configured, or HTTPS token used).
#     - The agent should not handle raw credentials directly.
#
# --- Example for data/tools.json (Conceptual) ---
# {
#   "push_ai_generated_commits": {
#     "tool_name": "push_ai_generated_commits",
#     "module_path": "ai_assistant.custom_tools.git_tools",
#     "function_name": "push_ai_generated_commits",
#     "description": "Pushes locally committed AI self-modifications to a remote repository after user confirmation. Args: project_root (str), branch_name (Optional[str]), remote_name (str, default 'origin').",
#     "category": "git_operations", 
#     "enabled": false, // Should be false until fully implemented and tested
#     "type": "custom_built_in" // Or "custom_discovered" if it were dynamically found
#   }
# }
#
# Note: The "type" field in tools.json could be "custom_built_in" if it's a tool
# specifically created as part of the agent's known capabilities, versus 
# "custom_discovered" which might imply it was found via filesystem scanning.
# For this, "custom_built_in" seems more appropriate.
# The "enabled" flag should be `false` initially.

if __name__ == '__main__':
    # Example of how the placeholder might be called (for manual testing)
    print("--- Testing push_ai_generated_commits placeholder ---")
    
    # Create a dummy project root for testing (if it doesn't exist)
    dummy_project_path = "temp_dummy_git_project"
    if not os.path.exists(dummy_project_path):
        os.makedirs(dummy_project_path)
        # In a real test, you might initialize a git repo here:
        # subprocess.run(["git", "init"], cwd=dummy_project_path, check=True)

    abs_dummy_project_path = os.path.abspath(dummy_project_path)

    # Test 1: Push current branch (placeholder)
    result1 = push_ai_generated_commits(project_root=abs_dummy_project_path)
    print(f"Test 1 Result: {result1}")

    # Test 2: Push specific branch (placeholder)
    result2 = push_ai_generated_commits(project_root=abs_dummy_project_path, branch_name="feature-branch-ai")
    print(f"Test 2 Result: {result2}")

    # Test 3: Push to different remote (placeholder)
    result3 = push_ai_generated_commits(project_root=abs_dummy_project_path, remote_name="upstream")
    print(f"Test 3 Result: {result3}")
    
    # Clean up dummy project path if it was created by this test script
    # For simplicity, manual cleanup might be preferred for this placeholder's __main__
    # if os.path.exists(dummy_project_path) and "temp_dummy_git_project" in dummy_project_path:
    #     logger.info(f"Consider manually removing: {dummy_project_path}")
    
    print("--- Placeholder tests finished ---")

# ### END FILE: ai_assistant/custom_tools/git_tools.py ###

# ### START FILE: ai_assistant/custom_tools/knowledge_tools.py ###
# ai_assistant/custom_tools/knowledge_tools.py
import json
from typing import Optional, List
from ai_assistant.memory.persistent_memory import load_learned_facts, save_learned_facts
from ai_assistant.llm_interface.ollama_client import invoke_ollama_model_async # Changed to async
from ai_assistant.config import get_model_for_task, is_debug_mode
import re # For cleaning LLM response

FACT_CURATION_PROMPT_TEMPLATE = """
You are an AI Knowledge Base Curator. Your primary responsibility is to maintain a clean, accurate, and non-redundant set of learned facts.
You will be given the CURRENT set of learned facts and a list of NEWLY OBSERVED potential facts.

Your task is to produce an UPDATED AND FINALIZED list of facts by performing the following operations:
1.  **Integrate New Information:** If a newly observed item provides genuinely new and persistent factual information, add it.
2.  **Consolidate & Refine:** If a new item is related to an existing fact but adds more detail, clarifies, or slightly corrects it, merge them into a single, more comprehensive fact. Prefer the more accurate or detailed version.
3.  **Eliminate Duplicates:** If a new item is identical or semantically equivalent to an existing fact, do not add the new item. Ensure the existing fact is retained if it's well-phrased.
4.  **Correct/Update:** If a new item clearly contradicts and updates an older fact (e.g., "User's favorite color was blue" and new is "User's favorite color is now green"), replace the older fact.
5.  **Discard Non-Factual/Transient Info:** If a newly observed item is not a persistent fact (e.g., it's a command, a question, a temporary state, an opinion, or a very specific detail unlikely to be broadly useful later), discard it.
    *   Examples of info to DISCARD: "run the search tool", "what is the weather?", "I feel tired today", "the file is missing right now", "tell me about X", "create a tool for Y".
    *   Examples of info to KEEP (if relevant and persistent): "The user's name is Alex.", "The project name is 'hangman'", "The project description is 'Create a hangman game...'", "The user prefers Python.", "The capital of France is Paris."
6.  **Maintain Conciseness & Clarity:** Ensure facts are stated clearly and concisely. Avoid overly long or ambiguous statements. Rephrase if necessary for clarity.
    *   Example of Refinement: Change "User mentioned their name is Alex Smith." to "The user's name is Alex Smith."

CURRENT LEARNED FACTS (JSON list of strings):
{current_facts_json}

NEWLY OBSERVED POTENTIAL FACTS (JSON list of strings):
{new_potential_facts_json}

Based on these inputs, output the UPDATED AND FINALIZED list of facts.
The output *MUST* be a JSON object containing a single key "updated_facts", which is a list of strings.

Example:
CURRENT LEARNED FACTS:
["The user's name is Jason.", "The capital of France is Paris."]
NEWLY OBSERVED POTENTIAL FACTS:
["User mentioned their name is Jason.", "Paris is in France."]

Expected JSON Response:
{{
  "updated_facts": [
    "The user's name is Jason",
    "The capital of France is Paris."
  ]
}}

If NEWLY OBSERVED POTENTIAL FACTS is empty, or if after processing no changes are warranted to CURRENT LEARNED FACTS, then the "updated_facts" list should be identical to the CURRENT LEARNED FACTS.
If CURRENT LEARNED FACTS is empty, then "updated_facts" will be the processed version of NEWLY OBSERVED POTENTIAL FACTS.

Respond ONLY with the JSON object.
"""

async def _curate_and_update_fact_store(newly_observed_facts: List[str]) -> bool:
    """
    Curates the fact store using an LLM by integrating newly observed facts
    with existing ones, then saves the updated fact store.

    Args:
        newly_observed_facts: A list of new potential facts to integrate.

    Returns:
        True if the curation and saving process was successful, False otherwise.
    """
    if not isinstance(newly_observed_facts, list) or not all(isinstance(f, str) for f in newly_observed_facts):
        print("Error (_curate_and_update_fact_store): newly_observed_facts must be a list of strings.")
        return False

    current_facts = load_learned_facts() # Load from persistent_memory.py

    try:
        current_facts_json = json.dumps(current_facts)
        new_potential_facts_json = json.dumps(newly_observed_facts)
    except TypeError:
        print("Error (_curate_and_update_fact_store): Could not serialize facts to JSON for LLM prompt.")
        return False

    prompt = FACT_CURATION_PROMPT_TEMPLATE.format(
        current_facts_json=current_facts_json,
        new_potential_facts_json=new_potential_facts_json
    )

    if is_debug_mode():
        print(f"[DEBUG KNOWLEDGE_TOOLS] Fact Curation Prompt (first 300 chars):\n{prompt[:300]}...")

    model_name = get_model_for_task("fact_management") # A new task type for config, or use "fact_extraction" / "reflection"
    if not model_name: # Fallback
        model_name = get_model_for_task("reflection")

    llm_response_str = await invoke_ollama_model_async(prompt, model_name=model_name)

    if not llm_response_str:
        print("Error (_curate_and_update_fact_store): LLM returned no response for fact curation.")
        return False

    if is_debug_mode():
        print(f"[DEBUG KNOWLEDGE_TOOLS] Raw LLM response for fact curation:\n'{llm_response_str}'")
    
    # Clean LLM output (remove markdown fences)
    cleaned_response = llm_response_str.strip()
    if cleaned_response.startswith("```json"):
        cleaned_response = cleaned_response.lstrip("```json").rstrip("```").strip()
    elif cleaned_response.startswith("```"):
        cleaned_response = cleaned_response.lstrip("```").rstrip("```").strip()

    try:
        parsed_response = json.loads(cleaned_response)
        if not isinstance(parsed_response, dict) or "updated_facts" not in parsed_response:
            print(f"Error (_curate_and_update_fact_store): LLM response for fact curation is not a dict with 'updated_facts'. Response: {cleaned_response}")
            return False
        
        updated_facts_list = parsed_response["updated_facts"]
        if not isinstance(updated_facts_list, list) or not all(isinstance(fact, str) for fact in updated_facts_list):
            print(f"Error (_curate_and_update_fact_store): 'updated_facts' from LLM is not a list of strings. Response: {parsed_response}")
            return False

        if save_learned_facts(updated_facts_list): # Save to persistent_memory.py
            print(f"Info (_curate_and_update_fact_store): Fact store updated and saved. Total facts: {len(updated_facts_list)}.")
            return True
        else:
            print("Error (_curate_and_update_fact_store): Failed to save curated facts.")
            return False

    except json.JSONDecodeError:
        print(f"Error (_curate_and_update_fact_store): Failed to parse LLM JSON response for fact curation. Response: {cleaned_response}")
        return False
    except Exception as e:
        print(f"Error (_curate_and_update_fact_store): Unexpected error during fact curation: {e}")
        return False

async def learn_fact(fact_text: str) -> str:
    """
    Learns a new fact by adding it to the knowledge base via LLM curation.
    This function is now asynchronous.

    Args:
        fact_text: The fact to be learned.

    Returns:
        A string confirming that the fact has been processed for learning,
        or an error message if the process failed.
    """
    if not isinstance(fact_text, str) or not fact_text.strip():
        return "Sorry, I can only learn non-empty facts provided as text."

    # For a single fact, wrap it in a list for the curation function
    success = await _curate_and_update_fact_store([fact_text])
    
    if success:
        # We can't be sure if *this specific fact* was added, modified, or discarded by the LLM.
        # The confirmation message indicates the process completed, including the save attempt.
        return f"Okay, I've processed the information: '{fact_text}'. My knowledge base has been updated and the changes were saved."
    else:
        # This now more clearly indicates a failure in the curation/save pipeline.
        return (f"Sorry, I encountered an error while trying to process and save the information: '{fact_text}'. "
                "The fact may not have been permanently learned.")

def recall_facts(query: Optional[str] = None) -> List[str]:
    """
    Retrieves a list of learned facts from the curated fact store.
    Can be filtered by an optional query string.

    Args:
        query: An optional keyword or phrase to filter facts.
               If omitted or empty, all facts are returned.
               The filter is case-insensitive.

    Returns:
        A list of strings, where each string is a learned fact matching the query.
        Returns all facts if no query is provided.
        Returns an empty list if no facts are stored or if no facts match the query.
    """
    all_facts = load_learned_facts() # From persistent_memory.py

    if not all_facts:
        return []

    if query and query.strip():
        query_lower = query.lower()
        return [fact for fact in all_facts if query_lower in fact.lower()]
    
    return all_facts

async def run_periodic_fact_store_curation_async() -> bool:
    """
    Performs a periodic curation of the entire fact store using the LLM.
    This is intended to be called by a background service.
    It calls the existing _curate_and_update_fact_store with no new facts,
    prompting the LLM to review and refine the current set of facts.

    Returns:
        True if the curation process was successful, False otherwise.
    """
    print("Info (run_periodic_fact_store_curation_async): Starting periodic fact store curation.")
    if is_debug_mode():
        current_facts_count = len(load_learned_facts())
        print(f"[DEBUG KNOWLEDGE_TOOLS] Periodic curation: {current_facts_count} facts before curation.")

    # Call curation with an empty list of new facts.
    # The LLM prompt is designed to consolidate/refine existing facts in this scenario.
    success = await _curate_and_update_fact_store([]) 

    if success:
        print("Info (run_periodic_fact_store_curation_async): Periodic fact store curation completed successfully.")
    else:
        print("Error (run_periodic_fact_store_curation_async): Periodic fact store curation failed.")
    return success

if __name__ == '__main__': # pragma: no cover
    import asyncio
    import os
    
    # --- Test Setup ---
    # This setup is for direct testing of this module.
    # In a real application, dependencies like Ollama and data files would be managed differently.
    if not os.path.exists("data"):
        os.makedirs("data", exist_ok=True)
    
    # Use a specific test facts file to avoid interfering with the main one
    # This assumes persistent_memory.py functions can accept a filepath override,
    # or we mock them to use this path. For simplicity, we'll assume
    # load_learned_facts and save_learned_facts in this module will use the default.
    # To test properly, we'd ideally mock persistent_memory's file path.
    # For this __main__, we'll just let it use the default `data/learned_facts.json`
    # and clean it up.

    # Backup original facts file if it exists
    DEFAULT_FACTS_FILE = "data/learned_facts.json"
    original_facts_content = None
    if os.path.exists(DEFAULT_FACTS_FILE):
        with open(DEFAULT_FACTS_FILE, 'r', encoding='utf-8') as f_orig:
            original_facts_content = f_orig.read()
    
    # Start with an empty facts file for tests
    save_learned_facts([]) 

    # Mock invoke_ollama_model_async for testing _curate_and_update_fact_store
    async def mock_curation_llm(prompt: str, model_name: str, **kwargs):
        print(f"\n--- MOCK FACT CURATION LLM CALLED (Model: {model_name}) ---")
        # print(f"Prompt received by mock LLM:\n{prompt}\n--------------------------")
        
        # Simple mock logic based on prompt content
        if '"The user\'s favorite color is blue."' in prompt and '"The user mentioned their favorite color is azure."' in prompt:
            return json.dumps({"updated_facts": ["The user's favorite color is azure (refined from blue).", "The capital of France is Paris."]})
        elif '"The capital of France is Paris."' in prompt and '"Paris is in France."' in prompt: # Test duplicate/redundancy
            return json.dumps({"updated_facts": ["The capital of France is Paris."]})
        elif '"The user asked about the weather today."' in prompt: # Test discarding transient
             return json.dumps({"updated_facts": ["The capital of France is Paris."]}) # Assume Paris was existing
        elif '"A new unique fact."' in prompt and not '"The capital of France is Paris."' in prompt: # Test adding new to empty
            return json.dumps({"updated_facts": ["A new unique fact."]})
        elif '"newly_observed_facts": "[]"' in prompt and '"current_facts_json": "[\"fact A\", \"fact A\", \"fact B\"]"' in prompt: # Test periodic de-duplication
            print("Mock LLM: Detected periodic de-duplication test.")
            return json.dumps({"updated_facts": ["fact A", "fact B"]})
        elif '"newly_observed_facts": "[]"' in prompt and '"current_facts_json": "[\"This is a very very very long and verbose fact that could be shorter.\", \"fact C\"]"' in prompt: # Test periodic refinement
            print("Mock LLM: Detected periodic refinement test.")
            return json.dumps({"updated_facts": ["Shorter fact.", "fact C"]})
        elif '"A new unique fact."' in prompt and '"The capital of France is Paris."' in prompt: # Test adding new to existing
            return json.dumps({"updated_facts": ["The capital of France is Paris.", "A new unique fact."]})
        
        # Default: return current facts if new facts are empty or no change
        current_facts_match = re.search(r"CURRENT LEARNED FACTS \(JSON list of strings\):\s*(\[.*?\])", prompt, re.DOTALL)
        if current_facts_match:
            try:
                current_facts_list = json.loads(current_facts_match.group(1))
                return json.dumps({"updated_facts": current_facts_list})
            except json.JSONDecodeError:
                pass # Fall through if parsing fails
        
        return json.dumps({"updated_facts": []}) # Fallback empty

    # Patch the LLM call within this module's scope for testing
    original_invoke_async = invoke_ollama_model_async
    # Need to assign to the name used within this module (knowledge_tools.py)
    globals()['invoke_ollama_model_async'] = mock_curation_llm


    async def run_knowledge_tool_tests():
        print("\n--- Testing learn_fact (with LLM Curation) ---")
        
        # Test 1: Learn a completely new fact (to an empty store)
        print("\nTest 1: Learn a new fact (empty store)")
        fact1 = "A new unique fact."
        result1 = await learn_fact(fact1)
        print(f"Learn fact 1 result: {result1}")
        assert "processed the information" in result1
        recalled1 = recall_facts()
        print(f"Recalled after fact 1: {recalled1}")
        assert fact1 in recalled1
        assert len(recalled1) == 1

        # Test 2: Learn another new fact (add to existing)
        print("\nTest 2: Learn another new fact")
        # First, ensure "The capital of France is Paris." is in the store for the mock to work as expected
        await _curate_and_update_fact_store(["The capital of France is Paris."]) # Prime the store
        
        fact2 = "A new unique fact." # This should be added alongside Paris fact by the mock
        result2 = await learn_fact(fact2) # This will trigger curation with ["Paris..."] and ["A new unique fact."]
        print(f"Learn fact 2 result: {result2}")
        assert "processed the information" in result2
        recalled2 = recall_facts()
        print(f"Recalled after fact 2: {recalled2}")
        assert "The capital of France is Paris." in recalled2
        assert fact2 in recalled2
        assert len(recalled2) == 2


        # Test 3: "Learn" a fact that refines an existing one
        print("\nTest 3: Learn a refining fact")
        # Current facts: ["The capital of France is Paris.", "A new unique fact."]
        # Let's try to refine "A new unique fact." to "A new unique fact has been refined."
        # The mock needs to handle this. Let's assume the mock for "The user's favorite color is blue."
        # and "The user mentioned their favorite color is azure." is a good proxy.
        # We'll set current facts to ["The user's favorite color is blue.", "The capital of France is Paris."]
        save_learned_facts(["The user's favorite color is blue.", "The capital of France is Paris."])
        
        fact3_refining = "The user mentioned their favorite color is azure."
        result3 = await learn_fact(fact3_refining)
        print(f"Learn fact 3 (refining) result: {result3}")
        assert "processed the information" in result3
        recalled3 = recall_facts()
        print(f"Recalled after fact 3: {recalled3}")
        assert "The user's favorite color is azure (refined from blue)." in recalled3
        assert "The capital of France is Paris." in recalled3
        assert len(recalled3) == 2


        # Test 4: "Learn" a duplicate fact
        print("\nTest 4: Learn a duplicate fact")
        # Current facts: ["The user's favorite color is azure (refined from blue).", "The capital of France is Paris."]
        fact4_duplicate = "The capital of France is Paris." # Already exists
        result4 = await learn_fact(fact4_duplicate) # Mock should handle this
        print(f"Learn fact 4 (duplicate) result: {result4}")
        assert "processed the information" in result4 # Still processed
        recalled4 = recall_facts()
        print(f"Recalled after fact 4: {recalled4}")
        assert len(recalled4) == 2 # Length should not change


        # Test 5: "Learn" a transient/non-factual piece of info
        print("\nTest 5: Learn transient info")
        # Current facts: ["The user's favorite color is azure (refined from blue).", "The capital of France is Paris."]
        fact5_transient = "The user asked about the weather today." # Mock should discard this
        result5 = await learn_fact(fact5_transient)
        print(f"Learn fact 5 (transient) result: {result5}")
        assert "processed the information" in result5
        recalled5 = recall_facts()
        print(f"Recalled after fact 5: {recalled5}")
        assert len(recalled5) == 2 # Length should not change, transient info discarded

        # Test 6: Empty fact
        print("\nTest 6: Learn empty fact")
        result6 = await learn_fact("   ")
        print(f"Learn fact 6 (empty) result: {result6}")
        assert "Sorry, I can only learn non-empty facts" in result6
        recalled6 = recall_facts()
        assert len(recalled6) == 2 # Length should not change

        print("\n--- Recall tests (already implicitly tested, but explicit checks) ---")
        # Current facts: ["The user's favorite color is azure (refined from blue).", "The capital of France is Paris."]
        all_facts = recall_facts()
        assert len(all_facts) == 2
        
        queried_facts = recall_facts("Paris")
        assert len(queried_facts) == 1
        assert "The capital of France is Paris." in queried_facts

        queried_facts_none = recall_facts("Berlin")
        assert len(queried_facts_none) == 0

        print("\n--- Testing run_periodic_fact_store_curation_async ---")
        # Test 1: Periodic curation with duplicates
        print("\nPeriodic Curation Test 1: De-duplication")
        save_learned_facts(["fact A", "fact A", "fact B"])
        periodic_result1 = await run_periodic_fact_store_curation_async()
        assert periodic_result1 is True
        recalled_periodic1 = recall_facts()
        print(f"Recalled after periodic de-duplication: {recalled_periodic1}")
        assert "fact A" in recalled_periodic1
        assert "fact B" in recalled_periodic1
        assert len(recalled_periodic1) == 2

        # Test 2: Periodic curation for refinement
        print("\nPeriodic Curation Test 2: Refinement")
        save_learned_facts(["This is a very very very long and verbose fact that could be shorter.", "fact C"])
        periodic_result2 = await run_periodic_fact_store_curation_async()
        assert periodic_result2 is True
        recalled_periodic2 = recall_facts()
        print(f"Recalled after periodic refinement: {recalled_periodic2}")
        assert "Shorter fact." in recalled_periodic2
        assert "fact C" in recalled_periodic2
        assert len(recalled_periodic2) == 2


        # Restore original LLM invoker
        globals()['invoke_ollama_model_async'] = original_invoke_async

        # Restore original facts file content or delete if it wasn't there
        print("\n--- Test Cleanup ---")
        if original_facts_content is not None:
            with open(DEFAULT_FACTS_FILE, 'w', encoding='utf-8') as f_restore:
                f_restore.write(original_facts_content)
            print(f"Restored original content to {DEFAULT_FACTS_FILE}")
        elif os.path.exists(DEFAULT_FACTS_FILE): 
            os.remove(DEFAULT_FACTS_FILE)
            print(f"Removed test file {DEFAULT_FACTS_FILE}")
        
        data_dir = os.path.dirname(DEFAULT_FACTS_FILE)
        if os.path.exists(data_dir) and not os.listdir(data_dir):
            try: os.rmdir(data_dir)
            except OSError: pass # Ignore if not empty or other issues

        print("\n--- Knowledge Tools (LLM Curation) Tests Finished ---")

    asyncio.run(run_knowledge_tool_tests())

# ### END FILE: ai_assistant/custom_tools/knowledge_tools.py ###

# ### START FILE: ai_assistant/custom_tools/meta_programming_tools.py ###
import os
import re
import logging
import time
from typing import TYPE_CHECKING, Optional

from ai_assistant.config import get_model_for_task # To get the right LLM model

if TYPE_CHECKING:
    from ai_assistant.core.action_executor import ActionExecutor

logger = logging.getLogger(__name__)

# Define the base path for custom tools and the subdirectory for generated tools
GENERATED_TOOLS_DIR_NAME = "generated"
GENERATED_TOOLS_MODULE_PATH_PREFIX = f"ai_assistant.custom_tools.{GENERATED_TOOLS_DIR_NAME}"


def get_generated_tools_path() -> str:
    """Returns the absolute path to the directory where generated tools are stored."""
    # __file__ is ai_assistant/custom_tools/meta_programming_tools.py
    # os.path.dirname(__file__) is ai_assistant/custom_tools/
    custom_tools_dir = os.path.dirname(__file__)
    path = os.path.join(custom_tools_dir, GENERATED_TOOLS_DIR_NAME)
    os.makedirs(path, exist_ok=True)
    return path

async def generate_new_tool_from_description(
    action_executor: "ActionExecutor",
    tool_description: str,
    suggested_tool_function_name: Optional[str] = None,
    suggested_filename: Optional[str] = None
) -> str:
    """
    Generates Python code for a new tool based on a description and saves it
    to 'ai_assistant/custom_tools/generated/'. The LLM infers the tool's
    name, arguments, and implementation. A restart or tool refresh mechanism
    is typically required to activate the new tool.

    Args:
        action_executor: Provides access to the LLM interface.
        tool_description: Natural language description of the tool to create.
        suggested_tool_function_name: Optional specific function name for the new tool.
        suggested_filename: Optional specific filename (e.g., my_tool.py).

    Returns:
        A string indicating the result of the operation.
    """
    if not action_executor or not hasattr(action_executor, 'llm_interface'):
        return "Error: ActionExecutor with LLM interface is required for tool generation."

    function_name_guidance = f"The primary tool function should be named: `{suggested_tool_function_name}`." if suggested_tool_function_name else ""
    
    if suggested_filename:
        processed_sugg_filename = os.path.basename(suggested_filename)
        if not processed_sugg_filename.endswith(".py"):
            processed_sugg_filename += ".py"
        filename_guidance = f"Save the tool in a file named: `{processed_sugg_filename}`."
    else:
        filename_guidance = "Suggest a suitable, PEP8-compliant Python filename for this tool (e.g., `utility_helpers.py` or `data_processor_tool.py`)."

    prompt = f"""
You are an expert Python programmer assisting an AI agent by creating new tools.
Your task is to generate the complete Python code for a new tool based on the following description.

Tool Description:
"{tool_description}"

{function_name_guidance}
{filename_guidance}

Your output MUST strictly follow this format:
1.  The Python code block for the tool, enclosed in triple backticks (```python ... ```).
2.  On a new line, after the code block, the suggested filename using the prefix "Suggested Filename: ".

The Python code should:
- Be a single, self-contained Python script/module.
- Include a clear function definition for the tool.
- Use type hints for all arguments and return types.
- Have a comprehensive docstring for the main tool function, explaining what it does, its arguments (name, type, description), and what it returns (type, description). This docstring will be used by the AI assistant.
- Include necessary import statements at the top of the script.
- Implement the core logic to fulfill the described functionality.
- Handle potential errors gracefully (e.g., using try-except blocks).
- If the tool needs `action_executor` (e.g., to call other tools), it should accept `action_executor: ActionExecutor` as its first argument.

Example Tool Structure:
```python
import os
from typing import TYPE_CHECKING, List

if TYPE_CHECKING:
    from ai_assistant.core.action_executor import ActionExecutor

async def example_tool_function(action_executor: "ActionExecutor", items: List[str]) -> str:
    \"\"\"
    This is an example docstring. It processes items.
    Args:
        action_executor: The action executor.
        items (List[str]): A list of strings to process.
    Returns:
        str: A summary of the processing.
    \"\"\"
    try:
        # Tool logic here
        return f"Processed {{len(items)}} items."
    except Exception as e:
        # import logging; logger = logging.getLogger(__name__); logger.error(f"Error: {{e}}")
        return f"Error: {{e}}"
```
Now, generate the Python code and the suggested filename for the described tool.
"""
    try:
        model_name = get_model_for_task("tool_creation")
        llm_response = await action_executor.llm_interface.send_request(
            prompt=prompt, model_name=model_name, temperature=0.2
        )

        if not isinstance(llm_response, str) or not llm_response.strip():
            return f"Error: LLM returned an empty or invalid response. Response: {llm_response}"

        code_match = re.search(r"```python\n(.*?)\n```", llm_response, re.DOTALL)
        filename_match = re.search(r"Suggested Filename:\s*([\w_.-]+\.py)", llm_response)

        if not code_match:
            return f"Error: LLM did not provide a Python code block. Response:\n{llm_response}"
        generated_code = code_match.group(1).strip()

        final_filename = ""
        if suggested_filename: # User-provided filename takes precedence
            final_filename = os.path.basename(suggested_filename)
            if not final_filename.endswith(".py"): final_filename += ".py"
        elif filename_match:
            final_filename = filename_match.group(1).strip()
        
        if not final_filename: # Fallback if no filename determined yet
            func_name_match = re.search(r"def\s+([\w_]+)\s*\(", generated_code)
            base_name = func_name_match.group(1) if func_name_match else f"generated_tool_{int(time.time())}"
            final_filename = f"{base_name}.py"
            logger.warning(f"No filename suggested by LLM or user. Using fallback: {final_filename}")

        final_filename = re.sub(r'[^\w_.-]', '', final_filename) # Sanitize
        if not final_filename or not final_filename.endswith(".py"):
            final_filename = f"tool_{int(time.time())}.py" # Ultimate fallback

        generated_tools_dir = get_generated_tools_path()
        file_path = os.path.join(generated_tools_dir, final_filename)

        # Ensure __init__.py exists in generated_tools_dir
        init_py_path = os.path.join(generated_tools_dir, "__init__.py")
        if not os.path.exists(init_py_path):
            with open(init_py_path, "w", encoding="utf-8") as f_init:
                f_init.write("# This file makes Python treat the 'generated' directory as a package.\n")
            logger.info(f"Created __init__.py in {generated_tools_dir}")

        with open(file_path, "w", encoding="utf-8") as f_tool:
            f_tool.write(generated_code)

        relative_file_path = os.path.join("custom_tools", GENERATED_TOOLS_DIR_NAME, final_filename).replace("\\", "/")
        return (
            f"Successfully generated tool code and saved to 'ai_assistant/{relative_file_path}'.\n"
            "IMPORTANT: A restart of the AI Assistant or a '/refresh_tools' command (if implemented) "
            "is usually required to make the new tool available for use."
        )
    except Exception as e:
        logger.error(f"Error in generate_new_tool_from_description: {e}", exc_info=True)
        return f"An unexpected error occurred during tool generation: {e}"
# ### END FILE: ai_assistant/custom_tools/meta_programming_tools.py ###

# ### START FILE: ai_assistant/custom_tools/my_extra_tools.py ###
# ai_assistant/custom_tools/my_extra_tools.py
from duckduckgo_search import DDGS
import json
from typing import Optional, Union, List, Dict, Any # Added List, Dict, Any
from ai_assistant.llm_interface.ollama_client import invoke_ollama_model
from ai_assistant.config import get_model_for_task, GOOGLE_API_KEY as CFG_GOOGLE_API_KEY, GOOGLE_CSE_ID as CFG_GOOGLE_CSE_ID
import os # For environment variables

def subtract_numbers(a: float, b: float) -> Union[float, str]:
    """Subtracts the second number from the first."""
    # The tool system passes arguments as strings if coming directly from CLI's /execute_tool
    # For dynamic registration, the function signature is key.
    # If the planner uses this, it should provide correct types.
    # If CLI's /execute_tool is used, it passes strings.
    try:
        return float(a) - float(b)
    except ValueError:
        return "Error: Invalid input. 'a' and 'b' must be numbers for subtract_numbers."

def echo_message(message: str, repeat: int = 1) -> str:
    """Repeats a message a specified number of times."""
    try:
        num_repeats = int(repeat)
        if num_repeats < 0:
            return "Error: repeat count cannot be negative."
    except ValueError:
        return "Error: repeat count must be an integer."
    return ' '.join([str(message)] * num_repeats)

def search_duckduckgo(*args, **kwargs) -> str:
    """
    Searches the internet using DuckDuckGo and returns the results as a JSON string.

    Args:
        *args: Positional arguments. The first one is taken as the query.
        **kwargs: Keyword arguments. 'query' is checked here.

    Returns:
        str: A JSON string representing a list of search results. Each result in the
             list is a dictionary containing 'title', 'href' (URL), and 'body' (snippet)
             keys. Returns an empty JSON list '[]' if an error occurs during the search,
             if no results are found, or if the results are not in the expected format.
             Typically returns up to the top 5 results.
    """
    query: Optional[str] = None
    if 'query' in kwargs:
        query = str(kwargs['query'])
    elif args:
        query = str(args[0]) # Take the first positional argument as the query
    
    if not query:
        print("Error: search_duckduckgo requires a query argument.")
        return "[]" # Return empty JSON list string as per original behavior on error
    
    results = []
    try:
        with DDGS() as ddgs:
            search_results = ddgs.text(query, max_results=5)
        
        if search_results:
            for r in search_results:
                if isinstance(r, dict) and 'title' in r and 'href' in r and 'body' in r:
                    results.append({
                        "title": r['title'],
                        "href": r['href'],
                        "body": r['body']
                    })
                else:
                    print(f"Warning: search_duckduckgo received an unexpected result format: {r}")
            
    except Exception as e:
        print(f"Error during DuckDuckGo search for query '{query}': {e}")
    
    try:
        return json.dumps(results)
    except TypeError as te:
        print(f"Error serializing search results to JSON for query '{query}': {te}. Results: {results}")
        return "[]"

def search_google_custom_search(query: str, num_results: Union[int, str] = 5) -> str:
    """
    Searches Google. Args: query (str). Optional in kwargs: num_results (str, 1-10, default '5').

    Args:
        query (str): The search term.
        num_results (Union[int, str]): Number of results to return (max 10 for Google CSE).
                                       Planner will pass this as a string in kwargs.

    Returns:
        str: A JSON string representing a list of search results, each with
             'title', 'href' (URL), and 'body' (snippet). Returns '[]' on error.
    """
    api_key = os.environ.get("GOOGLE_API_KEY") or CFG_GOOGLE_API_KEY
    cse_id = os.environ.get("GOOGLE_CSE_ID") or CFG_GOOGLE_CSE_ID

    if not api_key:
        print("Error: GOOGLE_API_KEY environment variable not set.")
        return "[]"
    if not cse_id:
        print("Error: GOOGLE_CSE_ID environment variable not set.")
        return "[]"

    results_to_return = []
    try:
        from googleapiclient.discovery import build
        service = build("customsearch", "v1", developerKey=api_key)
        
        parsed_num_results = 5 # Default
        if isinstance(num_results, str):
            try:
                parsed_num_results = int(num_results)
            except ValueError:
                print(f"Warning: Google Search num_results '{num_results}' is not a valid integer string. Using default 5.")
        elif isinstance(num_results, int):
            parsed_num_results = num_results
        else:
            print(f"Warning: Google Search num_results unexpected type '{type(num_results)}'. Using default 5.")
        # Google Custom Search API allows max 10 results per query.
        # Ensure num_results is within valid range [1, 10].
        actual_num_results = max(1, min(parsed_num_results, 10))

        res = service.cse().list(q=query, cx=cse_id, num=actual_num_results).execute()
        
        if 'items' in res:
            for item in res['items']:
                results_to_return.append({
                    "title": item.get("title"),
                    "href": item.get("link"),
                    "body": item.get("snippet")
                })
    except ImportError:
        print("Error: google-api-python-client library not found. Please install it.")
        return "[]"
    except Exception as e:
        print(f"Error during Google Custom Search for query '{query}': {e}")
        return "[]"

    return json.dumps(results_to_return)

def process_search_results(search_query: str, search_results_json: str, processing_instruction: str = "answer_query") -> str:
    """
    Processes JSON search results based on a specified instruction to generate a response.

    Args:
        search_query (str): The original query that was searched.
        search_results_json (str): A JSON string containing search results (e.g., from search_duckduckgo).
                                   Expected format is a list of dictionaries, each with 'title', 'href', 'body'.
        processing_instruction (str, optional): Specifies how to process the results. Defaults to "answer_query".
            Possible values:
            - "answer_query": Formulate a direct natural language answer to the search_query.
            - "summarize_results": Provide a concise summary of the key information.
            - "extract_entities": List key entities (people, organizations, locations, etc.).
            - "custom_instruction:<your specific request>": Follow a custom instruction.

    Returns:
        str: A string containing the processed output (answer, summary, entities, custom response),
             or an error message if processing fails.
    """
    ANSWER_QUERY_LLM_PROMPT_TEMPLATE = """
Given the original search query: "{query}"
And the following search results (JSON format):
---
{results_json}
---
Based *only* on the provided search results, formulate a comprehensive, natural language answer to the original search query.
If the search results are empty or do not seem relevant to the query, state that you couldn't find a specific answer from the provided information.
Do not make up information not present in the results.
Focus on directly answering the query.
Answer:
"""

    SUMMARIZE_RESULTS_LLM_PROMPT_TEMPLATE = """
Given the original search query: "{query}"
And the following search results (JSON format):
---
{results_json}
---
Based *only* on the provided search results, provide a concise summary of the main information found that is relevant to the original search query.
If the search results are empty or do not seem relevant, state that you couldn't find enough information to summarize.
Do not make up information not present in the results.
Summary:
"""

    EXTRACT_ENTITIES_LLM_PROMPT_TEMPLATE = """
Given the original search query: "{query}"
And the following search results (JSON format):
---
{results_json}
---
Based *only* on the provided search results, list the key entities (e.g., people, organizations, locations, dates, specific terms or concepts) that are relevant to the original search query.
If the search results are empty or no distinct entities can be extracted, state that.
Format the output as a comma-separated list or a bulleted list if more appropriate.
Entities:
"""

    CUSTOM_INSTRUCTION_LLM_PROMPT_TEMPLATE = """
Given the original search query: "{query}"
And the following search results (JSON format):
---
{results_json}
---
Based *only* on the provided search results, follow this specific instruction: {custom_instruction}
If the results are insufficient to follow the instruction, state that.
Response:
"""

    try:
        if not search_results_json.strip():
            return "Error: Search results JSON string is empty. Cannot process."
        # No need to parse JSON here if the LLM handles it as a string.
    except Exception as e: # Catch other potential errors with input
        return f"Error preparing data for LLM: {e}"

    formatted_prompt = ""
    model_for_processing = get_model_for_task("summarization") # Default model, can be specialized later if needed

    if processing_instruction == "summarize_results":
        formatted_prompt = SUMMARIZE_RESULTS_LLM_PROMPT_TEMPLATE.format(
            query=search_query,
            results_json=search_results_json
        )
        print(f"process_search_results: Using SUMMARIZE_RESULTS prompt for query '{search_query}'")
    elif processing_instruction == "extract_entities":
        formatted_prompt = EXTRACT_ENTITIES_LLM_PROMPT_TEMPLATE.format(
            query=search_query,
            results_json=search_results_json
        )
        print(f"process_search_results: Using EXTRACT_ENTITIES prompt for query '{search_query}'")
    elif processing_instruction.startswith("custom_instruction:"):
        custom_instruction_text = processing_instruction.split(":", 1)[1].strip()
        if not custom_instruction_text:
            return "Error: Custom instruction is empty."
        formatted_prompt = CUSTOM_INSTRUCTION_LLM_PROMPT_TEMPLATE.format(
            query=search_query,
            results_json=search_results_json,
            custom_instruction=custom_instruction_text
        )
        print(f"process_search_results: Using CUSTOM_INSTRUCTION prompt for query '{search_query}' with instruction: '{custom_instruction_text}'")
    elif processing_instruction == "answer_query":
        formatted_prompt = ANSWER_QUERY_LLM_PROMPT_TEMPLATE.format(
            query=search_query,
            results_json=search_results_json
        )
        print(f"process_search_results: Using ANSWER_QUERY prompt for query '{search_query}'")
    else:
        return f"Error: Unknown processing_instruction: '{processing_instruction}'. Valid options are 'answer_query', 'summarize_results', 'extract_entities', or 'custom_instruction:<your_request>'."

    print(f"process_search_results: Sending prompt to LLM (model: {model_for_processing}) for query '{search_query}' with instruction '{processing_instruction}'")
    
    llm_response = invoke_ollama_model(
        formatted_prompt,
        model_name=model_for_processing,
        temperature=0.5, 
        max_tokens=1000 
    )

    if llm_response:
        return llm_response.strip()
    else:
        return f"Error: LLM failed to generate a response for the query '{search_query}' with instruction '{processing_instruction}' from the provided search results."

# ### END FILE: ai_assistant/custom_tools/my_extra_tools.py ###

# ### START FILE: ai_assistant/custom_tools/project_execution_tools.py ###
### START FILE: ai_assistant/custom_tools/project_execution_tools.py ###
# ai_assistant/custom_tools/project_execution_tools.py
import json
import os
from typing import List, Dict, Any, Optional
from datetime import datetime, timezone

# Import BASE_PROJECTS_DIR and alias it for default usage
from ai_assistant.custom_tools.file_system_tools import (
    sanitize_project_name, 
    BASE_PROJECTS_DIR as DEFAULT_BASE_PROJECTS_DIR, # Alias for default
    read_text_from_file,
    write_text_to_file 
)
# Assuming generate_code_for_project_file is in project_management_tools as per your structure
from ai_assistant.custom_tools.project_management_tools import (
    generate_code_for_project_file,
    initiate_ai_project # Added import
)

# --- Plan Execution Tool ---

async def execute_project_coding_plan(project_name: str, base_projects_dir_override: Optional[str] = None) -> str:
    """
    Executes the coding plan for a given project by generating code for all
    tasks currently in a 'planned' state in the project manifest.

    Args:
        project_name: The name of the project whose coding plan is to be executed.
        base_projects_dir_override (Optional[str]): If provided, overrides the default base projects directory.

    Returns:
        A string summarizing the outcome of the code generation attempts for each
        planned file, including successes, failures, and skipped files.
    """
    if not project_name or not isinstance(project_name, str) or not project_name.strip():
        return "Error: Project name must be a non-empty string."

    successful_generations: List[str] = []
    failed_generations: List[str] = []
    skipped_files: List[str] = []
    files_processed_in_this_run_count = 0
    
    current_base_projects_dir = base_projects_dir_override if base_projects_dir_override is not None else DEFAULT_BASE_PROJECTS_DIR

    sanitized_name = sanitize_project_name(project_name) 
    project_dir = os.path.join(current_base_projects_dir, sanitized_name) 
    manifest_filepath = os.path.join(project_dir, "_ai_project_manifest.json")

    manifest_json_str = read_text_from_file(manifest_filepath)
    if manifest_json_str.startswith("Error:"):
        # If manifest is not found, try to initiate the project
        if "not found" in manifest_json_str.lower():
            print(f"Info: Manifest for project '{project_name}' not found. Attempting to auto-initiate project.")
            auto_description = f"Project '{project_name}' automatically initiated as it did not exist prior to plan execution attempt. User's original intent might provide more specific goals."
            init_result = await initiate_ai_project(project_name, auto_description)
            if init_result.startswith("Error:") or init_result.startswith("Warning:"):
                return f"Error: Failed to auto-initiate missing project '{project_name}'. Initiation tool said: {init_result}"
            print(f"Info: Project '{project_name}' auto-initiated successfully. {init_result}. Proceeding to execute plan.")
            # Re-read manifest after initiation
            manifest_json_str = read_text_from_file(manifest_filepath)
            if manifest_json_str.startswith("Error:"):
                return f"Error: Could not read manifest for '{project_name}' even after auto-initiation. {manifest_json_str}"
        else:
            return f"Error: Could not read project manifest for '{project_name}'. {manifest_json_str}"

    try:
        manifest_data = json.loads(manifest_json_str)
    except json.JSONDecodeError:
        return f"Error: Invalid project manifest format for project '{project_name}'."

    # MODIFIED: Use "development_tasks" key instead of "project_plan"
    development_tasks = manifest_data.get("development_tasks", [])
    if not development_tasks:
        return f"Info: Project plan (development_tasks) for '{project_name}' is empty or missing. Nothing to execute."

    # Iterate using the correct variable
    for task_entry in list(development_tasks): 
        if not isinstance(task_entry, dict): # Add a check
            print(f"Warning: Skipping malformed task entry in manifest for '{project_name}': {task_entry}")
            continue
            
        task_details = task_entry.get("details", {})
        filename = task_details.get("filename") # Filename is inside 'details' for CREATE_FILE tasks
        status = task_entry.get("status")
        task_type = task_entry.get("task_type")
        task_id = task_entry.get("task_id", "Unknown Task ID")


        if task_type != "CREATE_FILE" or not filename:
            # Log skipped non-CREATE_FILE tasks or tasks missing filenames appropriately
            if task_type != "CREATE_FILE":
                 print(f"Info: Skipping task '{task_id}' as it's not a CREATE_FILE task (type: {task_type}).")
            else: 
                 print(f"Warning: Skipping malformed CREATE_FILE task '{task_id}' for '{project_name}' (missing filename in details): {task_entry}")
            skipped_files.append(f"{filename or task_id} (type: {task_type or 'Unknown'}, status: {status or 'Unknown'})")
            continue

        if status == "planned":
            files_processed_in_this_run_count += 1
            print(f"Info: Attempting to generate code for '{filename}' (Task ID: {task_id}) in project '{project_name}'...")
            
            # generate_code_for_project_file is imported.
            # It's responsible for reading the manifest again to get detailed context for the specific file.
            generation_result_str = await generate_code_for_project_file(project_name, filename) 
            
            if generation_result_str.startswith("Success:"):
                successful_generations.append(f"{filename} (Task ID: {task_id}): Generation reported success.")
                # Note: generate_code_for_project_file is responsible for updating this task's status in the manifest
            else:
                failed_generations.append(f"{filename} (Task ID: {task_id}): {generation_result_str}")
                # Note: generate_code_for_project_file should ideally mark the task as 'failed' in the manifest
        else:
            skipped_files.append(f"{filename} (Task ID: {task_id}, status: {status})")
    
    if files_processed_in_this_run_count > 0 or successful_generations or failed_generations:
        # Re-read manifest to get its latest state (potentially modified by generate_code_for_project_file)
        current_manifest_content_for_timestamp_update = read_text_from_file(manifest_filepath) 
        if not current_manifest_content_for_timestamp_update.startswith("Error:"):
            try:
                final_manifest_data = json.loads(current_manifest_content_for_timestamp_update)
                final_manifest_data["last_modified_timestamp"] = datetime.now(timezone.utc).isoformat()
                write_result = write_text_to_file(manifest_filepath, json.dumps(final_manifest_data, indent=4))
                if write_result.startswith("Error:"): # pragma: no cover
                    print(f"Warning: Failed to update manifest's last_modified_timestamp for project '{project_name}' after plan execution: {write_result}")
            except json.JSONDecodeError: # pragma: no cover
                print(f"Warning: Could not parse manifest for final timestamp update in project '{project_name}' after plan execution.")
        else: # pragma: no cover
             print(f"Warning: Could not re-read manifest for final timestamp update in project '{project_name}' after plan execution: {current_manifest_content_for_timestamp_update}")

    if files_processed_in_this_run_count == 0 and not successful_generations and not failed_generations:
        report_lines_final = [
            f"Info: No files in 'planned' state found for project '{project_name}'. Nothing to do."
        ]
        if skipped_files:
            report_lines_final.append("\nFiles not processed (e.g., already generated, errored, or not CREATE_FILE type):")
            for skipped_info in skipped_files:
                report_lines_final.append(f"  - {skipped_info}")
        return "\n".join(report_lines_final)

    report_lines = [f"Code generation summary for project '{project_name}':"]
    
    if successful_generations:
        report_lines.append("\nSuccessfully generated code for:")
        for success_info in successful_generations:
            report_lines.append(f"  - {success_info}")
    
    if failed_generations:
        report_lines.append("\nFailed to generate code for:")
        for fail_info in failed_generations:
            report_lines.append(f"  - {fail_info}")
            
    if skipped_files: 
        report_lines.append("\nSkipped (not in 'planned' state, already processed, or not CREATE_FILE type):")
        for skipped_info in skipped_files:
            report_lines.append(f"  - {skipped_info}")
            
    report_lines.append(f"\nSummary: {len(successful_generations)} of {files_processed_in_this_run_count} attempted CREATE_FILE tasks generated successfully.")
    
    return "\n".join(report_lines)

# --- New Code Generation and Review Tool (as provided in your original file) ---
from ai_assistant.llm_interface.ollama_client import invoke_ollama_model_async
from ai_assistant.config import get_model_for_task
from ai_assistant.core.reviewer import ReviewerAgent
from ai_assistant.core.refinement import RefinementAgent 
import re 

GENERAL_CODE_GENERATION_PROMPT_TEMPLATE = """
You are an AI assistant tasked with generating Python code.
Description of desired code:
{description}

Target file path (optional, for context): {target_file_path}
Existing code context (optional, from the target file or related files):
{existing_code_context}

Please generate the Python code that fulfills the description.
Respond ONLY with the raw Python code. Do not include any other explanations or markdown formatting like ```python.
"""

async def generate_and_review_code_tool(
    description: str,
    target_file_path: Optional[str] = None,
    existing_code_context: Optional[str] = None
) -> Dict[str, Any]:
    """
    Generates Python code based on a description, then reviews it automatically.
    Includes iterative refinement based on review feedback.

    Args:
        description: Detailed description of the desired code functionality.
        target_file_path: (Optional) The intended file path for the generated code (for context).
        existing_code_context: (Optional) Snippets of existing code for context.

    Returns:
        A dictionary containing the generated code, review results,
        suggested file path, and overall status.
    """
    if not description:
        return {
            "generated_code": None,
            "review_results": {"status": "error", "comments": "Error: Description must be provided.", "suggestions": ""},
            "suggested_file_path": target_file_path,
            "status": "error"
        }

    # 1. Generate Code
    prompt = GENERAL_CODE_GENERATION_PROMPT_TEMPLATE.format(
        description=description,
        target_file_path=target_file_path or "Not specified",
        existing_code_context=existing_code_context or "Not provided"
    )
    
    code_gen_model = get_model_for_task("code_generation") 
    raw_generated_code = await invoke_ollama_model_async(prompt, model_name=code_gen_model, temperature=0.5, max_tokens=2000)

    if not raw_generated_code or not raw_generated_code.strip():
        return {
            "generated_code": None,
            "review_results": {"status": "error", "comments": "LLM failed to generate code.", "suggestions": ""},
            "suggested_file_path": target_file_path,
            "status": "error"
        }

    cleaned_code = re.sub(r"^\s*```python\s*\n?", "", raw_generated_code, flags=re.IGNORECASE)
    cleaned_code = re.sub(r"\n?\s*```\s*$", "", cleaned_code, flags=re.IGNORECASE).strip()

    if not cleaned_code:
        return {
            "generated_code": None,
            "review_results": {"status": "error", "comments": "LLM generated empty code after cleaning.", "suggestions": ""},
            "suggested_file_path": target_file_path,
            "status": "error"
        }

    current_generated_code = cleaned_code
    current_review_results: Dict[str, Any]
    reviewer = ReviewerAgent()
    refinement_agent = RefinementAgent()
    max_refinement_attempts = 2 

    for attempt in range(max_refinement_attempts + 1): 
        review_attempt_number = attempt + 1
        print(f"generate_and_review_code_tool: Reviewing code (Attempt {review_attempt_number})...")
        try:
            current_review_results = await reviewer.review_code(
                code_to_review=current_generated_code,
                original_requirements=description,
                attempt_number=review_attempt_number
            )
        except Exception as e: # pragma: no cover
            current_review_results = {
                "status": "error",
                "comments": f"Error during code review (Attempt {review_attempt_number}): {e}",
                "suggestions": ""
            }
            break 

        review_status = current_review_results.get("status", "error")
        print(f"generate_and_review_code_tool: Review Status (Attempt {review_attempt_number}): {review_status.upper()}")

        if review_status == "approved" or review_status == "rejected" or review_status == "error":
            break 

        if review_status == "requires_changes" and attempt < max_refinement_attempts:
            print(f"generate_and_review_code_tool: Code requires changes. Attempting refinement {attempt + 1}/{max_refinement_attempts}...")
            refined_code_str = await refinement_agent.refine_code(
                original_code=current_generated_code,
                requirements=description,
                review_feedback=current_review_results
            )
            if not refined_code_str or not refined_code_str.strip(): # pragma: no cover
                print(f"generate_and_review_code_tool: Refinement attempt {attempt + 1} did not produce new code. Using previous code.")
                break 
            current_generated_code = refined_code_str
        elif attempt >= max_refinement_attempts : # pragma: no cover
             print(f"generate_and_review_code_tool: Max refinement attempts reached. Using last reviewed code.")
             break

    return {
        "generated_code": current_generated_code,
        "review_results": current_review_results,
        "suggested_file_path": target_file_path,
        "status": current_review_results.get("status", "error") 
    }

if __name__ == '__main__':
    import asyncio
    import unittest
    from unittest.mock import patch, AsyncMock 
    import shutil 

    TEST_BASE_PROJECTS_DIR_FOR_DI = "temp_test_ai_projects_execution_di_final_v5" 

    def mock_sanitize_project_name_for_tests(name): 
        return "".join(c if c.isalnum() or c in (' ', '_', '-') else '_' for c in name).strip().replace(' ', '_').lower()

    class TestExecuteProjectCodingPlan(unittest.TestCase):
        
        TEST_BASE_DIR = TEST_BASE_PROJECTS_DIR_FOR_DI 

        @classmethod
        def setUpClass(cls):
            cls.TEST_BASE_DIR = TEST_BASE_PROJECTS_DIR_FOR_DI 
            if os.path.exists(cls.TEST_BASE_DIR): 
                shutil.rmtree(cls.TEST_BASE_DIR)
            os.makedirs(cls.TEST_BASE_DIR, exist_ok=True)

        @classmethod
        def tearDownClass(cls):
            if os.path.exists(cls.TEST_BASE_DIR):
                shutil.rmtree(cls.TEST_BASE_DIR)
        
        def _get_expected_manifest_path(self, project_name): 
            sanitized_name = mock_sanitize_project_name_for_tests(project_name)
            return os.path.join(self.TEST_BASE_DIR, sanitized_name, "_ai_project_manifest.json")

        def _create_mock_manifest_data(self, project_name, development_tasks_entries): 
            sanitized_proj_name = mock_sanitize_project_name_for_tests(project_name)
            project_dir = os.path.join(self.TEST_BASE_DIR, sanitized_proj_name)
            os.makedirs(project_dir, exist_ok=True) 
            
            return {
                "project_name": project_name,
                "sanitized_project_name": sanitized_proj_name,
                "project_directory": project_dir, 
                "project_description": "Test description",
                "creation_timestamp": datetime.now(timezone.utc).isoformat(),
                "last_modified_timestamp": datetime.now(timezone.utc).isoformat(),
                "version": "0.1.0",
                "manifest_version": "1.1.0",
                "project_type": "python",
                "entry_points": {},
                "dependencies": [],
                "build_config": {"build_command": None, "output_directory": None, "source_directories": ["src"]},
                "test_config": {"test_command": None, "test_directory": "tests"},
                "project_goals": [],
                "development_tasks": development_tasks_entries, 
                "project_notes": None
            }

        @patch('ai_assistant.custom_tools.project_execution_tools.sanitize_project_name', side_effect=mock_sanitize_project_name_for_tests)
        @patch('ai_assistant.custom_tools.project_execution_tools.read_text_from_file')
        @patch('ai_assistant.custom_tools.project_execution_tools.write_text_to_file')
        @patch('ai_assistant.custom_tools.project_execution_tools.generate_code_for_project_file', new_callable=AsyncMock)
        def test_one_file_planned_success(self, mock_gen_code, mock_write_file, mock_read_file, mock_sanitize_name_unused):
            project_name = "TestOneFileDI"
            manifest_data = self._create_mock_manifest_data(project_name, [
                {"task_id": "T001", "task_type": "CREATE_FILE", "description": "Main file task", 
                 "details": {"filename": "main.py", "original_description": "Main app logic"}, 
                 "status": "planned", "dependencies": []}
            ])
            mock_read_file.return_value = json.dumps(manifest_data)
            mock_gen_code.return_value = "Success: Code for 'main.py' generated."
            # Simulate manifest being updated by generate_code_for_project_file
            updated_manifest_data = self._create_mock_manifest_data(project_name, [
                {"task_id": "T001", "task_type": "CREATE_FILE", "description": "Main file task", 
                 "details": {"filename": "main.py", "original_description": "Main app logic"}, 
                 "status": "generated", "dependencies": [], "last_attempt_timestamp": datetime.now(timezone.utc).isoformat()}
            ])
            # Mock read_text_from_file to return updated manifest for the timestamp update part
            mock_read_file.side_effect = [json.dumps(manifest_data), json.dumps(updated_manifest_data)]
            mock_write_file.return_value = "Success: Manifest updated."

            result = asyncio.run(execute_project_coding_plan(project_name, base_projects_dir_override=self.TEST_BASE_DIR))
            
            mock_gen_code.assert_called_once_with(project_name, "main.py")
            self.assertIn("Successfully generated code for:", result)
            self.assertIn("main.py (Task ID: T001): Generation reported success.", result)
            self.assertIn("Summary: 1 of 1 attempted CREATE_FILE tasks generated successfully.", result)
            # Check if write_text_to_file was called to update the timestamp
            self.assertTrue(mock_write_file.called)


        @patch('ai_assistant.custom_tools.project_execution_tools.sanitize_project_name', side_effect=mock_sanitize_project_name_for_tests)
        @patch('ai_assistant.custom_tools.project_execution_tools.read_text_from_file')
        @patch('ai_assistant.custom_tools.project_execution_tools.write_text_to_file')
        @patch('ai_assistant.custom_tools.project_execution_tools.generate_code_for_project_file', new_callable=AsyncMock)
        def test_all_files_generated_nothing_to_do(self, mock_gen_code, mock_write_file, mock_read_file, mock_sanitize_name_unused):
            project_name = "TestAllGeneratedDI"
            manifest_data = self._create_mock_manifest_data(project_name, [
                 {"task_id": "T001", "task_type": "CREATE_FILE", "description": "Main file", 
                  "details": {"filename": "main.py"}, "status": "generated", "dependencies": []},
                 {"task_id": "T002", "task_type": "CREATE_FILE", "description": "Utils file", 
                  "details": {"filename": "utils.py"}, "status": "generated", "dependencies": []}
            ])
            mock_read_file.return_value = json.dumps(manifest_data)

            result = asyncio.run(execute_project_coding_plan(project_name, base_projects_dir_override=self.TEST_BASE_DIR))

            mock_gen_code.assert_not_called()
            self.assertIn("Info: No files in 'planned' state found", result)
            self.assertIn("Skipped (not in 'planned' state", result) 
            self.assertIn("main.py (Task ID: T001, status: generated)", result)
            mock_write_file.assert_not_called()

        @patch('ai_assistant.custom_tools.project_execution_tools.sanitize_project_name', side_effect=mock_sanitize_project_name_for_tests)
        @patch('ai_assistant.custom_tools.project_execution_tools.read_text_from_file')
        @patch('ai_assistant.custom_tools.project_execution_tools.write_text_to_file')
        @patch('ai_assistant.custom_tools.project_execution_tools.generate_code_for_project_file', new_callable=AsyncMock)
        def test_one_file_planned_generation_fails(self, mock_gen_code, mock_write_file, mock_read_file, mock_sanitize_name_unused):
            project_name = "TestOneFileFailDI"
            manifest_data = self._create_mock_manifest_data(project_name, [
                 {"task_id": "T001", "task_type": "CREATE_FILE", "description": "App file", 
                  "details": {"filename": "app.py"}, "status": "planned", "dependencies": []}
            ])
            mock_read_file.return_value = json.dumps(manifest_data)
            mock_gen_code.return_value = "Error: LLM failed for app.py"
            
            # Simulate manifest being updated by generate_code_for_project_file to mark as failed
            updated_manifest_data = self._create_mock_manifest_data(project_name, [
                {"task_id": "T001", "task_type": "CREATE_FILE", "description": "App file", 
                 "details": {"filename": "app.py"}, "status": "failed", "dependencies": [], 
                 "error_message": "LLM failed for app.py", "last_attempt_timestamp": datetime.now(timezone.utc).isoformat()}
            ])
            mock_read_file.side_effect = [json.dumps(manifest_data), json.dumps(updated_manifest_data)]
            mock_write_file.return_value = "Success: Manifest updated."


            result = asyncio.run(execute_project_coding_plan(project_name, base_projects_dir_override=self.TEST_BASE_DIR))

            mock_gen_code.assert_called_once_with(project_name, "app.py")
            self.assertIn("Failed to generate code for:", result)
            self.assertIn("app.py (Task ID: T001): Error: LLM failed for app.py", result)
            self.assertIn("Summary: 0 of 1 attempted CREATE_FILE tasks generated successfully.", result)
            self.assertTrue(mock_write_file.called)


        @patch('ai_assistant.custom_tools.project_execution_tools.sanitize_project_name', side_effect=mock_sanitize_project_name_for_tests)
        @patch('ai_assistant.custom_tools.project_execution_tools.read_text_from_file')
        @patch('ai_assistant.custom_tools.project_execution_tools.generate_code_for_project_file', new_callable=AsyncMock) 
        def test_missing_manifest(self, mock_gen_code, mock_read_file, mock_sanitize_name_unused):
            project_name = "TestMissingManifestDI"
            expected_manifest_path = self._get_expected_manifest_path(project_name)
            mock_read_file.return_value = f"Error: File '{expected_manifest_path}' not found."
            
            result = asyncio.run(execute_project_coding_plan(project_name, base_projects_dir_override=self.TEST_BASE_DIR))
            
            self.assertIn(f"Error: Could not read project manifest for '{project_name}'. Error: File '{expected_manifest_path}' not found.", result)
            mock_gen_code.assert_not_called()

        @patch('ai_assistant.custom_tools.project_execution_tools.sanitize_project_name', side_effect=mock_sanitize_project_name_for_tests)
        @patch('ai_assistant.custom_tools.project_execution_tools.read_text_from_file')
        @patch('ai_assistant.custom_tools.project_execution_tools.write_text_to_file')
        @patch('ai_assistant.custom_tools.project_execution_tools.generate_code_for_project_file', new_callable=AsyncMock)
        def test_empty_project_plan(self, mock_gen_code, mock_write_file, mock_read_file, mock_sanitize_name_unused):
            project_name = "TestEmptyPlanDI"
            # Pass an empty list for development_tasks
            manifest_data = self._create_mock_manifest_data(project_name, []) 
            mock_read_file.return_value = json.dumps(manifest_data)

            result = asyncio.run(execute_project_coding_plan(project_name, base_projects_dir_override=self.TEST_BASE_DIR))
            self.assertIn(f"Info: Project plan (development_tasks) for '{project_name}' is empty or missing. Nothing to execute.", result)
            mock_gen_code.assert_not_called()
            mock_write_file.assert_not_called() # Timestamp not updated if no files processed

    print("Running automated tests for execute_project_coding_plan (v5 - dependency injection)...")
    unittest.main(argv=['first-arg-is-ignored'], exit=False, verbosity=2)
### END FILE: ai_assistant/custom_tools/project_execution_tools.py ###
# ### END FILE: ai_assistant/custom_tools/project_execution_tools.py ###

# ### START FILE: ai_assistant/custom_tools/project_management_tools.py ###
# ai_assistant/custom_tools/project_management_tools.py
import json
import subprocess
import os
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional

# Updated imports for ProjectManifest and related dataclasses
from ai_assistant.project_management.manifest_schema import ProjectManifest, DevelopmentTask, BuildConfig, TestConfig, Dependency
from ai_assistant.llm_interface.ollama_client import invoke_ollama_model_async 
from ai_assistant.config import get_model_for_task
from ai_assistant.custom_tools.file_system_tools import (
    create_project_directory, 
    write_text_to_file, 
    sanitize_project_name, 
    BASE_PROJECTS_DIR,
    read_text_from_file # Added missing import
)
import logging # Added for consistency

logger = logging.getLogger(__name__) # Added logger

PROJECT_PLANNING_PROMPT_TEMPLATE = """
You are an AI assistant helping to plan a new software project.
Project Description:
{project_description}

Based on this description, break the project down into a list of essential code files.
For each file, provide:
- "filename" (e.g., "main.py", "utils/helper.js")
- "description" (a concise, one-sentence explanation of the file's purpose)
- "key_components" (a list of strings, each describing a key function, class, or module within that file)
- "dependencies" (a list of strings, each naming other planned filenames this file will depend on, or an empty list if none)

Respond with a JSON object containing a single key "project_plan", which is a list of these dictionaries.
Example:
{{
  "project_plan": [
    {{
      "filename": "app.py",
      "description": "Main application script that orchestrates the web server and routes.",
      "key_components": ["Flask app initialization", "Main route for /", "API endpoint /data"],
      "dependencies": ["data_handler.py", "utils.py"]
    }},
    {{
      "filename": "data_handler.py",
      "description": "Module for loading and processing data.",
      "key_components": ["load_data_from_source", "process_data_for_api"],
      "dependencies": []
    }},
    {{
      "filename": "utils.py",
      "description": "Utility functions for general use across the project.",
      "key_components": ["format_response", "error_handling_wrapper"],
      "dependencies": []
    }}
  ]
}}
If the project description is too vague or simple for a multi-file breakdown (e.g., "a script to print hello world"), plan for a single file, typically named after the project or 'main.py', with empty lists for key_components and dependencies.
Ensure filenames are conventional (e.g., use .py for Python, .js for JavaScript if language is implied or stated). If no language is specified, assume Python.
Do not include any other explanatory text or markdown formatting like ```json ... ``` around the JSON object.
"""

async def initiate_ai_project(project_name: str, project_description: str) -> str:
    """
    Initializes a new AI-managed software project. 
    
    This involves:
    1. Sanitizing the project name and creating a project directory.
    2. Calling an LLM to generate a basic project plan (list of files and descriptions).
    3. Saving a manifest file (_ai_project_manifest.json) in the project directory.

    Args:
        project_name: The desired name for the project.
        project_description: A brief description of what the project is about.

    Returns:
        A string confirming successful initialization and summarizing the plan, 
        or an error message if any step fails.
    """
    if not project_name or not isinstance(project_name, str) or not project_name.strip():
        return "Error: Project name must be a non-empty string."
    if not project_description or not isinstance(project_description, str) or not project_description.strip():
        return "Error: Project description must be a non-empty string."

    # 1. Sanitize project name (create_project_directory will also do this, but we need it for paths)
    sanitized_name = sanitize_project_name(project_name)
    
    # 2. Create project directory
    # create_project_directory returns a message like "Success: Project directory '{full_path}' created."
    # or "Error: ..."
    dir_creation_result = create_project_directory(project_name) # Uses the original name, sanitizes internally

    if dir_creation_result.startswith("Error:"):
        return dir_creation_result

    # Extract the actual path from the success message if possible, or reconstruct it.
    # Assuming success message format: "Success: Project directory '{path}' created."
    try:
        # More robust way to get the path: construct it as sanitize_project_name and create_project_directory would.
        project_dir_path = os.path.join(BASE_PROJECTS_DIR, sanitized_name)
        if not os.path.exists(project_dir_path): # Should exist if dir_creation_result was not an error
             return f"Error: Project directory creation reported success, but directory '{project_dir_path}' not found."
    except Exception as e: # Catch any parsing error
        return f"Error: Could not determine project directory path after creation. Detail: {e}"

    # 3. Create Basic Directory Structure (src, tests, README.md)
    try:
        os.makedirs(os.path.join(project_dir_path, "src"), exist_ok=True)
        os.makedirs(os.path.join(project_dir_path, "tests"), exist_ok=True)
        readme_content = f"# {project_name}\n\n{project_description}"
        readme_path = os.path.join(project_dir_path, "README.md")
        readme_write_result = write_text_to_file(readme_path, readme_content)
        if readme_write_result.startswith("Error:"):
            # Log warning but proceed, as manifest is more critical for now
            print(f"Warning: Failed to write README.md for project '{project_name}'. {readme_write_result}")
    except Exception as e_dir:
        return f"Error: Failed to create basic directory structure (src, tests, README.md) in '{project_dir_path}'. Detail: {e_dir}"

    # 4. LLM Call for Planning
    prompt = PROJECT_PLANNING_PROMPT_TEMPLATE.format(project_description=project_description)
    llm_model = get_model_for_task("planning")

    plan_response_str = await invoke_ollama_model_async(prompt, model_name=llm_model)
    development_tasks_list: List[DevelopmentTask] = [] # Changed to store DevelopmentTask objects

    if not plan_response_str or not plan_response_str.strip():
        return "Error: Failed to get project plan from LLM."

    try:
        if plan_response_str.startswith("```json"):
            plan_response_str = plan_response_str.lstrip("```json").rstrip("```").strip()
        elif plan_response_str.startswith("```"):
            plan_response_str = plan_response_str.lstrip("```").rstrip("```").strip()
        
        parsed_json = json.loads(plan_response_str)

        if not isinstance(parsed_json, dict) or "project_plan" not in parsed_json or not isinstance(parsed_json.get("project_plan"), list):
            return "Error: LLM returned an invalid plan structure (missing 'project_plan' list)."

        raw_plan_list = parsed_json.get("project_plan", [])
        
        if not raw_plan_list:
            print(f"Warning: LLM returned an empty project plan for '{project_name}'. Proceeding with empty plan.")

        for i, file_dict in enumerate(raw_plan_list): # Use raw_plan_list
            if not isinstance(file_dict, dict):
                print(f"Warning: Invalid item in LLM project plan (not a dict): {file_dict}. Skipping this item.")
                continue

            filename = file_dict.get("filename")
            description = file_dict.get("description") # This is file description
            key_components = file_dict.get("key_components")
            dependencies = file_dict.get("dependencies") # These are file-level dependencies

            if not (filename and isinstance(filename, str) and 
                    description and isinstance(description, str)):
                print(f"Warning: Invalid item in LLM project plan (missing/invalid filename or description): {file_dict}. Skipping this item.")
                continue

            if not isinstance(key_components, list):
                print(f"Warning: 'key_components' for file '{filename}' is not a list. Defaulting to empty list. Original: {key_components}")
                key_components = []
            else:
                key_components = [str(kc) for kc in key_components]

            if not isinstance(dependencies, list):
                print(f"Warning: 'dependencies' for file '{filename}' is not a list. Defaulting to empty list. Original: {dependencies}")
                dependencies = []
            else:
                dependencies = [str(dep) for dep in dependencies]
            
            task_id = f"TASK{i+1:03d}"
            dev_task = DevelopmentTask(
                task_id=task_id,
                task_type="CREATE_FILE",
                description=f"Define structure and generate code for {filename}", # Task description
                details={ # Original file plan details from LLM
                    "filename": filename,
                    "original_description": description, # File's own description
                    "key_components": key_components,
                    "file_dependencies": dependencies 
                },
                status="planned" # Default status for new tasks
            )
            development_tasks_list.append(dev_task)
            
    except json.JSONDecodeError:
        return f"Error: LLM returned invalid JSON for project plan. Response: {plan_response_str}"

    # 5. Create ProjectManifest Instance
    current_timestamp_iso = datetime.now(timezone.utc).isoformat()
    manifest_instance = ProjectManifest(
        project_name=project_name,
        sanitized_project_name=sanitized_name,
        project_directory=project_dir_path, # Relative path from BASE_PROJECTS_DIR
        project_description=project_description,
        creation_timestamp=current_timestamp_iso,
        last_modified_timestamp=current_timestamp_iso,
        manifest_version="1.1.0",
        version="0.1.0",
        project_type="python", # Default for now
        development_tasks=development_tasks_list,
        entry_points={},
        dependencies=[], # Project-level dependencies
        project_goals=[],
        build_config=BuildConfig(), # Default instance
        test_config=TestConfig()    # Default instance
    )
    
    manifest_data_dict = manifest_instance.to_json_dict()

    # 6. Save Manifest
    manifest_filepath = os.path.join(project_dir_path, "_ai_project_manifest.json")
    write_result = write_text_to_file(manifest_filepath, json.dumps(manifest_data_dict, indent=4))

    if write_result.startswith("Error:"):
        return f"Error: Project directory and structure created at '{project_dir_path}', but failed to write manifest file. {write_result}"

    # 7. Return Success Message
    task_count = len(development_tasks_list)
    if task_count == 0:
        plan_summary = "empty plan (no development tasks specified or all entries malformed)."
    else:
        plan_summary = f"{task_count} development task(s) (primarily file creation tasks)."
        
    return (f"Success: Project '{project_name}' initialized in '{project_dir_path}' with src, tests dirs and README.md. "
            f"Plan includes {plan_summary} You can now ask to generate code for these tasks.")

# --- Code Generation Tool ---

CODE_GENERATION_PROMPT_TEMPLATE = """
You are an AI assistant helping to write code for a software project.
Overall Project Description:
{project_description}

Current File to Generate: {filename}
Purpose of this file (from project plan): {file_description}

Key Components for this file:
{key_components_str}

Dependencies for this file (other files in this project):
{dependencies_str}

Based on the project description, the file's purpose, its key components, and its dependencies, generate the complete code for the file '{filename}'.
- Ensure the code is functional and adheres to common best practices for the inferred language (assume Python if not specified).
- Only output the raw code for the file. Do not include any explanations, comments that are not part of the code itself, or markdown formatting like ```python ... ```.
- If the file description implies it needs to interact with other planned files, write the code assuming those other files will exist and provide the described functionality.
"""

async def generate_code_for_project_file(project_name: str, filename: str) -> str:
    """
    Generates code for a specific file within an AI-managed project,
    based on the project plan stored in the project's manifest.

    Args:
        project_name: The name of the project.
        filename: The name of the file for which to generate code (e.g., "main.py").

    Returns:
        A string confirming code generation and saving, or an error/info message.
    """
    if not project_name or not isinstance(project_name, str) or not project_name.strip():
        return "Error: Project name must be a non-empty string."
    if not filename or not isinstance(filename, str) or not filename.strip():
        return "Error: Filename must be a non-empty string."

    sanitized_project_name = sanitize_project_name(project_name)
    project_dir = os.path.join(BASE_PROJECTS_DIR, sanitized_project_name)
    manifest_filepath = os.path.join(project_dir, "_ai_project_manifest.json")

    # 1. Load Manifest using ProjectManifest dataclass
    manifest_json_str = read_text_from_file(manifest_filepath)
    if manifest_json_str.startswith("Error:"):
        return f"Error: Could not read project manifest for '{project_name}'. {manifest_json_str}"

    try:
        manifest_dict = json.loads(manifest_json_str)
        manifest_instance = ProjectManifest.from_dict(manifest_dict)
    except json.JSONDecodeError as e:
        return f"Error: Failed to parse project manifest for '{project_name}'. Invalid JSON. {e}"
    except Exception as e_manifest: # Catch errors from ProjectManifest.from_dict
        return f"Error: Failed to load project manifest data into ProjectManifest object for '{project_name}'. Detail: {e_manifest}"

    # 2. Find File Task in development_tasks
    file_task_entry: Optional[DevelopmentTask] = None
    # task_index = -1 # Not strictly needed if DevelopmentTask objects are mutable and part of the list
    
    for task in manifest_instance.development_tasks:
        if task.task_type == "CREATE_FILE" and task.details.get("filename") == filename:
            file_task_entry = task
            break
    
    if not file_task_entry:
        return f"Error: File task for '{filename}' not found in project development tasks for '{project_name}'."

    if file_task_entry.status == "generated":
        return f"Info: Code for '{filename}' in project '{project_name}' (Task ID: {file_task_entry.task_id}) has already been generated. Overwrite functionality is not yet supported."

    # Extracting prompt info from the found task and manifest
    overall_project_desc = manifest_instance.project_description
    file_task_details = file_task_entry.details
    
    file_plan_description = file_task_details.get("original_description", "No specific file description provided in task details.")
    key_components_list = file_task_details.get("key_components", [])
    dependencies_list = file_task_details.get("file_dependencies", []) # Note: key in details is "file_dependencies"

    if not isinstance(key_components_list, list):
        print(f"Warning: 'key_components' for {filename} in task {file_task_entry.task_id} is not a list. Original: {key_components_list}. Using empty list.")
        key_components_list = []
    key_components_str = "\n".join([f"- {str(item)}" for item in key_components_list]) if key_components_list else "No specific key components listed."

    if not isinstance(dependencies_list, list):
        print(f"Warning: 'file_dependencies' for {filename} in task {file_task_entry.task_id} is not a list. Original: {dependencies_list}. Using empty list.")
        dependencies_list = []
    dependencies_str = ", ".join([str(item) for item in dependencies_list]) if dependencies_list else "None listed."

    # 3. LLM Call for Code Generation (remains largely the same)
    prompt = CODE_GENERATION_PROMPT_TEMPLATE.format(
        project_description=overall_project_desc,
        filename=filename,
        file_description=file_plan_description,
        key_components_str=key_components_str,
        dependencies_str=dependencies_str
    )
    llm_model = get_model_for_task("code_generation")
    
    print(f"Info: Generating code for '{filename}' (Task ID: {file_task_entry.task_id}) in project '{project_name}' using model '{llm_model}'...")
    generated_code = await invoke_ollama_model_async(prompt, model_name=llm_model, temperature=0.5, max_tokens=4096) # Increased max_tokens

    if not generated_code or not generated_code.strip():
        file_task_entry.status = "failed" # Update status on failure
        file_task_entry.error_message = "LLM failed to generate code or returned empty code."
        file_task_entry.last_attempt_timestamp = datetime.now(timezone.utc).isoformat()
        manifest_instance.last_modified_timestamp = datetime.now(timezone.utc).isoformat()
        # Attempt to save manifest even on code gen failure to record the attempt
        try:
            updated_manifest_dict_on_fail = manifest_instance.to_json_dict()
            write_text_to_file(manifest_filepath, json.dumps(updated_manifest_dict_on_fail, indent=4))
        except Exception as e_save_fail:
            print(f"Warning: Failed to update manifest after code generation failure for task {file_task_entry.task_id}. Error: {e_save_fail}")
        return f"Error: LLM failed to generate code for '{filename}'. Task '{file_task_entry.task_id}' marked as failed."
    
    if generated_code.startswith("```python"):
        generated_code = generated_code.lstrip("```python").rstrip("```").strip()
    elif generated_code.startswith("```"):
        generated_code = generated_code.lstrip("```").rstrip("```").strip()

    # 4. Handle Directory Structure for Saving Code
    target_dir = project_dir # Default to project root
    if manifest_instance.build_config and \
       manifest_instance.build_config.source_directories and \
       isinstance(manifest_instance.build_config.source_directories, list) and \
       len(manifest_instance.build_config.source_directories) > 0:
        target_dir = os.path.join(project_dir, manifest_instance.build_config.source_directories[0])
    
    try:
        os.makedirs(target_dir, exist_ok=True)
    except OSError as e_dir:
        return f"Error: Could not create target directory '{target_dir}' for file '{filename}'. Detail: {e_dir}"
        
    code_filepath = os.path.join(target_dir, filename)
    write_result = write_text_to_file(code_filepath, generated_code)
    
    if write_result.startswith("Error:"):
        file_task_entry.status = "failed" # Update status on failure
        file_task_entry.error_message = f"Failed to write generated code to file: {write_result}"
        file_task_entry.last_attempt_timestamp = datetime.now(timezone.utc).isoformat()
        manifest_instance.last_modified_timestamp = datetime.now(timezone.utc).isoformat()
        try:
            updated_manifest_dict_on_write_fail = manifest_instance.to_json_dict()
            write_text_to_file(manifest_filepath, json.dumps(updated_manifest_dict_on_write_fail, indent=4))
        except Exception as e_save_write_fail:
            print(f"Warning: Failed to update manifest after file write failure for task {file_task_entry.task_id}. Error: {e_save_write_fail}")
        return f"Error: Failed to write generated code for '{filename}' to file. {write_result}. Task '{file_task_entry.task_id}' marked as failed."

    # 5. Update Manifest After Successful Code Saving
    file_task_entry.status = "generated"
    file_task_entry.last_attempt_timestamp = datetime.now(timezone.utc).isoformat() # Or a new 'completion_timestamp'
    file_task_entry.error_message = None # Clear previous errors if any
    manifest_instance.last_modified_timestamp = datetime.now(timezone.utc).isoformat()
    
    try:
        updated_manifest_dict_success = manifest_instance.to_json_dict()
        manifest_write_result = write_text_to_file(manifest_filepath, json.dumps(updated_manifest_dict_success, indent=4))
        if manifest_write_result.startswith("Error:"):
            return (f"Warning: Code for '{filename}' (Task ID: {file_task_entry.task_id}) generated and saved to '{code_filepath}', "
                    f"but failed to update manifest. {manifest_write_result}")
    except Exception as e_final_save:
         return (f"Warning: Code for '{filename}' (Task ID: {file_task_entry.task_id}) generated and saved to '{code_filepath}', "
                 f"but encountered an error during final manifest serialization/save. Error: {e_final_save}")


    return (f"Success: Code for '{filename}' (Task ID: {file_task_entry.task_id}) generated and saved to '{code_filepath}' "
            f"in project '{project_name}'. Manifest updated.")


async def add_dependency_to_project(
    project_name: str, 
    dependency_name: str, 
    dependency_version: Optional[str] = None, 
    dependency_type: Optional[str] = None  # e.g., "pip", "npm" - could be inferred from project_type in manifest
) -> str:
    """
    Adds a dependency to the specified project's manifest.
    (Future implementation would also attempt to install it using package managers).

    Args:
        project_name: The name of the project.
        dependency_name: The name of the dependency (e.g., "requests", "react").
        dependency_version: The specific version of the dependency (e.g., "2.25.1", "^17.0.2").
        dependency_type: The type of dependency, if needed (e.g., "pip", "npm").

    Returns:
        A string confirming the action or an error message.
    """
    if not project_name or not isinstance(project_name, str) or not project_name.strip():
        return "Error: Project name must be a non-empty string."
    if not dependency_name or not isinstance(dependency_name, str) or not dependency_name.strip():
        return "Error: Dependency name must be a non-empty string."

    sanitized_proj_name = sanitize_project_name(project_name)
    project_dir = os.path.join(BASE_PROJECTS_DIR, sanitized_proj_name)
    manifest_filepath = os.path.join(project_dir, "_ai_project_manifest.json")

    # 1. Load ProjectManifest
    manifest_json_str = read_text_from_file(manifest_filepath)
    if manifest_json_str.startswith("Error:"):
        return f"Error: Could not read project manifest for '{project_name}'. {manifest_json_str}"

    try:
        manifest_dict = json.loads(manifest_json_str)
        manifest = ProjectManifest.from_dict(manifest_dict)
    except json.JSONDecodeError as e:
        return f"Error: Failed to parse project manifest for '{project_name}'. Invalid JSON. {e}"
    except Exception as e_manifest:
        return f"Error: Failed to load project manifest data for '{project_name}'. Detail: {e_manifest}"

    # 2. Create a new Dependency object
    new_dependency = Dependency(
        name=dependency_name,
        version=dependency_version,
        type=dependency_type
    )

    # 3. Add it to manifest.dependencies list (avoid duplicates by name, update if version/type changes)
    dependency_updated = False
    dependency_added = False
    existing_dep_index = -1

    for i, existing_dependency in enumerate(manifest.dependencies):
        if existing_dependency.name == new_dependency.name:
            existing_dep_index = i
            break
    
    if existing_dep_index != -1:
        # Dependency with the same name found, check if update is needed
        current_dep = manifest.dependencies[existing_dep_index]
        if (new_dependency.version is not None and current_dep.version != new_dependency.version) or \
           (new_dependency.type is not None and current_dep.type != new_dependency.type) or \
           (new_dependency.version is None and current_dep.version is not None) or \
           (new_dependency.type is None and current_dep.type is not None):
            # Update if version or type is different, or if new value is None and old was not (clearing a value)
            manifest.dependencies[existing_dep_index] = new_dependency
            dependency_updated = True
    else:
        # Dependency not found, add it
        manifest.dependencies.append(new_dependency)
        dependency_added = True

    if not dependency_added and not dependency_updated and existing_dep_index != -1:
        return f"Info: Dependency '{dependency_name}' already exists in project '{project_name}' with the same details. No changes made."

    # 4. Update manifest.last_modified_timestamp
    manifest.last_modified_timestamp = datetime.now(timezone.utc).isoformat()

    # 5. Save ProjectManifest
    try:
        updated_manifest_dict = manifest.to_json_dict()
        save_result = write_text_to_file(manifest_filepath, json.dumps(updated_manifest_dict, indent=4))
        if save_result.startswith("Error:"):
            return f"Error: Failed to save updated manifest for project '{project_name}'. {save_result}"
    except Exception as e_save:
        return f"Error: Unexpected error saving manifest for project '{project_name}'. {e_save}"

    action_taken = "added" if dependency_added else "updated"
    return f"Success: Dependency '{dependency_name}' (version: {dependency_version or 'any'}, type: {dependency_type or 'N/A'}) {action_taken} in project '{project_name}'. Manifest updated."

async def run_project_tests(project_name: str) -> str:
    """
    Runs the tests for the specified project based on 'test_config' in the manifest.

    Args:
        project_name: The name of the project.

    Returns:
        A string containing test results (e.g., stdout/stderr of test runner) or an error message.
    """
    if not project_name or not isinstance(project_name, str) or not project_name.strip():
        return "Error: Project name must be a non-empty string."

    sanitized_proj_name = sanitize_project_name(project_name)
    project_dir = os.path.join(BASE_PROJECTS_DIR, sanitized_proj_name)
    manifest_filepath = os.path.join(project_dir, "_ai_project_manifest.json")

    # 1. Load ProjectManifest
    manifest_json_str = read_text_from_file(manifest_filepath)
    if manifest_json_str.startswith("Error:"):
        return f"Error: Could not read project manifest for '{project_name}'. {manifest_json_str}"

    try:
        manifest_dict = json.loads(manifest_json_str)
        manifest = ProjectManifest.from_dict(manifest_dict)
    except json.JSONDecodeError as e:
        return f"Error: Failed to parse project manifest for '{project_name}'. Invalid JSON. {e}"
    except Exception as e_manifest:
        return f"Error: Failed to load project manifest data for '{project_name}'. Detail: {e_manifest}"

    # 2. Get test_config.test_command
    if not manifest.test_config or not manifest.test_config.test_command:
        return f"Info: No test command configured in the manifest for project '{project_name}'."

    test_command_str = manifest.test_config.test_command
    # Simple command splitting, assuming no complex shell features are needed in the command string itself.
    # For more complex commands, shlex.split might be better.
    test_command_parts = test_command_str.split()

    # 3. Execute it in the project directory
    try:
        logger.info(f"Running test command '{test_command_str}' for project '{project_name}' in directory '{project_dir}'...")
        process_result = subprocess.run(
            test_command_parts,
            cwd=project_dir, # Run from the project's root directory
            capture_output=True,
            text=True,
            timeout=300,  # 5-minute timeout for tests
            check=False   # Do not raise exception for non-zero exit codes
        )

        # 4. Capture and return output
        output_summary = [
            f"Test Results for Project: {project_name}",
            f"Command: {test_command_str}",
            f"Return Code: {process_result.returncode}",
            "--- STDOUT ---",
            process_result.stdout.strip() if process_result.stdout else "(No standard output)",
            "--- STDERR ---",
            process_result.stderr.strip() if process_result.stderr else "(No standard error)"
        ]
        return "\n".join(output_summary)

    except FileNotFoundError:
        return f"Error: Test command '{test_command_parts[0]}' not found. Ensure it's installed and in PATH."
    except subprocess.TimeoutExpired:
        return f"Error: Test command '{test_command_str}' timed out after 300 seconds."
    except Exception as e:
        return f"Error: An unexpected error occurred while running tests for '{project_name}': {e}"

async def build_project(project_name: str) -> str:
    """
    Builds the specified project based on 'build_config' in the manifest.

    Args:
        project_name: The name of the project.

    Returns:
        A string confirming build success or failure, including output or error messages.
    """
    if not project_name or not isinstance(project_name, str) or not project_name.strip():
        return "Error: Project name must be a non-empty string."

    sanitized_proj_name = sanitize_project_name(project_name)
    project_dir = os.path.join(BASE_PROJECTS_DIR, sanitized_proj_name)
    manifest_filepath = os.path.join(project_dir, "_ai_project_manifest.json")

    # 1. Load ProjectManifest
    manifest_json_str = read_text_from_file(manifest_filepath)
    if manifest_json_str.startswith("Error:"):
        return f"Error: Could not read project manifest for '{project_name}'. {manifest_json_str}"

    try:
        manifest_dict = json.loads(manifest_json_str)
        manifest = ProjectManifest.from_dict(manifest_dict)
    except json.JSONDecodeError as e:
        return f"Error: Failed to parse project manifest for '{project_name}'. Invalid JSON. {e}"
    except Exception as e_manifest:
        return f"Error: Failed to load project manifest data for '{project_name}'. Detail: {e_manifest}"

    # 2. Get build_config.build_command
    if not manifest.build_config or not manifest.build_config.build_command:
        return f"Info: No build command configured in the manifest for project '{project_name}'."

    build_command_str = manifest.build_config.build_command
    build_command_parts = build_command_str.split()

    # 3. Execute it in the project directory
    try:
        logger.info(f"Running build command '{build_command_str}' for project '{project_name}' in directory '{project_dir}'...")
        process_result = subprocess.run(
            build_command_parts,
            cwd=project_dir,
            capture_output=True,
            text=True,
            timeout=600,  # 10-minute timeout for builds
            check=False
        )

        # 4. Capture and return output
        output_summary = [
            f"Build Results for Project: {project_name}",
            f"Command: {build_command_str}",
            f"Return Code: {process_result.returncode}",
            "--- STDOUT ---",
            process_result.stdout.strip() if process_result.stdout else "(No standard output)",
            "--- STDERR ---",
            process_result.stderr.strip() if process_result.stderr else "(No standard error)"
        ]
        return "\n".join(output_summary)

    except FileNotFoundError:
        return f"Error: Build command '{build_command_parts[0]}' not found. Ensure it's installed and in PATH."
    except subprocess.TimeoutExpired:
        return f"Error: Build command '{build_command_str}' timed out after 600 seconds."
    except Exception as e:
        return f"Error: An unexpected error occurred while building project '{project_name}': {e}"


if __name__ == '__main__':
    import asyncio
    import shutil # Ensure shutil is imported for cleanup

    # --- Mocking Dependencies (Global for test functions) ---
    # _original_read_text_from_file = None # Already defined if needed, but new mocks are more specific
    _mock_captured_code_gen_prompt = None # Global to capture prompt for code gen tests

    async def _mock_invoke_ollama_code_generator(prompt: str, model_name: str, **kwargs):
        global _mock_captured_code_gen_prompt
        _mock_captured_code_gen_prompt = prompt # Capture the prompt
        # print(f"DEBUG: _mock_invoke_ollama_code_generator called with prompt:\n{prompt}")
        if "app.py" in prompt: # Corresponds to filename in detailed plan
            return "print('Hello from app.py!')"
        elif "util.py" in prompt: # Corresponds to filename in detailed plan
            return "def helper_util_function():\n    return 'Helper util output'"
        return "# Default mock code from _mock_invoke_ollama_code_generator"

    # _mock_read_files = {} # Old mock, replaced by _written_files_for_test for MOCK_FS

    # def mock_fs_read_text_from_file(full_filepath_fs: str) -> str: # Old mock
    #     global _mock_read_files 
    #     print(f"MOCK read_text_from_file from: '{full_filepath_fs}'")
    #     if full_filepath_fs in _mock_read_files:
    #         return _mock_read_files[full_filepath_fs]
    #     if os.path.exists(full_filepath_fs):
    #          try:
    #             with open(full_filepath_fs, 'r') as f_real:
    #                 return f_real.read()
    #          except Exception as e_real:
    #             return f"Error: Mock fs read (real file) failed {e_real}"
    #     return f"Error: File '{full_filepath_fs}' not found."
    
    # --- Mocking Dependencies (Global for test functions) ---
    # _mock_invoke_ollama_responses = {} # Not used with current _mock_invoke_ollama_planner
    _original_invoke_ollama_model_async = None # Will store original invoke_ollama_model_async
    _original_create_project_directory = None
    _original_write_text_to_file = None
    _original_read_text_from_file = None # Added for consistency
    _original_sanitize_project_name = None
    _created_dirs_for_test = []
    _written_files_for_test = {}
    _printed_warnings_for_test = []

    # This will be our test-specific projects directory
    TEST_BASE_PROJECTS_DIR_MAIN = "temp_test_pm_projects"


    async def _mock_invoke_ollama_planner(prompt: str, model_name: str, **kwargs):
        # print(f"DEBUG: _mock_invoke_ollama_planner called with prompt containing: {project_description_marker}")
        # Based on project_description_marker, return a specific response
        # This is a simplified way to control mock response per test based on prompt content
        if "Detailed Plan Test" in prompt:
            return json.dumps({
                "project_plan": [
                    {"filename": "app.py", "description": "Main app.", "key_components": ["comp1"], "dependencies": ["util.py"]},
                    {"filename": "util.py", "description": "Utilities.", "key_components": ["helper"], "dependencies": []}
                ]
            })
        elif "Missing Fields Test" in prompt:
            return json.dumps({
                "project_plan": [
                    {"filename": "core.py", "description": "Core logic."} 
                    # key_components, dependencies missing
                ]
            })
        elif "Wrong Types Test" in prompt:
            return json.dumps({
                "project_plan": [
                    {"filename": "service.py", "description": "Service layer.", "key_components": "not_a_list", "dependencies": 123}
                ]
            })
        elif "Empty Plan Test" in prompt:
             return json.dumps({"project_plan": []})
        elif "Malformed JSON Test" in prompt:
            return "This is not valid JSON { "
        elif "Invalid Structure Test 1" in prompt: # Missing 'project_plan' key
            return json.dumps({"project_files": []})
        elif "Invalid Structure Test 2" in prompt: # 'project_plan' is not a list
            return json.dumps({"project_plan": {"filename": "test.py"}})
        elif "LLM No Response Test" in prompt:
            return None
        # Default for other tests like code gen that might be indirectly called
        return json.dumps({"project_plan": [{"filename": "default.py", "description": "Default."}]})


    def _mock_create_project_directory(project_name: str):
        global _created_dirs_for_test, TEST_BASE_PROJECTS_DIR_MAIN # Use the specific test base dir
        s_name = sanitize_project_name(project_name) # Use the real sanitize
        path = os.path.join(TEST_BASE_PROJECTS_DIR_MAIN, s_name)
        _created_dirs_for_test.append(path)
        os.makedirs(path, exist_ok=True)
        # print(f"MOCK_FS: Created directory {path}")
        return f"Success: Project directory '{path}' created."

    def _mock_write_text_to_file(full_filepath: str, content: str):
        global _written_files_for_test
        _written_files_for_test[full_filepath] = content
        # print(f"MOCK_FS: Wrote to file {full_filepath}")
        # Simulate actual writing for manifest checks
        try:
            os.makedirs(os.path.dirname(full_filepath), exist_ok=True)
            with open(full_filepath, 'w') as f:
                f.write(content)
            return f"Success: Content written to '{full_filepath}'."
        except Exception as e:
            return f"Error: Mock fs write failed {e}"
            
    def _mock_read_text_from_file(full_filepath: str): # Added for generate_code_for_project_file if needed
        if full_filepath in _written_files_for_test:
            return _written_files_for_test[full_filepath]
        # Simulate actual file reading for manifest checks if it was written by _mock_write_text_to_file
        if os.path.exists(full_filepath):
            try:
                with open(full_filepath, 'r') as f_real:
                    return f_real.read()
            except Exception as e_real:
                return f"Error: Mock fs read (real file) failed {e_real}"
        return f"Error: File not found '{full_filepath}'"    # Capture prints for warning checks
    _original_print = __builtins__.print
    def _captured_print(*args, **kwargs):
        global _printed_warnings_for_test
        _original_print(*args, **kwargs) # Keep original print behavior
        _printed_warnings_for_test.append(" ".join(map(str, args)))

    async def run_tests():
        global invoke_ollama_model_async, create_project_directory, write_text_to_file, read_text_from_file, sanitize_project_name, print
        global _original_invoke_ollama_model_async, _original_create_project_directory, _original_write_text_to_file, _original_read_text_from_file, _original_sanitize_project_name
        global BASE_PROJECTS_DIR, _created_dirs_for_test, _written_files_for_test, _printed_warnings_for_test
        
        # Store original functions and BASE_PROJECTS_DIR
        _original_invoke_ollama_model_async = invoke_ollama_model_async
        _original_create_project_directory = create_project_directory
        _original_write_text_to_file = write_text_to_file
        _original_read_text_from_file = read_text_from_file
        _original_sanitize_project_name = sanitize_project_name 
        
        original_base_dir_for_module = BASE_PROJECTS_DIR # Store the module's default
        
        # Apply mocks
        invoke_ollama_model_async = _mock_invoke_ollama_planner
        create_project_directory = _mock_create_project_directory
        write_text_to_file = _mock_write_text_to_file
        read_text_from_file = _mock_read_text_from_file
        # sanitize_project_name is used directly from file_system_tools, so no need to mock if it's correct there
        print_backup = print # Backup original print
        print = _captured_print # Set print to our capturing version

        BASE_PROJECTS_DIR = TEST_BASE_PROJECTS_DIR_MAIN # Override for tests

        # Ensure clean test environment
        if os.path.exists(TEST_BASE_PROJECTS_DIR_MAIN):
            shutil.rmtree(TEST_BASE_PROJECTS_DIR_MAIN)
        os.makedirs(TEST_BASE_PROJECTS_DIR_MAIN, exist_ok=True)

        # --- Test Case 1: Successful detailed plan ---
        _original_print("--- Test 1: Successful Detailed Plan ---")
        _created_dirs_for_test.clear()
        _written_files_for_test.clear()
        _printed_warnings_for_test.clear()
        project_name_1 = "DetailedPlanProject"
        result_1 = await initiate_ai_project(project_name_1, "Detailed Plan Test")
        _original_print(f"Result 1: {result_1}")
        assert "Success" in result_1
        assert "2 file(s) with detailed components and dependencies." in result_1
        manifest_path_1 = os.path.join(TEST_BASE_PROJECTS_DIR_MAIN, sanitize_project_name(project_name_1), "_ai_project_manifest.json")
        assert os.path.exists(manifest_path_1)
        with open(manifest_path_1, 'r') as f:
            manifest_content_1 = json.load(f)
        assert len(manifest_content_1["project_plan"]) == 2
        assert manifest_content_1["project_plan"][0]["filename"] == "app.py"
        assert manifest_content_1["project_plan"][0]["key_components"] == ["comp1"]
        assert manifest_content_1["project_plan"][0]["dependencies"] == ["util.py"]
        assert manifest_content_1["project_plan"][0]["status"] == "planned"
        assert manifest_content_1["project_plan"][1]["filename"] == "util.py"
        assert len(_printed_warnings_for_test) == 0

        # --- Test Case 2: Missing optional fields (key_components, dependencies) ---
        _original_print("\n--- Test 2: Missing Optional Fields ---")
        _created_dirs_for_test.clear()
        _written_files_for_test.clear()
        _printed_warnings_for_test.clear()
        project_name_2 = "MissingFieldsProject"
        result_2 = await initiate_ai_project(project_name_2, "Missing Fields Test")
        _original_print(f"Result 2: {result_2}")
        assert "Success" in result_2
        manifest_path_2 = os.path.join(TEST_BASE_PROJECTS_DIR_MAIN, sanitize_project_name(project_name_2), "_ai_project_manifest.json")
        assert os.path.exists(manifest_path_2)
        with open(manifest_path_2, 'r') as f:
            manifest_content_2 = json.load(f)
        assert len(manifest_content_2["project_plan"]) == 1
        assert manifest_content_2["project_plan"][0]["filename"] == "core.py"
        assert manifest_content_2["project_plan"][0]["key_components"] == [] # Defaulted
        assert manifest_content_2["project_plan"][0]["dependencies"] == [] # Defaulted
        assert manifest_content_2["project_plan"][0]["status"] == "planned"
        assert any("key_components' for file 'core.py' is not present or not a list. Defaulting to empty list." in warn for warn in _printed_warnings_for_test)
        assert any("dependencies' for file 'core.py' is not present or not a list. Defaulting to empty list." in warn for warn in _printed_warnings_for_test)


        # --- Test Case 3: Optional fields have wrong types ---
        _original_print("\n--- Test 3: Wrong Types for Optional Fields ---")
        _created_dirs_for_test.clear()
        _written_files_for_test.clear()
        _printed_warnings_for_test.clear()
        project_name_3 = "WrongTypesProject"
        result_3 = await initiate_ai_project(project_name_3, "Wrong Types Test")
        _original_print(f"Result 3: {result_3}")
        assert "Success" in result_3
        manifest_path_3 = os.path.join(TEST_BASE_PROJECTS_DIR_MAIN, sanitize_project_name(project_name_3), "_ai_project_manifest.json")
        assert os.path.exists(manifest_path_3)
        with open(manifest_path_3, 'r') as f:
            manifest_content_3 = json.load(f)
        assert len(manifest_content_3["project_plan"]) == 1
        assert manifest_content_3["project_plan"][0]["filename"] == "service.py"
        assert manifest_content_3["project_plan"][0]["key_components"] == [] # Defaulted
        assert manifest_content_3["project_plan"][0]["dependencies"] == [] # Defaulted
        assert any("key_components' for file 'service.py' is not a list. Defaulting to empty list." in warn for warn in _printed_warnings_for_test)
        assert any("dependencies' for file 'service.py' is not a list. Defaulting to empty list." in warn for warn in _printed_warnings_for_test)

        # --- Test Case 4: LLM returns empty project plan list ---
        _original_print("\n--- Test 4: LLM Empty Plan List ---")
        _created_dirs_for_test.clear(); _written_files_for_test.clear(); _printed_warnings_for_test.clear()
        project_name_4 = "EmptyPlanProject"
        result_4 = await initiate_ai_project(project_name_4, "Empty Plan Test")
        _original_print(f"Result 4: {result_4}")
        assert "Success" in result_4
        assert "empty plan" in result_4 # Check success message
        manifest_path_4 = os.path.join(TEST_BASE_PROJECTS_DIR_MAIN, sanitize_project_name(project_name_4), "_ai_project_manifest.json")
        assert os.path.exists(manifest_path_4)
        with open(manifest_path_4, 'r') as f:
            manifest_content_4 = json.load(f)
        assert manifest_content_4["project_plan"] == []
        assert any("LLM returned an empty project plan" in warn for warn in _printed_warnings_for_test)


        # --- Test Case 5: LLM returns malformed JSON ---
        _original_print("\n--- Test 5: LLM Malformed JSON ---")
        result_5 = await initiate_ai_project("MalformedJSONProject", "Malformed JSON Test")
        _original_print(f"Result 5: {result_5}")
        assert "Error: LLM returned invalid JSON for project plan." in result_5

        # --- Test Case 6: LLM returns invalid plan structure (missing 'project_plan' key) ---
        _original_print("\n--- Test 6: LLM Invalid Structure 1 ---")
        result_6 = await initiate_ai_project("InvalidStruct1Project", "Invalid Structure Test 1")
        _original_print(f"Result 6: {result_6}")
        assert "Error: LLM returned an invalid plan structure (missing 'project_plan' list)." in result_6

        # --- Test Case 7: LLM returns invalid plan structure ('project_plan' not a list) ---
        _original_print("\n--- Test 7: LLM Invalid Structure 2 ---")
        result_7 = await initiate_ai_project("InvalidStruct2Project", "Invalid Structure Test 2")
        _original_print(f"Result 7: {result_7}")
        assert "Error: LLM returned an invalid plan structure (missing 'project_plan' list)." in result_7
        
        # --- Test Case 8: LLM returns no response ---
        _original_print("\n--- Test 8: LLM No Response ---")
        result_8 = await initiate_ai_project("NoResponseProject", "LLM No Response Test")
        _original_print(f"Result 8: {result_8}")
        assert "Error: Failed to get project plan from LLM." in result_8

        _original_print("\n--- All initiate_ai_project tests seem to have passed (check warnings) ---")

        # Restore original functions and BASE_PROJECTS_DIR
        invoke_ollama_model_async = _original_invoke_ollama_model_async
        create_project_directory = _original_create_project_directory
        write_text_to_file = _original_write_text_to_file
        read_text_from_file = _original_read_text_from_file
        sanitize_project_name = _original_sanitize_project_name
        # Restore global print to the _original_print captured at the start of run_tests' outer scope
        # which is __builtins__.print.
        print = _original_print
        BASE_PROJECTS_DIR = original_base_dir_for_module

        # Cleanup
        if os.path.exists(TEST_BASE_PROJECTS_DIR_MAIN):
            shutil.rmtree(TEST_BASE_PROJECTS_DIR_MAIN)
            _original_print(f"Cleaned up test directory: {TEST_BASE_PROJECTS_DIR_MAIN}")

        # --- Original tests for generate_code_for_project_file and execute_project_coding_plan ---
        # These tests are from the original file and might need adjustment if they depend on
        # the old plan structure or specific mock behaviors not replicated here.
        # For this subtask, we are primarily focused on initiate_ai_project.
        # I will keep them for now but comment out the parts that might fail due to
        # the new plan structure not being accounted for in their specific mocks or setup.

        _original_print("\n--- Running Original Prerequisite Tests for Code Gen (may need adjustments) ---")
        # Ensure print is the original built-in for any direct calls in __main__ after run_tests
        print = _original_print

        # Re-setup mocks for these older tests if they use different mock LLM logic
        # For simplicity, we'll reuse the planner mock, which might not be ideal.
        invoke_ollama_model_async = _mock_invoke_ollama_planner # This will use the detailed plan mock

        # --- Original tests for generate_code_for_project_file and execute_project_coding_plan ---
        _original_print("\n--- Running Updated Tests for generate_code_for_project_file ---")
        # Ensure print is the original built-in
        print = _original_print

        
        # Setup a project specifically for these tests using the detailed planner mock
        # This ensures the manifest has 'key_components' and 'dependencies'
        invoke_ollama_model_async = _mock_invoke_ollama_planner 
        
        project_name_cg_test = "CodeGenDetailedProject"
        project_desc_cg_test = "Detailed Plan Test for CodeGen" # Triggers detailed plan from _mock_invoke_ollama_planner
        
        # Clear previous test artifacts if any for this specific project name
        sanitized_cg_proj_name = sanitize_project_name(project_name_cg_test)
        cg_project_dir_path = os.path.join(TEST_BASE_PROJECTS_DIR_MAIN, sanitized_cg_proj_name)
        if os.path.exists(cg_project_dir_path):
            shutil.rmtree(cg_project_dir_path)
        
        # Initialize the project - this will use _mock_invoke_ollama_planner
        init_cg_result = await initiate_ai_project(project_name_cg_test, project_desc_cg_test)
        assert "Success" in init_cg_result
        manifest_path_cg = os.path.join(cg_project_dir_path, "_ai_project_manifest.json")
        assert os.path.exists(manifest_path_cg)

        # Now, switch LLM mock to the code generator that captures prompts
        invoke_ollama_model_async = _mock_invoke_ollama_code_generator
        global _mock_captured_code_gen_prompt # Ensure it's accessible
        _mock_captured_code_gen_prompt = None


        # Test CG 1: Successful code generation with detailed plan info in prompt
        _original_print("\n--- Test CG 1: Successful generation & Prompt Verification ---")
        # Ensure print is the original built-in
        print = _original_print
        filename_app_py = "app.py" # This file is in the detailed plan from the mock
        
        cg_result1 = await generate_code_for_project_file(project_name_cg_test, filename_app_py)
        _original_print(f"Code Gen Result 1: {cg_result1}")
        assert f"Success: Code for '{filename_app_py}' generated" in cg_result1
        assert _mock_captured_code_gen_prompt is not None
        assert "Overall Project Description:\nDetailed Plan Test for CodeGen" in _mock_captured_code_gen_prompt
        assert f"Current File to Generate: {filename_app_py}" in _mock_captured_code_gen_prompt
        assert "Purpose of this file (from project plan): Main app." in _mock_captured_code_gen_prompt
        assert "Key Components for this file:\n- comp1" in _mock_captured_code_gen_prompt
        assert "Dependencies for this file (other files in this project):\nutil.py" in _mock_captured_code_gen_prompt
        
        # Verify code file content
        expected_code_path_app = os.path.join(cg_project_dir_path, filename_app_py)
        assert os.path.exists(expected_code_path_app)
        with open(expected_code_path_app, 'r') as f:
            assert "print('Hello from app.py!')" in f.read()
        
        # Verify manifest update for this file
        with open(manifest_path_cg, 'r') as f_m_cg_updated:
            manifest_data_after_cg1 = json.load(f_m_cg_updated)
        app_py_entry = next((item for item in manifest_data_after_cg1["project_plan"] if item["filename"] == filename_app_py), None)
        assert app_py_entry is not None
        assert app_py_entry["status"] == "generated"
        assert "last_code_generation_timestamp" in app_py_entry

        # Test CG 2: File not in plan (should still work as before)
        _original_print("\n--- Test CG 2: File not in plan ---")
        # Ensure print is the original built-in
        print = _original_print
        _mock_captured_code_gen_prompt = None
        cg_result2 = await generate_code_for_project_file(project_name_cg_test, "non_existent_file.py")
        _original_print(f"Code Gen Result 2: {cg_result2}")
        assert "Error: File 'non_existent_file.py' not found" in cg_result2
        assert _mock_captured_code_gen_prompt is None # LLM should not have been called

        # Test CG 3: File already generated (should still work as before)
        _original_print("\n--- Test CG 3: File already generated ---")
        # Ensure print is the original built-in
        print = _original_print
        _mock_captured_code_gen_prompt = None
        cg_result3 = await generate_code_for_project_file(project_name_cg_test, filename_app_py) # app.py was generated in Test CG 1
        _original_print(f"Code Gen Result 3: {cg_result3}")
        assert f"Info: Code for '{filename_app_py}' in project '{project_name_cg_test}' has already been generated" in cg_result3
        assert _mock_captured_code_gen_prompt is None

        # Test CG 4: LLM fails to generate code (e.g., returns empty)
        _original_print("\n--- Test CG 4: LLM fails to generate code ---")
        # Ensure print is the original built-in
        print = _original_print
        filename_util_py = "util.py" # This file is in the plan and should be 'planned'
        # Temporarily make the mock return empty for this specific file
        original_code_gen_mock = invoke_ollama_model_async 
        async def mock_empty_for_util(prompt: str, model_name: str, **kwargs):
            global _mock_captured_code_gen_prompt
            _mock_captured_code_gen_prompt = prompt
            if filename_util_py in prompt:
                return "" # Empty response
            return await original_code_gen_mock(prompt, model_name, **kwargs) # Call original mock for others
        invoke_ollama_model_async = mock_empty_for_util
        _mock_captured_code_gen_prompt = None

        cg_result4 = await generate_code_for_project_file(project_name_cg_test, filename_util_py)
        _original_print(f"Code Gen Result 4: {cg_result4}")
        assert f"Error: LLM failed to generate code for '{filename_util_py}'" in cg_result4
        assert _mock_captured_code_gen_prompt is not None # Prompt should have been made
        # Check that the util.py status is still "planned"
        with open(manifest_path_cg, 'r') as f_m_cg_after_fail:
            manifest_data_after_cg4 = json.load(f_m_cg_after_fail)
        util_py_entry = next((item for item in manifest_data_after_cg4["project_plan"] if item["filename"] == filename_util_py), None)
        assert util_py_entry is not None
        assert util_py_entry["status"] == "planned"

        invoke_ollama_model_async = original_code_gen_mock # Restore the main code gen mock

        _original_print("\n--- End of generate_code_for_project_file tests ---")
        # Ensure print is the original built-in
        print = _original_print

        # Restore original functions and BASE_PROJECTS_DIR fully at the end
        invoke_ollama_model_async = _original_invoke_ollama_model_async # Restore original planner/codegen mock
        create_project_directory = _original_create_project_directory
        write_text_to_file = _original_write_text_to_file
        read_text_from_file = _original_read_text_from_file
        sanitize_project_name = _original_sanitize_project_name
        print = _original_print # Ensure it's restored to the true built-in
        BASE_PROJECTS_DIR = original_base_dir_for_module

        # Final cleanup
        if os.path.exists(TEST_BASE_PROJECTS_DIR_MAIN):
            shutil.rmtree(TEST_BASE_PROJECTS_DIR_MAIN)
            _original_print(f"Cleaned up test directory: {TEST_BASE_PROJECTS_DIR_MAIN}")

    # Explicitly restore print to __builtins__.print after asyncio.run completes,
    # to ensure Pylance is satisfied for any subsequent print calls in __main__.
    asyncio.run(run_tests())
    print = __builtins__.print

    if __name__ == '__main__':
        # This __main__ block now includes tests for initiate_ai_project, 
        # generate_code_for_project_file, and execute_project_coding_plan.

        # Define mock_invoke_ollama_good_plan and other LLM mocks if they are not already defined at this scope
        # For simplicity, we assume they are defined as in the previous version of the __main__ block.
        # If not, they would need to be defined here or imported if refactored into a test utility module.
        
        # Example mock LLM responses (ensure these are defined before run_tests if not already)
        async def mock_invoke_ollama_good_plan(prompt: str, model_name: str):
            # print(f"MOCK LLM call for: {model_name} (Good Plan - Detailed)")
            return json.dumps({
                "project_plan": [
                    {
                        "filename": "main.py", 
                        "description": "Main application script.",
                        "key_components": ["app_setup", "routes", "main_logic"],
                        "dependencies": ["utils.py", "models.py"]
                    },
                    {
                        "filename": "utils.py", 
                        "description": "Utility functions.",
                        "key_components": ["helper_function_1", "data_parser"],
                        "dependencies": []
                    },
                    {
                        "filename": "models.py", 
                        "description": "Data models/schemas.",
                        "key_components": ["UserSchema", "ProductSchema"],
                        "dependencies": []
                    }
                ]
            })

        async def mock_invoke_ollama_good_plan_missing_fields(prompt: str, model_name: str):
            # Test lenient validation: key_components and dependencies missing
            return json.dumps({
                "project_plan": [
                    {
                        "filename": "core_logic.py", 
                        "description": "Core business logic without explicit components/deps listed."
                        # key_components and dependencies are missing
                    }
                ]
            })

        async def mock_invoke_ollama_good_plan_wrong_types(prompt: str, model_name: str):
            # Test lenient validation: key_components and dependencies are not lists
            return json.dumps({
                "project_plan": [
                    {
                        "filename": "service.py", 
                        "description": "Service layer.",
                        "key_components": "should_be_list", # Incorrect type
                        "dependencies": "should_also_be_list" # Incorrect type
                    }
                ]
            })
        
        async def mock_invoke_ollama_empty_plan(prompt: str, model_name: str): 
            # print(f"MOCK LLM call for: {model_name} (Empty Plan)")
            return json.dumps({"project_plan": []})
        
        async def mock_invoke_ollama_malformed_json(prompt: str, model_name: str): 
            # print(f"MOCK LLM call for: {model_name} (Malformed JSON)")
            return "This is not JSON { definitely not json"
            
        async def mock_invoke_ollama_invalid_plan_structure1(prompt: str, model_name: str):
            # print(f"MOCK LLM call for: {model_name} (Invalid Plan Structure 1)")
            return json.dumps({"project_files": []}) # Missing "project_plan" key

        async def mock_invoke_ollama_invalid_plan_structure2(prompt: str, model_name: str):
            # print(f"MOCK LLM call for: {model_name} (Invalid Plan Structure 2)")
            return json.dumps({"project_plan": {"filename": "main.py"}}) # "project_plan" is not a list
            
        async def mock_invoke_ollama_no_response(prompt: str, model_name: str): 
            # print(f"MOCK LLM call for: {model_name} (No Response)")
            return None

        TEST_BASE_PROJECTS_DIR = "temp_test_ai_projects" 

        asyncio.run(run_tests())


# --- Code Review Tool ---
# Ensure this import is at the top level (column 1)
from ai_assistant.core.reviewer import ReviewerAgent 

async def request_code_review_tool(
    code_to_review: str,
    original_requirements: str,
    related_tests: Optional[str] = None
) -> Dict[str, Any]:
    """
    Requests a review for the provided code against original requirements and related tests.

    Args:
        code_to_review: The actual code content (string) to be reviewed.
        original_requirements: The description of what the code was supposed to do.
        related_tests: (Optional) String representation of tests related to this code.

    Returns:
        A dictionary containing the review results (status, comments, suggestions).
    """
    if not code_to_review or not original_requirements:
        return {
            "status": "error",
            "comments": "Error: Code to review and original requirements must be provided.",
            "suggestions": ""
        }

    reviewer_agent = ReviewerAgent()
    review_results = await reviewer_agent.review_code(
        code_to_review=code_to_review,
        original_requirements=original_requirements,
        related_tests=related_tests
    )
    return review_results

# ### END FILE: ai_assistant/custom_tools/project_management_tools.py ###

# ### START FILE: ai_assistant/custom_tools/search_tools.py ###
# ai_assistant/custom_tools/search_tools.py
import json
from googleapiclient.discovery import build
from ai_assistant.config import GOOGLE_API_KEY, GOOGLE_CSE_ID
from typing import List, Dict, Any, Optional

def google_custom_search(query: str, num_results: int = 5) -> Optional[List[Dict[str, Any]]]:
    """
    Your primary tool for finding information online using Google Search. Use this for questions
    like 'what is X?', 'who is Y?', 'explain Z', 'search for A', 'find information on B',
    or when you need current, up-to-date facts and details.
    Args:
        query (str): The search query.
        num_results (int): The number of search results to return (default is 5, max is 10).
    Returns:
        Optional[List[Dict[str, Any]]]: A list of search results, or None if an error occurs or keys are missing.
    """
    if not GOOGLE_API_KEY or not GOOGLE_CSE_ID:
        print("Error: Google API Key or CSE ID is not configured.")
        return None
    try:
        service = build("customsearch", "v1", developerKey=GOOGLE_API_KEY)
        # Clamp num_results between 1 and 10 (API limit per request)
        num_results = max(1, min(num_results, 10))
        
        res = service.cse().list(q=query, cx=GOOGLE_CSE_ID, num=num_results).execute()
        
        search_results = []
        if 'items' in res:
            for item in res['items']:
                search_results.append({
                    "title": item.get("title"),
                    "link": item.get("link"),
                    "snippet": item.get("snippet")
                })
            return search_results
        else:
            return [] # No items found
            
    except Exception as e:
        print(f"An error occurred during Google Custom Search: {e}")
        # Potentially log the error in more detail
        return None
# ### END FILE: ai_assistant/custom_tools/search_tools.py ###

# ### START FILE: ai_assistant/debugging/__init__.py ###
# This file marks debugging as a package.

# ### END FILE: ai_assistant/debugging/__init__.py ###

# ### START FILE: ai_assistant/debugging/logger.py ###
# Code for logging capabilities.
pass

# ### END FILE: ai_assistant/debugging/logger.py ###

# ### START FILE: ai_assistant/debugging/resilience.py ###
import asyncio
import functools
import logging
import random
import time
import traceback
import json
from typing import Optional, Dict, Any, Callable, TypeVar, Coroutine

# Configure basic logging if not already configured by the application's entry point
# This is a basic configuration; a real application might configure logging more centrally.
if not logging.getLogger().handlers:
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(name)s - %(message)s')

logger = logging.getLogger(__name__)

# Type variables for generic decorators
T = TypeVar('T')
CallableT = TypeVar('CallableT', bound=Callable[..., Any])
R = TypeVar('R') # Return type for async coroutine

def retry_with_backoff(
    retries: int = 3,
    base_delay: float = 1.0,
    max_delay: float = 10.0,
    jitter: bool = True
) -> Callable[[CallableT], CallableT]:
    """
    A decorator to retry a function with exponential backoff.

    Args:
        retries: Maximum number of retries.
        base_delay: Initial delay in seconds.
        max_delay: Maximum delay in seconds.
        jitter: Whether to add random jitter to the delay.
    """
    def decorator(func: CallableT) -> CallableT:
        if asyncio.iscoroutinefunction(func):
            @functools.wraps(func)
            async def async_wrapper(*args: Any, **kwargs: Any) -> Any: # Using Any for return type with Coroutine
                last_exception: Optional[Exception] = None
                for attempt in range(retries + 1): # retries=3 means 1 initial call + 3 retries = 4 total attempts
                    try:
                        return await func(*args, **kwargs)
                    except Exception as e:
                        last_exception = e
                        if attempt == retries:
                            logger.error(
                                f"Async function {func.__name__} failed after {attempt + 1} attempts. Re-raising last exception: {type(e).__name__}: {e}"
                            )
                            raise
                        
                        delay = min(max_delay, base_delay * (2 ** attempt))
                        if jitter:
                            delay += random.uniform(0, base_delay / 2.0) # Corrected to float division
                        
                        logger.warning(
                            f"Async function {func.__name__} failed (attempt {attempt + 1}/{retries + 1}). "
                            f"Retrying in {delay:.2f} seconds. Error: {type(e).__name__}: {e}"
                        )
                        await asyncio.sleep(delay)
                # This part should not be reached if logic is correct
                if last_exception: # Should have been re-raised
                    raise last_exception 
                return None # Should be unreachable
            return async_wrapper # type: ignore
        else:
            @functools.wraps(func)
            def sync_wrapper(*args: Any, **kwargs: Any) -> Any:
                last_exception: Optional[Exception] = None
                for attempt in range(retries + 1): # retries=3 means 1 initial call + 3 retries = 4 total attempts
                    try:
                        return func(*args, **kwargs)
                    except Exception as e:
                        last_exception = e
                        if attempt == retries:
                            logger.error(
                                f"Sync function {func.__name__} failed after {attempt + 1} attempts. Re-raising last exception: {type(e).__name__}: {e}"
                            )
                            raise
                        
                        delay = min(max_delay, base_delay * (2 ** attempt))
                        if jitter:
                            delay += random.uniform(0, base_delay / 2.0) # Corrected to float division
                            
                        logger.warning(
                            f"Sync function {func.__name__} failed (attempt {attempt + 1}/{retries + 1}). "
                            f"Retrying in {delay:.2f} seconds. Error: {type(e).__name__}: {e}"
                        )
                        time.sleep(delay)
                # This part should not be reached if logic is correct
                if last_exception: # Should have been re-raised
                    raise last_exception
                return None # Should be unreachable
            return sync_wrapper # type: ignore
    return decorator # type: ignore

def log_critical_error(
    exception: Exception,
    message: str,
    context_info: Optional[Dict[str, Any]] = None
) -> None:
    """
    Logs a critical error with detailed information.

    Args:
        exception: The exception object.
        message: A custom message describing the error context.
        context_info: Optional dictionary containing contextual information.
    """
    tb_str = "".join(traceback.format_exception(type(exception), exception, exception.__traceback__))
    
    log_message = f"CRITICAL ERROR: {message}\n"
    log_message += f"Exception Type: {type(exception).__name__}\n"
    log_message += f"Exception Message: {str(exception)}\n"
    
    if context_info:
        try:
            # Use default=str to handle common non-serializable types like datetime
            context_str = json.dumps(context_info, indent=2, default=str)
            log_message += f"Context Info:\n{context_str}\n"
        except TypeError as te: # Catch specific error if json.dumps fails with default=str
            logger.error(f"Could not serialize context_info to JSON for critical error logging. Error: {te}")
            log_message += f"Context Info (raw, serialization failed):\n{str(context_info)}\n"
            
    log_message += f"Traceback:\n{tb_str}"
    
    # Using CRITICAL level for this function as per its name
    logger.critical(log_message)

if __name__ == '__main__':
    # Example Usage of retry_with_backoff

    # --- Synchronous Example ---
    fail_sync_attempts_global = 0
    @retry_with_backoff(retries=3, base_delay=0.1, max_delay=0.5, jitter=True)
    def might_fail_sync(fail_times: int) -> str:
        global fail_sync_attempts_global
        if fail_sync_attempts_global < fail_times:
            fail_sync_attempts_global += 1
            logger.info(f"might_fail_sync: attempt count {fail_sync_attempts_global}, going to raise error")
            raise ValueError(f"Simulated sync failure on attempt count {fail_sync_attempts_global}")
        return f"Successfully executed might_fail_sync after {fail_sync_attempts_global} initial failures."

    # --- Asynchronous Example ---
    fail_async_attempts_global = 0
    @retry_with_backoff(retries=3, base_delay=0.1, max_delay=0.5, jitter=True)
    async def might_fail_async(fail_times: int) -> str:
        global fail_async_attempts_global
        if fail_async_attempts_global < fail_times:
            fail_async_attempts_global += 1
            logger.info(f"might_fail_async: attempt count {fail_async_attempts_global}, going to raise error")
            raise ValueError(f"Simulated async failure on attempt count {fail_async_attempts_global}")
        return f"Successfully executed might_fail_async after {fail_async_attempts_global} initial failures."

    async def main_examples():
        print("\n--- Testing synchronous retry ---")
        # Test 1: Succeeds on 3rd attempt (2 failures)
        global fail_sync_attempts_global
        fail_sync_attempts_global = 0
        try:
            print(f"Result: {might_fail_sync(2)}") # Should succeed after 2 failures
        except ValueError as e:
            print(f"Caught unexpected error: {e}")

        # Test 2: Fails after all retries (4 failures)
        fail_sync_attempts_global = 0
        try:
            print(might_fail_sync(4)) # Should fail after 3 retries (4 total attempts)
        except ValueError as e:
            print(f"Caught expected error after retries: {e}")

        print("\n--- Testing asynchronous retry ---")
        # Test 3: Async succeeds on 3rd attempt (2 failures)
        global fail_async_attempts_global
        fail_async_attempts_global = 0
        try:
            print(f"Result: {await might_fail_async(2)}")
        except ValueError as e:
            print(f"Caught unexpected error during async test: {e}")

        # Test 4: Async fails after all retries (4 failures)
        fail_async_attempts_global = 0
        try:
            print(await might_fail_async(4))
        except ValueError as e:
            print(f"Caught expected error after async retries: {e}")

        print("\n--- Testing log_critical_error ---")
        try:
            x = 1 / 0
        except ZeroDivisionError as e:
            log_critical_error(
                e,
                "A test critical division error occurred.",
                context_info={"user_id": "test_user", "action": "divide_by_zero_test", "data": {"numerator": 1, "denominator": 0}}
            )
        
        class NonSerializableForTest:
            def __repr__(self):
                return "<NonSerializableForTest object>"

        try:
            raise TypeError("This is a test type error with non-serializable context.")
        except TypeError as e:
            log_critical_error(
                e,
                "Testing non-serializable context in log_critical_error.",
                context_info={"object": NonSerializableForTest(), "status": "problematic_serialization"}
            )

    if __name__ == '__main__':
        asyncio.run(main_examples())

# ### END FILE: ai_assistant/debugging/resilience.py ###

# ### START FILE: ai_assistant/execution/__init__.py ###
# This file makes the 'execution' directory a Python package.

# ### END FILE: ai_assistant/execution/__init__.py ###

# ### START FILE: ai_assistant/execution/action_executor.py ###
# ai_assistant/execution/action_executor.py
from typing import Dict, Any, Optional, Tuple, List, TYPE_CHECKING
import datetime 
import re
import asyncio
import os
import uuid
import logging

from ai_assistant.config import is_debug_mode
from ai_assistant.core import self_modification
from ..core.reflection import global_reflection_log, ReflectionLogEntry  # Add ReflectionLogEntry to import
from ai_assistant.memory.persistent_memory import load_learned_facts, save_learned_facts, LEARNED_FACTS_FILEPATH
# Removed: invoke_ollama_model_async, get_model_for_task, LLM_CODE_FIX_PROMPT_TEMPLATE
from ai_assistant.planning.planning import PlannerAgent
from ai_assistant.tools.tool_system import tool_system_instance
from ai_assistant.code_services.service import CodeService # Added

if TYPE_CHECKING:
    from ai_assistant.learning.learning import LearningAgent # For type hinting only

logger = logging.getLogger(__name__)

class ActionExecutor:
    """
    Responsible for taking proposed actions (derived from ActionableInsights)
    and attempting to execute them.
    """
    def __init__(self, learning_agent: "LearningAgent"): # Use string for forward reference
        """
        Initializes the ActionExecutor.
        """
        if is_debug_mode():
            print("[DEBUG] Initializing ActionExecutor...")
        print("ActionExecutor initialized.")
        # Import necessary modules for dependency injection
        from ai_assistant.llm_interface import ollama_client as default_llm_provider
        from ai_assistant.core import self_modification as default_self_modification_service

        self.code_service = CodeService(
            llm_provider=default_llm_provider,
            self_modification_service=default_self_modification_service
        )
        self.learning_agent = learning_agent
        if is_debug_mode():
            print(f"[DEBUG] CodeService instance in ActionExecutor: {self.code_service}")

    def _find_original_reflection_entry(self, entry_id: str) -> Optional[ReflectionLogEntry]:
        """
        Finds the original ReflectionLogEntry based on its unique entry_id.
        """
        for entry in reversed(global_reflection_log.log_entries):
            if entry.entry_id == entry_id:
                return entry
        print(f"ActionExecutor: Warning - Could not find original reflection entry with ID '{entry_id}'.")
        return None

    async def _run_post_modification_test(self, source_insight_id: Optional[str],
                                          original_reflection_entry_id: Optional[str],
                                          modified_tool_name: str) -> Tuple[Optional[bool], str]:
        if not original_reflection_entry_id:
            return None, "Post-modification test skipped: No original_reflection_entry_id provided."

        original_entry = self._find_original_reflection_entry(original_reflection_entry_id)

        if not original_entry:
            return None, f"Post-modification test skipped: Could not find original reflection entry for ID '{original_reflection_entry_id}'."

        if not original_entry.plan:
            return None, f"Post-modification test skipped: Original reflection entry '{original_reflection_entry_id}' had no plan to re-execute."

        print(f"ActionExecutor: Running post-modification test for tool '{modified_tool_name}'. Re-executing plan from original entry ID '{original_reflection_entry_id}'.")

        from ai_assistant.planning.execution import ExecutionAgent # Moved import here
        executor = ExecutionAgent()
        test_goal_description = f"POST_MOD_TEST for insight {source_insight_id} (orig_goal: {original_entry.goal_description[:50]})"

        try:
            # Create a temporary planner instance for executing the test
            temp_planner = PlannerAgent()
            test_results = await executor.execute_plan(
                goal_description=test_goal_description,
                initial_plan=original_entry.plan,
                tool_system=tool_system_instance,
                planner_agent=temp_planner,
                learning_agent=self.learning_agent # Pass the learning_agent instance
            )
            test_passed = not any(isinstance(res, Exception) for res in test_results)
            notes = f"Re-ran original plan. Test passed: {test_passed}. Results: {str(test_results)[:200]}..."
            
            if not test_passed:
                current_run_errors = [type(res).__name__ for res in test_results if isinstance(res, Exception)]
                if current_run_errors:
                    notes += f" Errors occurred: {current_run_errors}."
                else: # pragma: no cover
                    notes += " Test failed but could not identify specific error types in results."
            print(f"ActionExecutor: Post-modification test result for {modified_tool_name}: {'PASSED' if test_passed else 'FAILED' if test_passed is False else 'SKIPPED'}. Notes: {notes}")
            return test_passed, notes
        except Exception as e: # pragma: no cover
            error_msg = f"Exception during post-modification test execution for {modified_tool_name}: {type(e).__name__} - {e}"
            print(f"ActionExecutor: {error_msg}")
            return False, error_msg

    async def _apply_test_and_revert_code(
        self,
        module_path: str,
        function_name: str,
        code_to_apply: str,
        original_description: str,
        source_insight_id: str,
        action_type: str,
        details_for_logging: Dict[str, Any]
    ) -> bool:
        tool_name = details_for_logging.get("tool_name", function_name)
        source_of_code = details_for_logging.get("source_of_code", "Unknown")
        original_reflection_id_for_test = details_for_logging.get("original_reflection_entry_id")
        log_notes_prefix = f"Action for insight {source_insight_id} ({source_of_code}): "

        modification_type_ast = "MODIFY_TOOL_CODE_LLM_AST" if source_of_code == "CodeService_LLM" else "MODIFY_TOOL_CODE_AST"
        modification_type_exception = "MODIFY_TOOL_CODE_LLM_AST_EXCEPTION" if source_of_code == "CodeService_LLM" else "MODIFY_TOOL_CODE_AST_EXCEPTION"

        # Determine project_root_path for self_modification calls
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..")) # Relative to this file's location

        try:
            modification_result_msg = self_modification.edit_function_source_code(
                module_path=module_path,
                function_name=function_name,
                new_code_string=code_to_apply,
                project_root_path=project_root
            )
            edit_success = "success" in modification_result_msg.lower()
            print(f"ActionExecutor: Code modification result for {tool_name} (from {source_of_code}): {modification_result_msg}")

            test_passed_status: Optional[bool] = None
            test_run_notes: str = "Test not run."
            reversion_successful: Optional[bool] = None
            reversion_notes: str = ""

            if edit_success:
                if not original_reflection_id_for_test:
                    print(f"ActionExecutor: Warning - 'original_reflection_entry_id' not found in details for insight {source_insight_id}. Post-modification testing will be skipped.")
                    test_run_notes = "Post-modification test skipped: original_reflection_entry_id not provided."
                else:
                    test_passed_status, test_run_notes = await self._run_post_modification_test(
                        source_insight_id=source_insight_id,
                        original_reflection_entry_id=original_reflection_id_for_test,
                        modified_tool_name=tool_name
                    )

                if test_passed_status is False:
                    print(f"ActionExecutor: Post-modification test failed for {tool_name}. Attempting to revert.")
                    original_code_from_backup = self_modification.get_backup_function_source_code(module_path, function_name)
                    if original_code_from_backup:
                        try:
                            revert_msg = self_modification.edit_function_source_code(
                                module_path, function_name, 
                                original_code_from_backup,
                                project_root_path=project_root
                            )
                            reversion_successful = "success" in revert_msg.lower()
                            reversion_notes = f"Reverted: {reversion_successful}. Msg: {revert_msg}"
                        except Exception as e_revert: # pragma: no cover
                            reversion_successful = False
                            reversion_notes = f"Exception during revert: {e_revert}"
                        print(f"ActionExecutor: {reversion_notes}")
                    else: # pragma: no cover
                        reversion_successful = False
                        reversion_notes = f"Could not get backup code for {function_name}. Reversion not attempted."
                        print(f"ActionExecutor: {reversion_notes}")
                    test_run_notes += f" | Reversion attempted: {reversion_successful is not None}. Notes: {reversion_notes}"
            else:
                test_run_notes = "Test not run as code edit failed."

            final_overall_success = edit_success and (test_passed_status is True)
            try:
                global_reflection_log.log_execution(
                    goal_description=f"Self-modification ({source_of_code}) for insight {source_insight_id}",
                    plan=[{"action_type": action_type, "details": details_for_logging}],
                    execution_results=[modification_result_msg], overall_success=final_overall_success,
                    notes=log_notes_prefix + f"Applied code for {tool_name}. Edit success: {edit_success}. Test notes: {test_run_notes}",
                    is_self_modification_attempt=True, source_suggestion_id=source_insight_id,
                    modification_type=modification_type_ast,
                    modification_details={
                        "module_path": module_path, "function_name": function_name,
                        "applied_code_change_preview": code_to_apply[:200] + "..." if code_to_apply else "N/A",
                        "reason": original_description, "source_of_code": source_of_code,
                        "reversion_attempted": reversion_successful is not None,
                        "reversion_successful": reversion_successful
                    },
                    post_modification_test_passed=test_passed_status,
                    post_modification_test_details={"notes": test_run_notes}
                )
                return final_overall_success

            except Exception as e_main_apply: # pragma: no cover
                error_message = f"Exception during code application process for {tool_name} (from {source_of_code}): {e_main_apply}"
                print(f"ActionExecutor: {error_message}")
                global_reflection_log.log_execution(
                    goal_description=f"Self-modification ({source_of_code}) for insight {source_insight_id}",
                    plan=[{"action_type": action_type, "details": details_for_logging}],
                    execution_results=[error_message], overall_success=False,
                    notes=f"Exception for {tool_name} from {source_of_code}: {e_main_apply}",
                    is_self_modification_attempt=True, source_suggestion_id=source_insight_id,
                    modification_type=modification_type_exception,
                    modification_details={"module_path": module_path, "function_name": function_name, "reason": original_description, "source_of_code": source_of_code},
                    post_modification_test_passed=None,
                    first_error_type=type(e_main_apply).__name__, first_error_message=str(e_main_apply),
                    post_modification_test_details={"notes": "Test not run due to apply failure."}
                )
                return False

        except Exception as e_main_apply: # pragma: no cover
            error_message = f"Exception during code application process for {tool_name} (from {source_of_code}): {e_main_apply}"
            print(f"ActionExecutor: {error_message}")
            global_reflection_log.log_execution(
                goal_description=f"Self-modification ({source_of_code}) for insight {source_insight_id}",
                plan=[{"action_type": action_type, "details": details_for_logging}],
                execution_results=[error_message], overall_success=False,
                notes=f"Exception for {tool_name} from {source_of_code}: {e_main_apply}",
                is_self_modification_attempt=True, source_suggestion_id=source_insight_id,
                modification_type=modification_type_exception,
                modification_details={"module_path": module_path, "function_name": function_name, "reason": original_description, "source_of_code": source_of_code},
                post_modification_test_passed=None, 
                post_modification_test_details={"notes": "Test not run due to apply failure."}
            )
            return False

    async def execute_action(self, proposed_action: Dict[str, Any]) -> bool:
        action_type = proposed_action.get("action_type")
        details = proposed_action.get("details", {})
        source_insight_id = proposed_action.get("source_insight_id")
        log_notes_prefix = f"Action for insight {source_insight_id}: "

        print(f"ActionExecutor: Received action '{action_type}' for insight '{source_insight_id}'. Details: {details}")

        if action_type == "PROPOSE_TOOL_MODIFICATION":
            tool_name = details.get("tool_name")
            suggested_code = details.get("suggested_code_change") # This is the code string from the insight
            module_path = details.get("module_path")
            function_name = details.get("function_name")
            original_description = details.get("suggested_change_description", "No specific description provided.")
            original_reflection_id_for_test = details.get("original_reflection_entry_id")

            if not module_path or not function_name:
                log_message_details = f"Missing module_path or function_name for tool '{tool_name}'. Cannot attempt modification."
                print(f"ActionExecutor: {log_message_details}")
                global_reflection_log.log_execution(
                    goal_description=f"Self-modification attempt for insight {source_insight_id}",
                    plan=[{"action_type": action_type, "details": details}], execution_results=[f"Failure: {log_message_details}"],
                    overall_success=False, notes=log_notes_prefix + log_message_details,
                    status_override="SELF_MODIFICATION_FAILED_PRECONDITIONS",
                    post_modification_test_passed=None, post_modification_test_details={"notes": "Test not run due to precondition failure."}
                )
                return False

            details_for_apply_log = details.copy() # For passing to the helper

            if suggested_code:
                details_for_apply_log["source_of_code"] = "Insight"
                return await self._apply_test_and_revert_code(
                    module_path, function_name, suggested_code, original_description,
                    str(source_insight_id) if source_insight_id else "NO_INSIGHT_ID", action_type, details_for_apply_log
                )
            else:
                print(f"ActionExecutor: No direct code for {tool_name}. Requesting CodeService for fix.")
                # Fetch original code to pass to CodeService if needed, or CodeService can fetch it.
                # For this refactor, CodeService's modify_code expects `existing_code` to be passed if available,
                # or it will fetch if `module_path` and `function_name` are given.
                # Here, we pass `existing_code=None` to signal CodeService to fetch.
                code_service_result = await self.code_service.modify_code(
                    context="SELF_FIX_TOOL",
                    modification_instruction=original_description,
                    existing_code=None, # Signal CodeService to fetch
                    module_path=module_path,
                    function_name=function_name
                )

                llm_generated_code = None
                if code_service_result.get("status") == "SUCCESS_CODE_GENERATED":
                    llm_generated_code = code_service_result.get("modified_code_string")
                    logger.info(f"CodeService generated code for {function_name}. Length: {len(llm_generated_code) if llm_generated_code else 0}")
                    global_reflection_log.log_execution(
                        goal_description=f"CodeService code generation for insight {source_insight_id}",
                        plan=[{"action_type": "CODE_SERVICE_MODIFY_CODE", "details": {"module": module_path, "func": function_name}}],
                        execution_results=[f"CodeService status: SUCCESS_CODE_GENERATED. Code length: {len(llm_generated_code) if llm_generated_code else 0}"],
                        overall_success=True, status_override="CODE_SERVICE_GEN_SUCCESS"
                    )
                else: # pragma: no cover
                    logger.error(f"CodeService failed to generate code. Status: {code_service_result.get('status')}, Error: {code_service_result.get('error')}")
                    global_reflection_log.log_execution(
                        goal_description=f"CodeService code generation for insight {source_insight_id}",
                        plan=[{"action_type": "CODE_SERVICE_MODIFY_CODE", "details": {"module": module_path, "func": function_name}}],
                        execution_results=[f"CodeService status: {code_service_result.get('status')}. Error: {code_service_result.get('error')}"],
                        overall_success=False, status_override="CODE_SERVICE_GEN_FAILED",
                        post_modification_test_passed=None, post_modification_test_details={"notes": "Test not run as LLM code generation failed."}
                    )
                    return False

                if llm_generated_code:
                    details_for_apply_log["source_of_code"] = "CodeService_LLM"
                    return await self._apply_test_and_revert_code(
                        module_path, function_name, llm_generated_code, original_description,
                        str(source_insight_id) if source_insight_id else "NO_INSIGHT_ID", action_type, details_for_apply_log
                    )
                else: # Should have been caught by previous check, but as a safeguard
                    return False # pragma: no cover

        elif action_type == "ADD_LEARNED_FACT":
            fact_to_learn = details.get("fact_to_learn")
            source_description = details.get("source", f"Insight {source_insight_id}")
            if not fact_to_learn:
                # ... (rest of ADD_LEARNED_FACT logic remains the same) ...
                log_message_details = "Missing 'fact_to_learn' in details. Cannot add fact."
                print(f"ActionExecutor: {log_message_details}")
                global_reflection_log.log_execution(
                    goal_description=f"Add learned fact attempt from insight {source_insight_id}",
                    plan=[{"action_type": action_type, "details": details}], execution_results=[f"Failure: {log_message_details}"],
                    overall_success=False, notes=log_notes_prefix + log_message_details,
                    status_override="ADD_FACT_FAILED_PRECONDITIONS"
                )
                return False
            try:
                current_facts = load_learned_facts()
                if any(fact.lower() == fact_to_learn.lower() for fact in current_facts):
                    log_message = f"Fact '{fact_to_learn}' already exists. No action taken."
                    print(f"ActionExecutor: {log_message}")
                    global_reflection_log.log_execution(
                        goal_description=f"Add learned fact from insight {source_insight_id}",
                        plan=[{"action_type": action_type, "details": details}], execution_results=[log_message],
                        overall_success=True, notes=log_notes_prefix + "Fact already present.",
                        is_self_modification_attempt=True, source_suggestion_id=source_insight_id,
                        modification_type="ADD_LEARNED_FACT_DUPLICATE_IGNORED"
                    )
                    return True
                current_facts.append(fact_to_learn)
                if save_learned_facts(current_facts):
                    log_message = f"Successfully added learned fact: '{fact_to_learn}'."
                    print(f"ActionExecutor: {log_message}")
                    global_reflection_log.log_execution(
                        goal_description=f"Add learned fact from insight {source_insight_id}",
                        plan=[{"action_type": action_type, "details": details}], execution_results=[log_message],
                        overall_success=True, notes=log_notes_prefix + f"Added '{fact_to_learn}'.",
                        is_self_modification_attempt=True, source_suggestion_id=source_insight_id,
                        modification_type="ADD_LEARNED_FACT_SUCCESS",
                        modification_details={"fact_added": fact_to_learn}
                    )
                    return True
                else: # pragma: no cover
                    log_message = "Failed to save updated learned facts."
                    print(f"ActionExecutor: {log_message}")
                    global_reflection_log.log_execution(
                        goal_description=f"Add learned fact from insight {source_insight_id}",
                        plan=[{"action_type": action_type, "details": details}], execution_results=[log_message],
                        overall_success=False, notes=log_notes_prefix + f"Failed to save fact '{fact_to_learn}'.",
                        is_self_modification_attempt=True, source_suggestion_id=source_insight_id,
                        modification_type="ADD_LEARNED_FACT_SAVE_FAILED"
                    )
                    return False
            except Exception as e: # pragma: no cover
                error_message = f"Exception during adding learned fact '{fact_to_learn}': {e}"
                print(f"ActionExecutor: {error_message}")
                global_reflection_log.log_execution(
                    goal_description=f"Add learned fact attempt from insight {source_insight_id}",
                    plan=[{"action_type": action_type, "details": details}], execution_results=[error_message],
                    overall_success=False, notes=log_notes_prefix + f"Exception for '{fact_to_learn}'.",
                    is_self_modification_attempt=True, source_suggestion_id=source_insight_id,
                    modification_type="ADD_LEARNED_FACT_EXCEPTION", # Corrected parameter names
                    first_error_type=type(e).__name__, first_error_message=str(e)
                )
                return False

        else: # pragma: no cover
            print(f"ActionExecutor: Unknown or unsupported action_type: {action_type}")
            return False

        # This line should ideally not be reached if all action types return explicitly.
        # However, it can be reached if a PROPOSE_TOOL_MODIFICATION path that should return doesn't.
        print(f"ActionExecutor: Unhandled path or placeholder for action '{action_type}'. Returning False.")
        return False

if __name__ == '__main__': # pragma: no cover
    from dataclasses import dataclass, field
    from ai_assistant.config import get_data_dir # Added for test setup

    CUSTOM_TOOLS_DIR = os.path.join("ai_assistant", "custom_tools")
    os.makedirs(CUSTOM_TOOLS_DIR, exist_ok=True)
    dummy_tool_file_path = os.path.join(CUSTOM_TOOLS_DIR, "my_extra_tools.py")

    if not os.path.exists(dummy_tool_file_path):
        with open(dummy_tool_file_path, "w") as f:
            f.write("# Dummy tool file for testing ActionExecutor\n")
            f.write("def subtract_numbers(a: float, b: float) -> float:\n")
            f.write("    # Original version, might have a bug or needs enhancement\n")
            f.write("    return float(a) - float(b)\n\n")
            f.write("def echo_message(message: str) -> str:\n")
            f.write("    return message # Original echo\n")
            f.write("\n# Placeholder for a function that might cause an error during post-modification test if not careful\n")
            f.write("def original_failing_function(data: dict) -> str:\n")
            f.write("    return data['key_that_might_be_missing_after_edit']\n")

    # Ensure the central data directory exists, as other modules (like persistent_memory)
    # are expected to use it.
    data_dir = get_data_dir()
    print(f"[ActionExecutor Test Setup] Ensured data directory exists at: {data_dir}")

    # LEARNED_FACTS_FILEPATH is imported from persistent_memory.
    # It is now expected to point to a file within the data_dir.
    # The following check and save_learned_facts will use that (hopefully updated) path.
    if not os.path.exists(LEARNED_FACTS_FILEPATH):
        print(f"[ActionExecutor Test Setup] Attempting to create dummy learned facts at: {LEARNED_FACTS_FILEPATH}")
        save_learned_facts(["Initial dummy fact from action_executor test setup (should be in data dir)."])
    else:
        print(f"[ActionExecutor Test Setup] Learned facts file already exists at: {LEARNED_FACTS_FILEPATH}")

    @dataclass
    class MockReflectionLogEntryForTest:
        entry_id: str = field(default_factory=lambda: str(uuid.uuid4()))
        goal_description: str = "Mock Goal"
        plan: Optional[List[Dict[str, Any]]] = None
        execution_results: Optional[List[Any]] = None
        status: str = "UNKNOWN"
        notes: Optional[str] = ""
        timestamp: datetime.datetime = field(default_factory=lambda: datetime.datetime.now(datetime.timezone.utc))
        error_type: Optional[str] = None
        error_message: Optional[str] = None
        # ... (other fields as needed by ReflectionLogEntry) ...

    async def main_test():
        # For testing ActionExecutor, we need a LearningAgent instance or a mock.
        # Let's use the actual LearningAgent for this test, assuming it can be instantiated simply.
        # If LearningAgent has complex dependencies for init, a mock would be better.
        test_learning_agent = LearningAgent(insights_filepath="test_action_executor_insights.json")
        executor = ActionExecutor(learning_agent=test_learning_agent)

        original_timestamp = datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(minutes=5)
        original_goal = "Test original goal that failed"
        original_plan = [{"tool_name": "subtract_numbers_tool_alias", "args": (10, 5), "kwargs": {}}]

        mock_entry_id = str(uuid.uuid4())
        # Create a proper ReflectionLogEntry for testing
        mock_original_entry = ReflectionLogEntry(
            entry_id=mock_entry_id,
            goal_description=original_goal,
            status="FAILURE",
            plan=original_plan,
            execution_results=[TypeError("Simulated original error")],
            error_type="TypeError",
            timestamp=original_timestamp
        )
        # Replace the log entries with our test entry
        global_reflection_log.log_entries = [mock_original_entry]  # Reset and add

        mock_tool_mod_action_with_code = {
            "source_insight_id": "insight_tool_mod_001",
            "action_type": "PROPOSE_TOOL_MODIFICATION",
            "details": {
                "module_path": "ai_assistant.custom_tools.my_extra_tools",
                "function_name": "subtract_numbers",
                "tool_name": "subtract_numbers_tool_alias",
                "suggested_change_description": "Enhance subtract_numbers to log output.",
                "suggested_code_change": "def subtract_numbers(a: float, b: float) -> float:\n    result = float(a) - float(b)\n    print(f'Subtracting: {a} - {b} = {result}')\n    return result",
                "original_reflection_entry_id": mock_entry_id
            }
        }
        print("\n--- Testing PROPOSE_TOOL_MODIFICATION (with code & post-mod test) ---")
        success_tool_mod = await executor.execute_action(mock_tool_mod_action_with_code)
        print(f"Tool modification with code and post-mod test action success: {success_tool_mod}")
        if not success_tool_mod:
            print("INFO: If this was due to test failure, reversion should have been attempted (see logs).")

        print("\n--- Testing PROPOSE_TOOL_MODIFICATION (LLM Gen - CodeService will be called) ---")
        mock_tool_mod_llm_attempt = {
            "source_insight_id": "insight_tool_mod_llm_002",
            "action_type": "PROPOSE_TOOL_MODIFICATION",
            "details": {
                "module_path": "ai_assistant.custom_tools.my_extra_tools",
                "function_name": "echo_message",
                "tool_name": "echo_message_tool_alias",
                "suggested_change_description": "echo_message needs to shout more and return original.",
                "original_reflection_entry_id": str(uuid.uuid4()) # Dummy ID for this test
            }
        }

        # Temporarily mock CodeService.modify_code to simulate LLM behavior for this test
        original_code_service_modify = executor.code_service.modify_code
        async def mock_modify_code_no_suggestion(*args, **kwargs):
            print("Mocked CodeService.modify_code: Simulating LLM no suggestion.")
            return {"status": "ERROR_LLM_NO_SUGGESTION", "modified_code_string": None, "logs": ["LLM failed"], "error": "LLM no suggestion"}

        executor.code_service.modify_code = mock_modify_code_no_suggestion
        try:
            success_tool_mod_llm = await executor.execute_action(mock_tool_mod_llm_attempt)
            print(f"Tool modification (LLM attempt - no code from CodeService) action success: {success_tool_mod_llm} (expected False)")
        finally:
            executor.code_service.modify_code = original_code_service_modify # Restore

    asyncio.run(main_test())

# ### END FILE: ai_assistant/execution/action_executor.py ###

# ### START FILE: ai_assistant/expansion/__init__.py ###
# This file marks expansion as a package.

# ### END FILE: ai_assistant/expansion/__init__.py ###

# ### START FILE: ai_assistant/expansion/modularity.py ###
# Code for modular design.
pass

# ### END FILE: ai_assistant/expansion/modularity.py ###

# ### START FILE: ai_assistant/expansion/plugins.py ###
# Code for plugin management.
pass

# ### END FILE: ai_assistant/expansion/plugins.py ###

# ### START FILE: ai_assistant/goals/__init__.py ###
# This file marks goals as a package.

# ### END FILE: ai_assistant/goals/__init__.py ###

# ### START FILE: ai_assistant/goals/goal_management.py ###
# Code for goal management.
import uuid
from typing import List, Dict, Optional, Union
from ai_assistant.memory.persistent_memory import save_goals_to_file, load_goals_from_file
import os

# --- Constants ---
DEFAULT_GOALS_FILE_DIR = "data"
DEFAULT_GOALS_FILE = os.path.join(DEFAULT_GOALS_FILE_DIR, "goals.json")

# --- Goal Data Structure ---
# A goal will be represented as a dictionary.
# Example:
# {
#     "id": "unique_id_string",
#     "description": "Achieve world peace.",
#     "status": "pending",  # "pending", "in_progress", "completed", "failed"
#     "priority": 1  # Lower number means higher priority
# }

# --- In-Memory Storage ---
_goals_db: Dict[str, Dict] = {} # This will be initialized by load_initial_goals
# _next_goal_int_id: int = 1 # Not currently used with UUIDs, but could be if IDs change

def _generate_goal_id() -> str:
    """Generates a unique goal ID."""
    return uuid.uuid4().hex

# --- Persistence Functions ---

def save_current_goals() -> bool:
    """Saves the current in-memory _goals_db to the DEFAULT_GOALS_FILE."""
    # The save_goals_to_file function in persistent_memory.py handles os.makedirs
    return save_goals_to_file(DEFAULT_GOALS_FILE, _goals_db)

def load_persisted_goals() -> bool:
    """
    Loads goals from DEFAULT_GOALS_FILE and replaces the in-memory _goals_db.
    Returns True if loading was successful and _goals_db was updated, False otherwise.
    """
    global _goals_db
    loaded_data = load_goals_from_file(DEFAULT_GOALS_FILE)
    if loaded_data or (not loaded_data and os.path.exists(DEFAULT_GOALS_FILE)): 
        # If loaded_data is not empty, or it's empty because the file itself was empty JSON ({})
        # and not because the file was missing (which load_goals_from_file handles by returning {}).
        # This logic ensures we overwrite with an empty DB if the file is truly empty vs. not found.
        # However, load_goals_from_file returns {} for missing file as well, so the check for
        # os.path.exists isn't strictly necessary if we trust its "file not found" behavior.
        # Let's simplify: if load_goals_from_file returns a dict (even empty), it's "successful".
        _goals_db = loaded_data
        # print(f"Goal management: _goals_db updated from {DEFAULT_GOALS_FILE}") # For debugging
        return True
    # If load_goals_from_file returned {} AND the file didn't exist, it's not an "error" for this function,
    # but it means no goals were loaded to replace the current DB.
    # The current implementation of load_goals_from_file returns {} for file not found,
    # so this function will effectively reset the _goals_db if the file is not found.
    # This might be desired, or one might want to only replace if the file *is* found.
    # For now, we'll consider any dict returned (even empty from a non-existent file) as a valid load.
    # The function `load_goals_from_file` already prints messages for file not found or errors.
    # So, if loaded_data is {} because file wasn't found, _goals_db becomes {}.
    _goals_db = loaded_data # This line ensures _goals_db is replaced even if file was empty or not found.
    return True # Consider it successful in terms of attempting a load.

def _initialize_goals_db():
    """Loads goals from the default file when the module is first initialized."""
    global _goals_db
    print(f"GoalManagement: Initializing goals database from '{DEFAULT_GOALS_FILE}'...")
    _goals_db = load_goals_from_file(DEFAULT_GOALS_FILE)
    if _goals_db:
        print(f"GoalManagement: Successfully loaded {len(_goals_db)} goals on startup.")
    else:
        print("GoalManagement: No goals loaded on startup or file not found/empty. Starting with an empty database.")

# --- CRUD Functions ---

def create_goal(description: str, priority: int = 3) -> Dict:
    """
    Creates a new goal and stores it in the in-memory database.
    Does not automatically save to file; call save_current_goals() for that.
    """
    goal_id = _generate_goal_id()
    goal = {
        "id": goal_id,
        "description": description,
        "status": "pending",
        "priority": priority,
    }
    _goals_db[goal_id] = goal
    return goal

def get_goal(goal_id: str) -> Optional[Dict]:
    """
    Retrieves a goal by its ID.

    Args:
        goal_id: The ID of the goal to retrieve.

    Returns:
        The goal dictionary if found, otherwise None.
    """
    return _goals_db.get(goal_id)

def update_goal(goal_id: str, description: Optional[str] = None, 
                status: Optional[str] = None, priority: Optional[int] = None) -> Optional[Dict]:
    """
    Updates an existing goal.

    Args:
        goal_id: The ID of the goal to update.
        description: The new description (if provided).
        status: The new status (if provided).
        priority: The new priority (if provided).

    Returns:
        The updated goal dictionary if found and updated, otherwise None.
    """
    goal = _goals_db.get(goal_id)
    if goal:
        if description is not None:
            goal["description"] = description
        if status is not None:
            # Basic validation for status, can be expanded
            valid_statuses = ["pending", "in_progress", "completed", "failed"]
            if status in valid_statuses:
                goal["status"] = status
            else:
                print(f"Warning: Invalid status '{status}' for goal '{goal_id}'. Not updated.")
        if priority is not None:
            goal["priority"] = int(priority) # Ensure priority is stored as int
        return goal
    return None

def delete_goal(goal_id: str) -> bool:
    """
    Deletes a goal by its ID.

    Args:
        goal_id: The ID of the goal to delete.

    Returns:
        True if the goal was found and deleted, otherwise False.
    """
    if goal_id in _goals_db:
        del _goals_db[goal_id]
        return True
    return False

def list_goals(status: Optional[str] = None) -> List[Dict]:
    """
    Lists all goals, optionally filtering by status.

    Args:
        status: If provided, filters goals by this status.

    Returns:
        A list of goal dictionaries.
    """
    if status:
        return [goal for goal in _goals_db.values() if goal["status"] == status]
    return list(_goals_db.values())

# --- Initialization ---
_initialize_goals_db() # Load goals when module is imported

# --- Basic Manual Testing (Persistence Focus) ---
if __name__ == '__main__':
    print("\n--- Testing Goal Management with Persistence ---")
    
    # Ensure the test data directory is clean or doesn't interfere
    # For these tests, goal_management uses DEFAULT_GOALS_FILE ("data/goals.json")
    # We should probably use a different file for its own tests to avoid side effects
    # with the main application's data. However, for this example, we'll assume
    # it's okay or that we clean up "data/goals.json" after.

    # Current state (should be from file if it existed, or empty)
    print(f"Initial goals loaded: {list_goals()}")

    # Create some goals
    print("\nCreating new goals...")
    g1 = create_goal("Test persistence goal 1", 1)
    g2 = create_goal("Test persistence goal 2", 2)
    print(f"Goals after creation: {list_goals()}")

    # Save goals
    print("\nSaving current goals...")
    if save_current_goals():
        print("Goals saved successfully to DEFAULT_GOALS_FILE.")
    else:
        print("Error saving goals.")
    
    # Clear in-memory DB to simulate restart / fresh load
    print("\nSimulating restart: Clearing in-memory _goals_db and loading...")
    _goals_db.clear() 
    print(f"In-memory goals after clearing: {list_goals()}") # Should be empty

    if load_persisted_goals():
        print("Goals loaded successfully from DEFAULT_GOALS_FILE.")
    else:
        print("Error loading goals or file not found.")
    
    print(f"Goals after loading: {list_goals()}")
    # Verify that g1 and g2 (or their equivalents) are present
    found_g1 = any(g['description'] == "Test persistence goal 1" for g in _goals_db.values())
    assert found_g1, "Goal 1 not found after loading."
    print("Verified that loaded goals include the saved ones.")

    # Modify a goal and save again
    print("\nUpdating a goal and re-saving...")
    if _goals_db.get(g1['id']):
        update_goal(g1['id'], status="in_progress", description="Updated persistence goal 1")
        print(f"Goal {g1['id']} updated.")
    else:
        print(f"Could not find goal {g1['id']} to update after load, something is wrong.")

    save_current_goals()
    print("Re-saved goals.")

    # Simulate another restart
    print("\nSimulating another restart and load...")
    _goals_db.clear()
    load_persisted_goals()
    print(f"Goals after second loading: {list_goals()}")
    updated_g1_check = get_goal(g1['id'])
    if updated_g1_check:
        assert updated_g1_check['status'] == "in_progress", "Goal 1 status not updated after re-load."
        assert updated_g1_check['description'] == "Updated persistence goal 1", "Goal 1 description not updated."
        print("Verified updates persisted.")
    else:
        print(f"Goal {g1['id']} not found after second load. This is an error.")

    # Clean up the default goals file for next time / other tests
    # In a real app, you wouldn't usually do this in the __main__ block.
    if os.path.exists(DEFAULT_GOALS_FILE):
        print(f"\nCleaning up by removing {DEFAULT_GOALS_FILE}...")
        os.remove(DEFAULT_GOALS_FILE)
        if os.path.exists(DEFAULT_GOALS_FILE_DIR) and not os.listdir(DEFAULT_GOALS_FILE_DIR):
            os.rmdir(DEFAULT_GOALS_FILE_DIR)


    print("\n--- End of Goal Management Persistence Testing ---")

# ### END FILE: ai_assistant/goals/goal_management.py ###

# ### START FILE: ai_assistant/learning/__init__.py ###
# This file marks learning as a package.

# ### END FILE: ai_assistant/learning/__init__.py ###

# ### START FILE: ai_assistant/learning/autonomous_learning.py ###
# ai_assistant/learning/autonomous_learning.py

import json
from typing import List, Optional

from ai_assistant.llm_interface.ollama_client import invoke_ollama_model_async
from ai_assistant.config import get_model_for_task, is_debug_mode
import re # For cleaning LLM response

# Import the LLM-powered curation function from knowledge_tools
from ai_assistant.custom_tools.knowledge_tools import _curate_and_update_fact_store

FACT_EXTRACTION_PROMPT_TEMPLATE = """
You are an AI assistant analyzing a conversation snippet to identify potential facts that can be learned and stored.
Focus on factual statements made by the user or the AI that are general knowledge or specific to the user's context if they seem like persistent information (e.g., user's name, preferences).
Avoid extracting questions, commands, opinions, or highly transient information.

Conversation Snippet:
---
{conversation_snippet}
---

Based on the snippet, identify distinct factual statements.
If no clear facts can be extracted, respond with the exact string "NO_FACTS_IDENTIFIED".
Otherwise, respond with a JSON object containing a single key "facts", which is a list of strings. Each string should be a self-contained factual statement. Strive to capture user-specific information in a canonical way.

Example 1:
Conversation Snippet:
---
User: My favorite color is blue.
AI: That's a nice color! I also learned that the capital of France is Paris.
---
Expected JSON Response:
{{
  "facts": [
    "The user's favorite color is blue.",
    "The capital of France is Paris."
  ]
}}

Example 2:
Conversation Snippet:
---
User: Can you search for nearby coffee shops?
AI: Sure, I found three nearby.
---
Expected JSON Response:
NO_FACTS_IDENTIFIED

Example 3:
Conversation Snippet:
---
User: My cat's name is Whiskers.
AI: Whiskers is a lovely name for a cat.
---
Expected JSON Response:
{{
  "facts": [
    "The user's cat's name is Whiskers."
  ]
}}

Example 4 (User Name Identification):
Conversation Snippet:
---
User: Hi, my name is Alex.
AI: Hello Alex! Nice to meet you.
---
Expected JSON Response:
{{
  "facts": [
    "The user's name is Alex."
  ]
}}

Example 5 (User Name Identification - different phrasing):
Conversation Snippet:
---
User: You can call me Dr. Smith.
AI: Understood, Dr. Smith.
---
Expected JSON Response:
{{
  "facts": [
    "The user's name is Dr. Smith."
  ]
}}

Respond only with the JSON object or the "NO_FACTS_IDENTIFIED" string.
"""

async def extract_potential_facts(conversation_snippet: str, llm_model_name: Optional[str] = None) -> List[str]:
    """
    Analyzes a conversation snippet using an LLM to extract potential factual statements.

    Args:
        conversation_snippet: A string representing the part of the conversation to analyze.
        llm_model_name: Optional name of the LLM model to use. If None, uses the default
                        model configured for "fact_extraction".

    Returns:
        A list of strings, where each string is an extracted factual statement.
        Returns an empty list if no facts are identified or if an error occurs.
    """
    model_to_use = llm_model_name if llm_model_name is not None else get_model_for_task("fact_extraction")
    
    prompt = FACT_EXTRACTION_PROMPT_TEMPLATE.format(conversation_snippet=conversation_snippet)

    if is_debug_mode():
        print(f"[DEBUG AUTONOMOUS_LEARNING] Extracting facts from conversation snippet (first 100 chars): {conversation_snippet[:100]}...")
        # print(f"[DEBUG AUTONOMOUS_LEARNING] Fact extraction prompt (first 300 chars):\n{prompt[:300]}...") 

    llm_response = await invoke_ollama_model_async(prompt, model_name=model_to_use)

    if not llm_response or not llm_response.strip():
        print("Warning (extract_potential_facts): Received no or empty response from LLM for fact extraction.")
        return []

    llm_response = llm_response.strip()
    if is_debug_mode():
        print(f"[DEBUG AUTONOMOUS_LEARNING] Raw LLM response for fact extraction:\n'{llm_response}'")

    if llm_response == "NO_FACTS_IDENTIFIED":
        return []

    cleaned_response = llm_response
    if cleaned_response.startswith("```json"):
        cleaned_response = cleaned_response.lstrip("```json").rstrip("```").strip()
    elif cleaned_response.startswith("```"):
        cleaned_response = cleaned_response.lstrip("```").rstrip("```").strip()

    try:
        parsed_response = json.loads(cleaned_response)
        
        if not isinstance(parsed_response, dict):
            print(f"Warning (extract_potential_facts): LLM response for fact extraction was not a JSON dictionary. Response: {cleaned_response}")
            return []
            
        facts = parsed_response.get("facts")
        
        if facts is None:
            print(f"Warning (extract_potential_facts): LLM response JSON is missing 'facts' key. Response: {cleaned_response}")
            return []

        if not isinstance(facts, list) or not all(isinstance(fact, str) for fact in facts):
            print(f"Warning (extract_potential_facts): LLM response 'facts' key is not a list of strings. Response: {parsed_response}")
            return []
            
        return facts

    except json.JSONDecodeError:
        print(f"Error (extract_potential_facts): Failed to parse LLM response as JSON for fact extraction. Response: {cleaned_response}")
        return []
    except Exception as e:
        print(f"Error (extract_potential_facts): An unexpected error occurred during LLM response processing for fact extraction: {e}")
        return []


async def learn_facts_from_interaction(user_input: str, ai_response: str, enabled: bool) -> Optional[List[str]]:
    """
    Orchestrates the process of extracting potential facts from a user-AI interaction
    and then submitting them to the knowledge base for LLM-powered curation and storage.

    Args:
        user_input: The input provided by the user.
        ai_response: The response generated by the AI.
        enabled: A boolean flag to enable or disable this learning feature.

    Returns:
        A list of fact strings that were *sent* for curation if the process was initiated 
        and the curation step reported success.
        Returns None if the feature is disabled, no facts were extracted, 
        or the curation process failed.
    """
    if not enabled:
        return None

    if not user_input and not ai_response:
        return None # Nothing to process

    conversation_snippet = f"User: {user_input}\nAI: {ai_response}"
    
    if is_debug_mode():
        print(f"[DEBUG AUTONOMOUS_LEARNING] Analyzing snippet for autonomous learning:\n{conversation_snippet}")

    extracted_facts = await extract_potential_facts(conversation_snippet)

    if not extracted_facts:
        if is_debug_mode():
            print("[DEBUG AUTONOMOUS_LEARNING] No potential facts extracted for autonomous learning.")
        return None

    # Filter out empty or whitespace-only strings from extracted_facts before sending to curator
    valid_extracted_facts = [fact for fact in extracted_facts if isinstance(fact, str) and fact.strip()]

    if not valid_extracted_facts:
        if is_debug_mode():
            print("[DEBUG AUTONOMOUS_LEARNING] All extracted facts were empty or invalid. No facts sent for curation.")
        return None
    
    if is_debug_mode():
        print(f"[DEBUG AUTONOMOUS_LEARNING] Sending {len(valid_extracted_facts)} extracted fact(s) for LLM curation: {valid_extracted_facts}")

    # Call the curation function from knowledge_tools with the batch of extracted facts
    # _curate_and_update_fact_store is already async
    curation_successful = await _curate_and_update_fact_store(valid_extracted_facts)
    
    if curation_successful:
        if is_debug_mode():
            print(f"[DEBUG AUTONOMOUS_LEARNING] Fact curation process reported success for: {valid_extracted_facts}")
        # Return the facts that were sent for curation, as an indication of what was processed.
        # The actual learned facts might be different due to LLM's curation.
        return valid_extracted_facts 
    else:
        if is_debug_mode():
            print(f"[DEBUG AUTONOMOUS_LEARNING] Fact curation process failed for: {valid_extracted_facts}")
        return None

if __name__ == '__main__': # pragma: no cover
    import asyncio
    import os
    from ai_assistant.memory.persistent_memory import save_learned_facts, load_learned_facts, LEARNED_FACTS_FILEPATH
    # We need to import the real _curate_and_update_fact_store from knowledge_tools for the __main__ block's patching logic
    # or ensure the global patch targets the correct module.
    # The import `from ai_assistant.custom_tools.knowledge_tools import _curate_and_update_fact_store` at the top
    # is what learn_facts_from_interaction will use.

    # --- Test Setup ---
    if not os.path.exists("data"):
        os.makedirs("data", exist_ok=True)
    
    DEFAULT_FACTS_FILE = LEARNED_FACTS_FILEPATH 
    original_facts_content_main = None
    if os.path.exists(DEFAULT_FACTS_FILE):
        with open(DEFAULT_FACTS_FILE, 'r', encoding='utf-8') as f_orig_main:
            original_facts_content_main = f_orig_main.read()
    save_learned_facts([]) 

    # Mock invoke_ollama_model_async for extract_potential_facts
    async def mock_fact_extraction_llm_main(prompt: str, model_name: str, **kwargs):
        if "My favorite color is blue" in prompt and "capital of France is Paris" in prompt:
            return json.dumps({"facts": ["The user's favorite color is blue.", "The capital of France is Paris."]})
        elif "My cat's name is Whiskers" in prompt:
            return json.dumps({"facts": ["The user's cat's name is Whiskers."]})
        elif "my name is Alex" in prompt.lower(): # Test for new name extraction example
            return json.dumps({"facts": ["The user's name is Alex."]})
        elif "call me dr. smith" in prompt.lower(): # Test for another name extraction example
            return json.dumps({"facts": ["The user's name is Dr. Smith."]})
        elif "search for nearby coffee shops" in prompt:
            return "NO_FACTS_IDENTIFIED"
        return json.dumps({"facts": []}) 

    # Mock _curate_and_update_fact_store from knowledge_tools for testing autonomous_learning.py
    _mock_main_curated_facts_store: List[str] = [] 
    async def mock_curate_store_main(newly_observed: List[str]):
        global _mock_main_curated_facts_store
        
        current_facts_for_mock_curation = list(_mock_main_curated_facts_store)
        
        updated_for_mock_curation = list(current_facts_for_mock_curation)
        for f_new in newly_observed:
            # Simple de-duplication for mock; real one is LLM based
            is_new_name_fact = "The user's name is " in f_new
            existing_name_fact_index = -1
            if is_new_name_fact:
                for i, existing_f in enumerate(updated_for_mock_curation):
                    if "The user's name is " in existing_f:
                        existing_name_fact_index = i
                        break
            
            if existing_name_fact_index != -1: # Update existing name fact
                updated_for_mock_curation[existing_name_fact_index] = f_new
            elif f_new not in updated_for_mock_curation: # Add if not a duplicate (and not updating name)
                updated_for_mock_curation.append(f_new)
        
        _mock_main_curated_facts_store = updated_for_mock_curation
        
        save_success = save_learned_facts(_mock_main_curated_facts_store, DEFAULT_FACTS_FILE)
        return save_success


    import ai_assistant.learning.autonomous_learning as this_module
    original_invoke_async_auto_learn_main = this_module.invoke_ollama_model_async
    this_module.invoke_ollama_model_async = mock_fact_extraction_llm_main

    original_curate_store_import_main = this_module._curate_and_update_fact_store 
    this_module._curate_and_update_fact_store = mock_curate_store_main


    async def run_autonomous_learning_tests_main():
        print("\n--- Testing learn_facts_from_interaction (with updated mocks for batch curation) ---")
        
        global _mock_main_curated_facts_store
        
        # Test 1: Learn new facts from interaction
        print("\nTest 1: Learn new facts from interaction")
        _mock_main_curated_facts_store = [] 
        save_learned_facts([]) 

        user_input1 = "My favorite color is blue."
        ai_response1 = "That's a nice color! I also learned that the capital of France is Paris."
        
        processed_facts1 = await learn_facts_from_interaction(user_input1, ai_response1, enabled=True)
        print(f"Processed facts from interaction 1: {processed_facts1}")
        assert processed_facts1 is not None
        assert "The user's favorite color is blue." in processed_facts1
        assert "The capital of France is Paris." in processed_facts1
        
        recalled_after_test1 = load_learned_facts(DEFAULT_FACTS_FILE) 
        print(f"Facts in store after Test 1 ({DEFAULT_FACTS_FILE}): {recalled_after_test1}")
        assert "The user's favorite color is blue." in recalled_after_test1
        assert "The capital of France is Paris." in recalled_after_test1

        # Test for name extraction
        print("\nTest Name Extraction 1: 'my name is Alex'")
        _mock_main_curated_facts_store = [] # Reset for this specific test segment
        save_learned_facts([])
        user_input_name1 = "Hi, my name is Alex."
        ai_response_name1 = "Hello Alex!"
        processed_name1 = await learn_facts_from_interaction(user_input_name1, ai_response_name1, enabled=True)
        print(f"Processed name fact 1: {processed_name1}")
        assert processed_name1 and "The user's name is Alex." in processed_name1
        recalled_name1 = load_learned_facts(DEFAULT_FACTS_FILE)
        print(f"Facts in store after name fact 1: {recalled_name1}")
        assert "The user's name is Alex." in recalled_name1

        print("\nTest Name Extraction 2: 'call me Dr. Smith' (should update previous name)")
        # Current store: ["The user's name is Alex."]
        user_input_name2 = "Actually, you can call me Dr. Smith."
        ai_response_name2 = "Understood, Dr. Smith."
        processed_name2 = await learn_facts_from_interaction(user_input_name2, ai_response_name2, enabled=True)
        print(f"Processed name fact 2: {processed_name2}")
        assert processed_name2 and "The user's name is Dr. Smith." in processed_name2
        recalled_name2 = load_learned_facts(DEFAULT_FACTS_FILE)
        print(f"Facts in store after name fact 2: {recalled_name2}")
        assert "The user's name is Dr. Smith." in recalled_name2
        assert "The user's name is Alex." not in recalled_name2 # Check if old name was replaced by mock logic
        assert len(recalled_name2) == 1 # Should only be one name fact if mock logic correctly updated

        # Restore original functions
        this_module.invoke_ollama_model_async = original_invoke_async_auto_learn_main
        this_module._curate_and_update_fact_store = original_curate_store_import_main

        # Cleanup
        print("\n--- Test Cleanup ---")
        if original_facts_content_main is not None:
            with open(DEFAULT_FACTS_FILE, 'w', encoding='utf-8') as f_restore_main:
                f_restore_main.write(original_facts_content_main)
            print(f"Restored original content to {DEFAULT_FACTS_FILE}")
        elif os.path.exists(DEFAULT_FACTS_FILE): 
            os.remove(DEFAULT_FACTS_FILE)
            print(f"Removed test file {DEFAULT_FACTS_FILE}")
        
        data_dir_main = os.path.dirname(DEFAULT_FACTS_FILE)
        if os.path.exists(data_dir_main) and not os.listdir(data_dir_main):
            try: os.rmdir(data_dir_main)
            except OSError: pass

        print("\n--- Autonomous Learning Module Tests Finished ---")

    asyncio.run(run_autonomous_learning_tests_main())

# ### END FILE: ai_assistant/learning/autonomous_learning.py ###

# ### START FILE: ai_assistant/learning/evolution.py ###
import logging
import os
import shutil
import subprocess
import sys
import tempfile
from typing import Dict, Any, Optional # Added Optional
from unittest.mock import patch, MagicMock, ANY # Added for testing

from ai_assistant.core.self_modification import edit_function_source_code

# Configure logger for this module
logger = logging.getLogger(__name__)
# BasicConfig should ideally be set at the application entry point.
# If this module is run as a script, this will configure it.
if not logging.getLogger().handlers: # Check if root logger has handlers
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

# Define at module level for use in __main__
dummy_tool_module_name_for_test = "dummy_tool_module.py"


def test_modified_tool_in_sandbox(module_path: str, function_name: str, project_root: str) -> Dict[str, Any]:
    """
    Tests a modified tool by executing a temporary script in a sandboxed environment (subprocess).
    This function attempts to import the specified module and function, and calls
    the function if it takes no arguments.
    in an isolated and secure manner to ensure it works as expected and
    doesn't introduce regressions or security issues.

    Args:
        module_path: The Python module path of the modified tool (e.g., "ai_assistant.tools.my_tool").
        function_name: The name of the modified function.
        project_root: The absolute path to the root of the project.

    Returns:
        True if the tests pass (or placeholder is active), False otherwise.
    
    Design Considerations for Future Implementation:
    1.  Approach 1 (Subprocess with Restricted Scope):
        *   Create a temporary copy of the modified tool file and minimal necessary
            surrounding code/project structure.
        *   Execute a separate Python interpreter process using `subprocess.run()`.
        *   The target script for the subprocess would import the modified tool and
            run predefined or generated test cases.
        *   Capture stdout, stderr, and return code for analysis.
        *   Concerns: Managing dependencies, complexity of isolating the exact code needed,
            ensuring the subprocess doesn't have unwanted system access (e.g., network, filesystem beyond designated temp areas).

    2.  Approach 2 (Docker Container):
        *   Dynamically create a Dockerfile that installs necessary dependencies from the project
            (e.g., requirements.txt) and copies the modified tool/module.
        *   Build and run the Docker container.
        *   Execute tests within the container (e.g., by running pytest against the specific file or generated test script).
        *   Concerns: Docker availability on the host system, overhead of Docker image
            building for each test, potential slowness.

    3.  Approach 3 (Library like `RestrictedPython` or `pysandbox`):
        *   Investigate libraries that allow executing Python code with restricted permissions
            within the same process or a tightly controlled child process.
        *   Concerns: Maturity and limitations of such libraries (e.g., might not catch all
            malicious behavior, might interfere with legitimate but complex tool operations),
            thoroughness of the sandboxing, compatibility with async code or specific modules.

    4.  Test Case Generation/Selection:
        *   How to get/generate test cases for the modified function?
            *   Attempt to discover and run existing unit tests for the specific tool/module.
                This requires a robust test discovery mechanism within the project.
            *   Use an LLM to generate simple test cases based on the function's
                docstring, signature, and potentially the nature of the code change.
            *   Perform basic "does it import/load" checks.
            *   Execute with a set of predefined "smoke test" inputs if applicable.
        *   The quality and coverage of test cases are critical for this step's effectiveness.
    """
    logger.info(
        f"Placeholder: Sandboxed testing for {module_path}.{function_name} at project_root '{project_root}' is not yet implemented. Defaulting to True (success)."
    )
    # For now, simulate success to allow the main flow to proceed.
    # In a real implementation, this would return True only if actual tests pass.
    # --- Start of implemented logic ---
    script_content_template = """
import importlib
import sys
import traceback
import os

# It's crucial that project_root is added to sys.path
# so that the module_path can be imported correctly.
project_root_placeholder = "{project_root_escaped_for_string}"
sys.path.insert(0, project_root_placeholder)

module_path_placeholder = "{module_path_str}"
function_name_placeholder = "{function_name_str}"

success = False
output_capture = []

def log_print(message):
    print(message)
    output_capture.append(str(message))

try:
    log_print(f"Attempting to import module: {{module_path_placeholder}} from {{project_root_placeholder}}")
    target_module = importlib.import_module(module_path_placeholder)
    log_print(f"Successfully imported module: {{module_path_placeholder}}")
    
    target_function = getattr(target_module, function_name_placeholder)
    log_print(f"Successfully retrieved function: {{function_name_placeholder}}")
    
    import inspect
    sig = inspect.signature(target_function)
    
    if not sig.parameters:
        log_print(f"Function {{function_name_placeholder}} takes no arguments. Attempting to call.")
        target_function() 
        log_print(f"Successfully called {{function_name_placeholder}} in {{module_path_placeholder}}")
        success = True
    else:
        log_print(f"Function {{function_name_placeholder}} in {{module_path_placeholder}} loaded but not called due to parameters: {{str(sig.parameters)}}.")
        success = True # Consider loading success for now

except Exception as e:
    log_print(f"Error during sandboxed test of {{module_path_placeholder}}.{{function_name_placeholder}}:")
    # Use os.linesep to ensure cross-platform compatibility for newlines in the output.
    tb_lines = traceback.format_exc().splitlines()
    for line in tb_lines:
        log_print(line)
    success = False

if not success:
    # Optionally, print all captured output to stderr of this script before exiting
    # for easier debugging if the calling process captures stderr.
    # sys.stderr.write("\\n".join(output_capture) + "\\n")
    sys.exit(1)
"""
    abs_project_root = os.path.abspath(project_root)
    script_content = script_content_template.format(
        project_root_escaped_for_string=abs_project_root,
        module_path_str=module_path,
        function_name_str=function_name
    )

    try:
        with tempfile.TemporaryDirectory() as tmpdir:
            temp_script_path = os.path.join(tmpdir, f"temp_test_runner_{os.urandom(4).hex()}.py")
            with open(temp_script_path, "w", encoding="utf-8") as f:
                f.write(script_content)
            
            logger.info(f"Executing sandboxed test script: {temp_script_path} with cwd: {abs_project_root}")
            
            process_result = subprocess.run(
                [sys.executable, temp_script_path],
                capture_output=True,
                text=True,
                timeout=30,
                cwd=abs_project_root,
                check=False
            )
            
            logger.info(f"Sandboxed test script STDOUT:\n{process_result.stdout}")
            if process_result.stderr:
                logger.error(f"Sandboxed test script STDERR:\n{process_result.stderr}")

            notes = ""
            passed_status = process_result.returncode == 0
            if passed_status:
                # Note: The placeholder in script_content is "function_name_placeholder", 
                # but we should check against the actual function_name passed to this function.
                if f"Successfully called {function_name}" in process_result.stdout:
                    notes = f"No-args function '{function_name}' in '{module_path}' called successfully in sandbox."
                elif f"Function {function_name}" in process_result.stdout and "loaded but not called due to parameters" in process_result.stdout:
                    notes = f"Function '{function_name}' in '{module_path}' with parameters loaded successfully in sandbox (not called)."
                else:
                    notes = f"Sandboxed test script completed successfully for '{module_path}.{function_name}'."
                logger.info(f"Sandboxed test for {module_path}.{function_name} PASSED. Notes: {notes}")
            else:
                combined_output = process_result.stdout + "\n" + process_result.stderr
                # Check if import was attempted but not successful
                import_attempted_msg = f"Attempting to import module: {module_path}"
                import_succeeded_msg = f"Successfully imported module: {module_path}"

                if (import_attempted_msg in process_result.stdout and import_succeeded_msg not in process_result.stdout) or \
                   "ImportError" in combined_output or "ModuleNotFoundError" in combined_output:
                    notes = f"Import of '{module_path}' likely failed in sandbox for function '{function_name}'. Output indicates import issues."
                else:
                    notes = f"Sandboxed test script for '{module_path}.{function_name}' FAILED with return code {process_result.returncode}."
                logger.error(f"Sandboxed test for {module_path}.{function_name} FAILED. Notes: {notes}")

            return {"passed": passed_status, "stdout": process_result.stdout, "stderr": process_result.stderr, "notes": notes}
                
    except subprocess.TimeoutExpired:
        notes = f"Sandboxed test for {module_path}.{function_name} TIMED OUT after 30 seconds."
        logger.error(notes)
        return {"passed": False, "stdout": "", "stderr": "Timeout during execution.", "notes": notes}
    except FileNotFoundError:
        notes = f"Could not find Python interpreter '{sys.executable}' for sandboxed test of {module_path}.{function_name}."
        logger.error(notes, exc_info=True)
        return {"passed": False, "stdout": "", "stderr": "Python interpreter not found.", "notes": notes}
    except Exception as e:
        notes = f"An unexpected error occurred during sandboxed test execution for {module_path}.{function_name}: {e}"
        logger.error(notes, exc_info=True)
        return {"passed": False, "stdout": "", "stderr": str(e), "notes": notes}
    # --- End of implemented logic ---


def commit_tool_change(
    module_path: str, 
    function_name: str, 
    project_root: str, 
    suggestion_id: Optional[str] = None, 
    suggestion_details: Optional[Dict[str, Any]] = None
) -> tuple[bool, Optional[str]]:
    """
    Commits the modified tool file to a version control system (Git).

    Args:
        module_path: The Python module path of the modified tool.
        function_name: The name of the modified function.
        project_root: Absolute path to the project's Git repository root.
        suggestion_id: Optional ID from the suggestion that led to this change.
        suggestion_details: Optional dictionary containing more details from the suggestion,
                            e.g., for constructing a commit message body.

    Returns:
        A tuple (bool, Optional[str]): 
        (True, commit_message_string) if the commit was successful.
        (False, None) if the commit failed.
    """
    logger.info(f"Attempting Git commit for {module_path}.{function_name} in project {project_root}")

    # 1. Prerequisites Check
    git_path = shutil.which("git")
    if not git_path:
        logger.error("Git command not found. Cannot perform commit. Please ensure Git is installed and in PATH.")
        return False, None
    logger.info(f"Git executable found at: {git_path}")

    # Log git version for debugging (optional, can be noisy)
    # try:
    #     version_result = subprocess.run([git_path, '--version'], capture_output=True, text=True, check=False, timeout=5)
    #     logger.debug(f"Git version: {version_result.stdout.strip()}")
    # except Exception: pass # Ignore if this fails

    if not os.path.isdir(os.path.join(project_root, ".git")):
        logger.error(f"Project root '{project_root}' is not a Git repository (missing .git directory). Cannot perform commit.")
        return False, None
    logger.info(f"Project root '{project_root}' appears to be a Git repository.")

    # 2. File Path Conversion
    path_parts = module_path.split('.')
    relative_module_file_path = os.path.join(*path_parts) + ".py"
    full_module_file_path = os.path.join(project_root, relative_module_file_path)
    
    if not os.path.exists(full_module_file_path):
        logger.error(f"Module file to commit does not exist at calculated path: {full_module_file_path}")
        return False, None
        
    logger.info(f"File path for git add (relative to repo root): {relative_module_file_path}")

    # 3. Construct Commit Message
    commit_subject = f"AI Autocommit: Modified {function_name} in {module_path}"
    if suggestion_id:
        commit_subject += f" (Suggestion ID: {suggestion_id})"
    
    commit_body_content = ""
    if suggestion_details:
        change_desc = suggestion_details.get("suggested_change_description") or suggestion_details.get("suggestion_text")
        if change_desc and isinstance(change_desc, str):
            commit_body_content = change_desc.strip()

    full_commit_message = commit_subject
    if commit_body_content and commit_body_content.lower() != commit_subject.lower(): # Avoid duplicate if body is same as subject
        full_commit_message += f"\n\n{commit_body_content}"

    # 4. Execute Git Commands
    try:
        logger.info(f"Running: {git_path} add {relative_module_file_path} (cwd: {project_root})")
        add_result = subprocess.run(
            [git_path, 'add', relative_module_file_path],
            capture_output=True, text=True, cwd=project_root, check=False, timeout=15
        )
        logger.debug(f"Git add STDOUT:\n{add_result.stdout}")
        if add_result.stderr: 
            logger.info(f"Git add STDERR:\n{add_result.stderr}") # Stderr is not always an error for git add
        if add_result.returncode != 0:
            logger.error(f"Git add command failed for '{relative_module_file_path}' with return code {add_result.returncode}.")
            return False, None
        logger.info(f"Git add successful for '{relative_module_file_path}'.")

        commit_command = [git_path, 'commit', '-m', commit_subject]
        if commit_body_content and commit_body_content.lower() != commit_subject.lower():
            commit_command.extend(['-m', commit_body_content])
        
        commit_command_str_for_log = " ".join(f"'{arg}'" if " " in arg else arg for arg in commit_command)
        logger.info(f"Running: {commit_command_str_for_log} (cwd: {project_root})")
        
        commit_result = subprocess.run(
            commit_command,
            capture_output=True, text=True, cwd=project_root, check=False, timeout=15
        )
        logger.debug(f"Git commit STDOUT:\n{commit_result.stdout}")
        if commit_result.stderr: 
            logger.info(f"Git commit STDERR:\n{commit_result.stderr}")
            
        if commit_result.returncode != 0:
            if "nothing to commit" in commit_result.stdout.lower() or \
               "nothing to commit" in commit_result.stderr.lower():
                 logger.warning(f"Git commit indicated nothing to commit for {relative_module_file_path}. This might be okay.")
                 return True, full_commit_message # Commit successful (no changes)
            logger.error(f"Git commit command failed for '{relative_module_file_path}' with return code {commit_result.returncode}.")
            return False, None
        
        logger.info(f"Git commit successful for '{relative_module_file_path}'.")
        return True, full_commit_message

    except subprocess.TimeoutExpired as e:
        logger.error(f"Git operation timed out for {module_path}.{function_name}. Command: {' '.join(e.cmd if e.cmd else [])}", exc_info=True)
        return False, None
    except Exception as e:
        logger.error(f"An unexpected error occurred during Git operations for {module_path}.{function_name}: {e}", exc_info=True)
        return False, None


def apply_code_modification(suggestion: Dict[str, Any]) -> Dict[str, Any]:
    """
    Applies a suggested code modification, including sandboxed testing, potential revert, and version control commit.

    Args:
        suggestion: A dictionary containing the details for the code modification.
                    Expected keys: "module_path", "function_name", "suggested_code_change",
                    and optionally "suggestion_id".

    Returns:
        A dictionary with a detailed breakdown of the operation's outcome.
        See task description for the new required return structure.
    """
    result = {
        "overall_status": False,
        "overall_message": "",
        "edit_outcome": {"status": False, "message": "", "backup_path": None},
        "test_outcome": None,
        "revert_outcome": None,
        "commit_outcome": None,
    }

    required_keys = ["module_path", "function_name", "suggested_code_change"]
    missing_keys = [key for key in required_keys if key not in suggestion]
    if missing_keys:
        msg = f"Suggestion dictionary is missing required keys: {', '.join(missing_keys)}"
        logger.error(msg)
        result["overall_message"] = msg
        return result

    module_path = suggestion["module_path"]
    function_name = suggestion["function_name"]
    new_code_string = suggestion["suggested_code_change"]
    suggestion_id = suggestion.get("suggestion_id")

    type_error_msg = ""
    if not isinstance(module_path, str): type_error_msg += f"'module_path' must be a string. "
    if not isinstance(function_name, str): type_error_msg += f"'function_name' must be a string. "
    if not isinstance(new_code_string, str): type_error_msg += f"'suggested_code_change' must be a string."
    if type_error_msg:
        logger.error(f"Type errors in suggestion: {type_error_msg.strip()}")
        result["overall_message"] = f"Type errors in suggestion: {type_error_msg.strip()}"
        return result

    logger.info(f"Attempting to apply code modification for function '{function_name}' in module '{module_path}'. Suggestion ID: {suggestion_id or 'N/A'}")
    
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
    # target_file_for_edit is relative to project_root for edit_function_source_code
    # but os.path.exists needs full path if CWD is not project_root.
    # edit_function_source_code now expects module_path to be dot-separated, and constructs file path relative to project_root
    # So target_file_for_edit here is mostly for the backup path construction and pre-check.
    relative_target_file = os.path.join(*module_path.split('.')) + ".py"
    full_target_file_path = os.path.join(project_root, relative_target_file)

    try:
        if not os.path.exists(full_target_file_path):
            msg = f"Target file for modification '{full_target_file_path}' does not exist."
            logger.error(msg)
            result["overall_message"] = msg
            result["edit_outcome"]["message"] = msg
            return result

        # --- 1. Edit Step ---
        # edit_function_source_code expects module_path like "ai_assistant.tools.some_tool"
        # and project_root to find the base of the project.
        edit_message = edit_function_source_code(module_path, function_name, new_code_string, project_root_path=project_root)
        
        result["edit_outcome"]["message"] = edit_message
        if "success" not in edit_message.lower():
            logger.error(f"Failed to apply code modification: {edit_message}")
            result["overall_message"] = f"Code editing failed: {edit_message}"
            return result
        
        result["edit_outcome"]["status"] = True
        result["edit_outcome"]["backup_path"] = full_target_file_path + ".bak" # edit_function_source_code creates this
        logger.info(f"Code modification successful: {edit_message}")

        # --- 2. Test Step ---
        logger.info(f"Proceeding to sandboxed testing for {module_path}.{function_name}.")
        test_results_dict = test_modified_tool_in_sandbox(module_path, function_name, project_root)
        result["test_outcome"] = test_results_dict

        if not test_results_dict.get("passed"):
            test_failure_notes = test_results_dict.get('notes', 'No specific notes from test.')
            msg = f"Sandboxed testing of modified tool {module_path}.{function_name} FAILED. Notes: {test_failure_notes}"
            logger.critical(msg)
            result["overall_message"] = msg
            
            # --- 2a. Revert Step (due to test failure) ---
            revert_outcome_dict = {"status": False, "message": ""}
            backup_to_restore_from = result["edit_outcome"]["backup_path"]
            if backup_to_restore_from and os.path.exists(backup_to_restore_from):
                try:
                    shutil.move(backup_to_restore_from, full_target_file_path) # Use full_target_file_path
                    revert_msg = f"Successfully reverted {full_target_file_path} from backup {backup_to_restore_from}."
                    logger.info(revert_msg)
                    revert_outcome_dict["status"] = True
                    revert_outcome_dict["message"] = revert_msg
                except Exception as e_revert:
                    revert_err_msg = f"CRITICAL: Failed to revert {full_target_file_path} from backup. Manual intervention required. Error: {e_revert}"
                    logger.error(revert_err_msg, exc_info=True)
                    revert_outcome_dict["message"] = revert_err_msg
                    result["overall_message"] += f" {revert_err_msg}" # Append to overall message
            else:
                revert_not_found_msg = f"CRITICAL: Backup file {backup_to_restore_from} not found. Cannot revert."
                logger.error(revert_not_found_msg)
                revert_outcome_dict["message"] = revert_not_found_msg
                result["overall_message"] += f" {revert_not_found_msg}"
            result["revert_outcome"] = revert_outcome_dict
            return result # overall_status remains False

        logger.info(f"Sandboxed testing of modified tool {module_path}.{function_name} passed.")
        
        # Clean up backup file if tests passed
        if result["edit_outcome"]["backup_path"] and os.path.exists(result["edit_outcome"]["backup_path"]):
            try:
                os.remove(result["edit_outcome"]["backup_path"])
                logger.info(f"Removed backup file: {result['edit_outcome']['backup_path']}")
                result["edit_outcome"]["backup_path"] = None # Indicate it's removed
            except OSError as e_remove:
                logger.warning(f"Could not remove backup file {result['edit_outcome']['backup_path']}: {e_remove}")
        
        # --- 3. Commit Step ---
        commit_status, generated_commit_msg = commit_tool_change(
            module_path, function_name, project_root, 
            suggestion_id=suggestion_id,
            suggestion_details=suggestion # Pass the whole suggestion for more context if needed by commit_tool_change
        )
        
        result["commit_outcome"] = {
            "status": commit_status,
            "commit_message_generated": generated_commit_msg,
            "error_message": None
        }

        if commit_status:
            logger.info(f"Automated commit for {module_path}.{function_name} successful.")
            result["overall_status"] = True
            result["overall_message"] = "Tool modification, testing, and local commit successful."
        else:
            # Commit failed, but edit and test were successful.
            commit_fail_msg = f"Tool modification and testing successful, but local commit failed for {module_path}.{function_name}."
            logger.error(commit_fail_msg)
            result["commit_outcome"]["error_message"] = "Commit command failed or Git prerequisites not met. See logs."
            result["overall_message"] = commit_fail_msg
            # overall_status remains False as per requirement if commit fails
        
        return result

    except Exception as e:
        msg = f"An unexpected error occurred during apply_code_modification for {module_path}.{function_name}: {e}"
        logger.error(msg, exc_info=True)
        if not result["overall_message"]: # Ensure overall_message is set
            result["overall_message"] = msg
        # Populate specific step outcome if error happened there, if identifiable
        if result["edit_outcome"]["status"] is False and not result["edit_outcome"]["message"]:
            result["edit_outcome"]["message"] = f"Unexpected error during edit phase: {e}"
        # Add more specific error context if possible based on where it occurred.
        return result

if __name__ == '__main__':
    from unittest.mock import MagicMock, call # Added call
    from subprocess import CompletedProcess # Added for mocking subprocess.run

    # --- Test Setup ---
    TEST_WORKSPACE_PARENT_DIR = "temp_evolution_test_sandbox"
    TEST_PROJECT_ROOT_FOR_DUMMY = os.path.join(TEST_WORKSPACE_PARENT_DIR, "dummy_project_root")
    DUMMY_MODULE_DIR_STRUCTURE = os.path.join(TEST_PROJECT_ROOT_FOR_DUMMY, "ai_assistant", "dummy_modules")
    dummy_tool_fs_path = os.path.join(DUMMY_MODULE_DIR_STRUCTURE, dummy_tool_module_name_for_test)
    dummy_module_py_path = f"ai_assistant.dummy_modules.{dummy_tool_module_name_for_test.replace('.py', '')}"

    # Updated dummy function to have one with no args and one with args for testing the sandbox script logic
    original_dummy_function_no_args_code = (
        "def sample_tool_function_no_args():\n"
        "    '''This is a sample tool function with no arguments.'''\n"
        "    print('Original no-args function called successfully!')\n"
        "    return 'Original no-args result'\n"
    )
    original_dummy_function_with_args_code = (
        "def sample_tool_function(param1: int, param2: str) -> str:\n"
        "    '''This is a sample tool function.'''\n"
        "    print(f'Original function called with {param1} and {param2}')\n"
        "    return f'Original result: {param1} - {param2}'\n"
    )
    original_dummy_file_content = (
        f"import os\n\n{original_dummy_function_no_args_code}\n\n"
        f"{original_dummy_function_with_args_code}\n"
    )

    def setup_test_environment():
        if os.path.exists(TEST_WORKSPACE_PARENT_DIR):
            shutil.rmtree(TEST_WORKSPACE_PARENT_DIR)
        os.makedirs(DUMMY_MODULE_DIR_STRUCTURE, exist_ok=True)
        with open(dummy_tool_fs_path, "w", encoding="utf-8") as f:
            f.write(original_dummy_file_content)
        logger.info(f"Created dummy tool file for testing: {dummy_tool_fs_path}")
        # Create a dummy .bak file as if edit_function_source_code created it
        # This is for testing the revert logic.
        # The path needs to be relative to where apply_code_modification thinks the file is.
        # If CWD = TEST_PROJECT_ROOT_FOR_DUMMY, then module_path is "ai_assistant..."
        # and edit_function_source_code creates backup at "ai_assistant/.../file.py.bak"
        backup_path_for_tests = os.path.join(TEST_PROJECT_ROOT_FOR_DUMMY, *dummy_module_py_path.split('.')) + ".py.bak"
        with open(backup_path_for_tests, "w", encoding="utf-8") as f:
            f.write("# This is a dummy backup content for testing revert\n" + original_dummy_file_content)
        logger.info(f"Created dummy .bak file for testing revert: {backup_path_for_tests}")


    def read_dummy_tool_file_content() -> str:
        # This function reads from the true path, not relative to a changed CWD
        if os.path.exists(dummy_tool_fs_path):
            with open(dummy_tool_fs_path, "r", encoding="utf-8") as f:
                return f.read()
        return ""

    def cleanup_test_environment():
        if os.path.exists(TEST_WORKSPACE_PARENT_DIR):
            shutil.rmtree(TEST_WORKSPACE_PARENT_DIR)
            logger.info(f"Cleaned up test workspace: {TEST_WORKSPACE_PARENT_DIR}")

    logger.info("Starting tests for apply_code_modification with sandboxing...")
    all_tests_passed = True
    original_cwd = os.getcwd()
    
    try:
        setup_test_environment()
        os.chdir(TEST_PROJECT_ROOT_FOR_DUMMY) 

        # Test 1: Successful modification, sandbox, and commit (with commit message body)
        logger.info("--- Test 1: Successful modification, sandbox, and commit (with body) ---")
        with patch('subprocess.run') as mock_subprocess_run_t1, \
             patch('shutil.which', MagicMock(return_value="/usr/bin/git")) as mock_which_t1, \
             patch('os.path.isdir', MagicMock(return_value=True)) as mock_isdir_t1, \
             patch('ai_assistant.core.self_modification.edit_function_source_code', MagicMock(return_value="Successfully updated function.")) as mock_edit_t1:
            
            # Simulate sandbox success, then successful 'git add' and 'git commit'
            mock_subprocess_run_t1.side_effect = [
                subprocess.CompletedProcess(args=[sys.executable, ANY], returncode=0, stdout="Sandbox success", stderr=""), # Sandbox
                subprocess.CompletedProcess(args=['git', 'add', ANY], returncode=0, stdout="Added", stderr=""),             # Git Add
                subprocess.CompletedProcess(args=['git', 'commit', ANY, ANY, ANY, ANY], returncode=0, stdout="Committed", stderr="") # Git Commit
            ]
            
            suggestion_t1 = {
                "suggestion_id": "SUG001_T1",
                "suggested_change_description": "This is a detailed description of the change for the commit body.",
                "module_path": dummy_module_py_path,
                "function_name": "sample_tool_function_no_args", 
                "suggested_code_change": "def sample_tool_function_no_args(): print('Modified no-args version')\n"
            }
            result_t1 = apply_code_modification(suggestion_t1)
            logger.info(f"Test 1 Result: {result_t1}")
            assert result_t1 is True, "Test 1 Failed: Should return True on full success path."
            mock_edit_t1.assert_called_once()
            
            # Assertions for commit_tool_change's subprocess calls
            expected_rel_path = os.path.join("ai_assistant", "dummy_modules", dummy_tool_module_name_for_test)
            expected_commit_subject = f"AI Autocommit: Modified sample_tool_function_no_args in {dummy_module_py_path} (Suggestion ID: SUG001_T1)"
            expected_commit_body = "This is a detailed description of the change for the commit body."
            
            # Check sandbox call (1st call to subprocess.run)
            assert mock_subprocess_run_t1.call_args_list[0][0][0][0] == sys.executable
            assert mock_subprocess_run_t1.call_args_list[0][1]['cwd'] == os.path.abspath(TEST_PROJECT_ROOT_FOR_DUMMY)
            # Check git add call (2nd call to subprocess.run)
            assert mock_subprocess_run_t1.call_args_list[1][0][0] == [mock_which_t1.return_value, 'add', expected_rel_path]
            # Check git commit call (3rd call to subprocess.run)
            assert mock_subprocess_run_t1.call_args_list[2][0][0] == [mock_which_t1.return_value, 'commit', '-m', expected_commit_subject, '-m', expected_commit_body]
        logger.info("Test 1 Passed.")

        # Test 2: Successful modification and sandbox, but FAILED 'git commit' (no commit body)
        logger.info("--- Test 2: Successful modification and sandbox, failed 'git commit' (no body) ---")
        # We need to ensure a .bak file exists that edit_function_source_code would have created.
        # edit_function_source_code creates it at target_file_for_edit + ".bak"
        # target_file_for_edit is os.path.join(*module_path.split('.')) + ".py"
        # So, ai_assistant/dummy_modules/dummy_tool_module.py.bak (relative to CWD=TEST_PROJECT_ROOT_FOR_DUMMY)
        
        # Reset dummy file for this test, as edit_function_source_code is mocked
        # and won't create the .bak file itself in this mocked scenario.
        # The `setup_test_environment` already creates a .bak for us.
        os.chdir(original_cwd)
        setup_test_environment() # This recreates the dummy .bak file too.
        os.chdir(TEST_PROJECT_ROOT_FOR_DUMMY)

        with patch('subprocess.run') as mock_subprocess_run_t2, \
             patch('shutil.which', MagicMock(return_value="/usr/bin/git")) as mock_which_t2, \
             patch('os.path.isdir', MagicMock(return_value=True)) as mock_isdir_t2, \
             patch('ai_assistant.core.self_modification.edit_function_source_code', MagicMock(return_value="Successfully updated function.")) as mock_edit_t2:
            
            mock_subprocess_run_t2.side_effect = [
                subprocess.CompletedProcess(args=[sys.executable, ANY], returncode=0, stdout="Sandbox OK"),
                subprocess.CompletedProcess(args=['git', 'add', ANY], returncode=0, stdout="Add OK"),
                subprocess.CompletedProcess(args=['git', 'commit', ANY, ANY], returncode=1, stderr="Commit Fail") # No body for this test
            ]
            suggestion_t2 = { "suggestion_id": "SUG002_T2", "module_path": dummy_module_py_path, "function_name": "sample_tool_function_no_args", "suggested_code_change": "def f(): pass" }
            result_t2 = apply_code_modification(suggestion_t2)
            logger.info(f"Test 2 Result: {result_t2}")
            assert result_t2.get("status") is False, f"Test 2 Failed: Status False expected for commit fail. Got: {result_t2}"
            assert "commit failed" in result_t2.get("message", "").lower()
        logger.info("Test 2 Passed.")

        # Test 3: Successful modification BUT FAILED sandboxed test (revert)
        logger.info("--- Test 3: Successful modification, failed sandbox test (revert) ---")
        os.chdir(original_cwd)
        setup_test_environment()
        os.chdir(TEST_PROJECT_ROOT_FOR_DUMMY)
        with patch('subprocess.run', MagicMock(return_value=subprocess.CompletedProcess(args=[], returncode=1, stderr="Sandbox script error"))) as mock_subprocess_run_t3, \
             patch('shutil.which', MagicMock(return_value="/usr/bin/git")), \
             patch('os.path.isdir', MagicMock(return_value=True)), \
             patch('ai_assistant.core.self_modification.edit_function_source_code', MagicMock(return_value="Successfully updated function.")) as mock_edit_t3, \
             patch('shutil.move') as mock_shutil_move_t3:
            
            suggestion_t3 = { "suggestion_id": "SUG003_T3", "module_path": dummy_module_py_path, "function_name": "sample_tool_function_no_args", "suggested_code_change": "def f(): pass" }
            result_t3 = apply_code_modification(suggestion_t3)
            logger.info(f"Test 3 Result: {result_t3}")
            assert result_t3.get("status") is False, f"Test 3 Failed: Status False for sandbox fail. Got: {result_t3}"
            assert "sandboxed testing" in result_t3.get("message", "").lower() and "failed" in result_t3.get("message", "").lower()
            mock_shutil_move_t3.assert_called_once()
        logger.info("Test 3 Passed.")

        # Test 4: edit_function_source_code fails
        logger.info("--- Test 4: edit_function_source_code fails ---")
        os.chdir(original_cwd)
        setup_test_environment()
        os.chdir(TEST_PROJECT_ROOT_FOR_DUMMY)
        with patch('subprocess.run'), \
             patch('shutil.which', MagicMock(return_value="/usr/bin/git")), \
             patch('os.path.isdir', MagicMock(return_value=True)), \
             patch('ai_assistant.core.self_modification.edit_function_source_code', MagicMock(return_value="Error: Function not found.")) as mock_edit_t4:
            
            suggestion_t4 = {"suggestion_id": "SUG004_T4", "module_path": dummy_module_py_path, "function_name": "non_existent", "suggested_code_change": "def f(): pass"}
            result_t4 = apply_code_modification(suggestion_t4)
            logger.info(f"Test 4 Result: {result_t4}")
            assert result_t4.get("status") is False, f"Test 4 Failed: Status False for edit fail. Got: {result_t4}"
            assert "Error: Function not found" in result_t4.get("message", "")
        logger.info("Test 4 Passed.")

    except AssertionError as e:
        logger.error(f"TEST ASSERTION FAILED: {e}", exc_info=True)
        all_tests_passed = False
    except Exception as e:
        logger.error(f"AN UNEXPECTED ERROR OCCURRED DURING TESTS: {e}", exc_info=True)
        all_tests_passed = False
    finally:
        os.chdir(original_cwd) 
        cleanup_test_environment()

    if all_tests_passed:
        logger.info("All evolution.py tests (including sandboxing placeholders) passed successfully!")
    else:
        logger.error("One or more evolution.py tests (including sandboxing placeholders) failed.")

# ### END FILE: ai_assistant/learning/evolution.py ###

# ### START FILE: ai_assistant/learning/learning.py ###
"""
This module is central to the AI assistant's ability to learn and evolve from its experiences.
Its primary responsibilities include:

1.  **Processing Reflection Data:**
    *   Analyzing `ReflectionLogEntry` objects provided by the `core.reflection` module.
    *   Leveraging outputs from `reflection.analyze_last_failure()` and `reflection.get_learnings_from_reflections()`.

2.  **Identifying Actionable Insights:**
    *   Distilling concrete, actionable insights from the processed reflection data.
    *   Recognizing patterns of failures, successes, or inefficiencies.
    *   Identifying needs for new knowledge or capabilities.

3.  **Formulating Improvement Proposals:**
    *   Translating insights into specific, testable proposals for improvement. This can include:
        *   **Tool Modification:** Suggesting changes to the source code of existing tools (to be implemented via `core.self_modification`). This could be to fix bugs, enhance functionality, or improve reliability.
        *   **Tool Description Enhancement:** Proposing updates to tool descriptions to make them clearer for the planning module.
        *   **New Tool Suggestion:** Identifying the need for entirely new tools and potentially outlining their desired functionality.
        *   **Knowledge Base Update:** Formulating new facts to be added to the agent's persistent memory (`memory.persistent_memory.save_learned_facts()`).
        *   **Planning Heuristic Refinement:** (Future Goal) Suggesting improvements to the planning strategies or heuristics used by the `planning.planning` module.

4.  **Managing and Prioritizing Insights:**
    *   Storing these actionable insights persistently.
    *   Developing a mechanism to prioritize which insights to act upon first.

5.  **Initiating Improvement Actions:**
    *   Triggering the actual implementation of high-priority improvements. This might involve:
        *   Creating tasks for self-modification using `core.self_modification`.
        *   Adding new facts to its knowledge base.
        *   (In the future) Interacting with a human developer for complex changes or approvals.

This module aims to close the loop in the agent's operational cycle:
Plan -> Execute -> Reflect -> Learn -> Evolve.
"""
import datetime
import os
import asyncio
import uuid # Added for entry_id in MockReflectionLogEntry
from typing import Optional, Dict, Any, List, Tuple
from enum import Enum, auto
from dataclasses import dataclass, field, asdict

from ai_assistant.core.reflection import ReflectionLogEntry
from ai_assistant.memory.persistent_memory import save_actionable_insights, load_actionable_insights, ACTIONABLE_INSIGHTS_FILEPATH
from ai_assistant.execution.action_executor import ActionExecutor

class InsightType(Enum):
    TOOL_BUG_SUSPECTED = auto()
    TOOL_USAGE_ERROR = auto()
    TOOL_ENHANCEMENT_SUGGESTED = auto()
    NEW_TOOL_SUGGESTED = auto()
    KNOWLEDGE_GAP_IDENTIFIED = auto()
    LEARNED_FACT_CORRECTION = auto()
    PLANNING_HEURISTIC_SUGGESTION = auto()
    SELF_CORRECTION_SUCCESS = auto()
    SELF_CORRECTION_FAILURE = auto()

@dataclass
class ActionableInsight:
    type: InsightType
    description: str
    source_reflection_entry_ids: List[str]
    insight_id: Optional[str] = None
    related_tool_name: Optional[str] = None
    suggested_code_change: Optional[str] = None
    suggested_tool_description: Optional[str] = None
    new_tool_requirements: Optional[str] = None
    knowledge_to_learn: Optional[str] = None
    incorrect_fact_to_correct: Optional[str] = None
    corrected_fact: Optional[str] = None
    planning_heuristic_details: Optional[Dict[str, Any]] = None
    priority: int = 5
    status: str = "NEW"
    creation_timestamp: str = field(default_factory=lambda: datetime.datetime.now(datetime.timezone.utc).isoformat())
    metadata: Dict[str, Any] = field(default_factory=dict)

    def __post_init__(self):
        if not self.insight_id:
            # Generate a new UUID-based insight_id if not provided or empty
            self.insight_id = f"{self.type.name}_{uuid.uuid4().hex[:8]}"
class LearningAgent:
    def __init__(self, insights_filepath: Optional[str] = None):
        self.insights: List[ActionableInsight] = []
        self.insights_filepath = insights_filepath if insights_filepath is not None else ACTIONABLE_INSIGHTS_FILEPATH
        self.action_executor = ActionExecutor(learning_agent=self)
        self._load_insights()

    def _load_insights(self):
        print(f"LearningAgent: Loading insights from '{self.insights_filepath}'...")
        insights_data = load_actionable_insights(filepath=self.insights_filepath)
        loaded_count = 0
        for data in insights_data:
            if not isinstance(data, dict): # pragma: no cover
                print(f"LearningAgent: Warning - skipping non-dictionary item in loaded insights data: {data}")
                continue
            try:
                if 'type' in data and isinstance(data['type'], str):
                    try:
                        data['type'] = InsightType[data['type']]
                    except KeyError: # pragma: no cover
                        print(f"LearningAgent: Warning - Invalid InsightType string '{data['type']}' in loaded data. Skipping insight: {data.get('insight_id')}")
                        continue

                required_fields = ['insight_id', 'type', 'description', 'source_reflection_entry_ids']
                if not all(field_name in data for field_name in required_fields): # pragma: no cover
                     print(f"LearningAgent: Warning - Missing required fields in loaded insight data {data.get('insight_id', '')}. Skipping insight.")
                     continue

                self.insights.append(ActionableInsight(**data))
                loaded_count += 1
            except Exception as e: # pragma: no cover
                print(f"LearningAgent: Error deserializing insight data: '{str(data)[:100]}...'. Error: {e}. Skipping.")
        print(f"LearningAgent: Loaded {loaded_count} actionable insights from '{self.insights_filepath}'.")
        if not self.insights and insights_data: # pragma: no cover
             print(f"LearningAgent: Warning - Insights data file '{self.insights_filepath}' was not empty, but no valid insights were loaded. File might be corrupted or in an old format.")

    def _save_insights(self):
        print(f"LearningAgent: Saving {len(self.insights)} insights to '{self.insights_filepath}'...")
        insights_as_dicts = []
        for insight in self.insights:
            insight_dict = asdict(insight)
            insight_dict['type'] = insight.type.name
            insights_as_dicts.append(insight_dict)

        if save_actionable_insights(insights_as_dicts, filepath=self.insights_filepath):
            print(f"LearningAgent: Successfully saved insights.")
        else: # pragma: no cover
            print(f"LearningAgent: Failed to save insights.")

    def process_reflection_entry(self, entry: ReflectionLogEntry) -> Optional[ActionableInsight]:
        # Use the new unique entry_id from ReflectionLogEntry
        source_entry_ref_id = entry.entry_id # NEW WAY
        generated_insight: Optional[ActionableInsight] = None
        metadata_for_insight: Dict[str, Any] = {}

        # Store original_reflection_entry_ref_id in metadata for PROPOSE_TOOL_MODIFICATION
        # This will be used by ActionExecutor to find the original failing plan for re-testing
        metadata_for_insight["original_reflection_entry_ref_id"] = source_entry_ref_id


        if entry.status in ["FAILURE", "PARTIAL_SUCCESS"] and entry.error_type:
            description = f"Tool execution failed or partially failed for goal '{entry.goal_description}'. Error: {entry.error_type} - {entry.error_message}."
            related_tool_name = None
            insight_type_to_use = InsightType.TOOL_BUG_SUSPECTED

            if entry.plan and entry.execution_results and len(entry.plan) == len(entry.execution_results):
                for i, result in enumerate(entry.execution_results):
                    is_error = False
                    if isinstance(result, dict) and result.get("_is_error_representation_"):
                        is_error = True
                    elif isinstance(result, Exception):
                        is_error = True

                    if is_error:
                        if entry.plan[i] and isinstance(entry.plan[i], dict):
                            failed_step_details = entry.plan[i]
                            related_tool_name = failed_step_details.get("tool_name")

                            if related_tool_name in ["subtract_numbers", "echo_message"]:
                                metadata_for_insight["module_path"] = "ai_assistant.custom_tools.my_extra_tools"
                                metadata_for_insight["function_name"] = related_tool_name
                            elif failed_step_details.get("module_path") and failed_step_details.get("function_name_in_module"): # pragma: no cover
                                metadata_for_insight["module_path"] = failed_step_details.get("module_path")
                                metadata_for_insight["function_name"] = failed_step_details.get("function_name_in_module")

                            if not failed_step_details.get("args") and not failed_step_details.get("kwargs"):
                                insight_type_to_use = InsightType.TOOL_USAGE_ERROR
                                description += f" The tool '{related_tool_name}' was called without arguments, suggesting a usage error."
                            else:
                                description += f" The failure occurred at the step involving tool '{related_tool_name}'."
                            break

            if related_tool_name:
                generated_insight = ActionableInsight(
                    type=insight_type_to_use,
                    description=description,
                    source_reflection_entry_ids=[source_entry_ref_id], # Use the new entry_id
                    related_tool_name=related_tool_name,
                    priority=3,
                    metadata=metadata_for_insight
                )
                print(f"LearningAgent: Generated insight: {generated_insight.insight_id} for tool {related_tool_name} due to failure.")
            else:
                generated_insight = ActionableInsight(
                    type=InsightType.TOOL_BUG_SUSPECTED,
                    description=f"A failure occurred for goal '{entry.goal_description}' (Error: {entry.error_type}) but could not be attributed to a specific tool in the plan. Manual review might be needed.",
                    source_reflection_entry_ids=[source_entry_ref_id], # Use the new entry_id
                    priority=4,
                    metadata=metadata_for_insight
                )
                print(f"LearningAgent: Generated general failure insight: {generated_insight.insight_id}.")

        elif entry.status == "SUCCESS" and entry.notes and "retry" in entry.notes.lower():
            description = f"Goal '{entry.goal_description}' succeeded after retries. This might indicate transient issues or sensitivity in the involved tools."
            related_tool_name = None
            if entry.plan and len(entry.plan) == 1 and isinstance(entry.plan[0], dict):
                related_tool_name = entry.plan[0].get("tool_name")
                if related_tool_name in ["subtract_numbers", "echo_message"]:
                    metadata_for_insight["module_path"] = "ai_assistant.custom_tools.my_extra_tools"
                    metadata_for_insight["function_name"] = related_tool_name

            generated_insight = ActionableInsight(
                type=InsightType.TOOL_ENHANCEMENT_SUGGESTED,
                description=description,
                source_reflection_entry_ids=[source_entry_ref_id], # Use the new entry_id
                related_tool_name=related_tool_name,
                priority=7,
                suggested_tool_description="Consider reviewing tool for robustness against transient errors or improving error handling if retries were involved.",
                metadata=metadata_for_insight
            )
            print(f"LearningAgent: Generated insight for success after retry: {generated_insight.insight_id}")

        if generated_insight:
            self.insights.append(generated_insight)
            self._save_insights()
            return generated_insight

        return None

    async def review_and_propose_next_action(self) -> Optional[Tuple[Dict[str, Any], bool]]:
        actionable_new_insights = [insight for insight in self.insights if insight.status == "NEW"]
        if not actionable_new_insights:
            print("LearningAgent: No new actionable insights to review.")
            return None
        actionable_new_insights.sort(key=lambda insight: (insight.priority, insight.creation_timestamp))
        selected_insight = actionable_new_insights[0]

        print(f"LearningAgent: Selected insight for action: {selected_insight.insight_id} (Priority: {selected_insight.priority}, Type: {selected_insight.type.name})")
        print(f"LearningAgent: Description: {selected_insight.description}")

        proposed_action = {
            "source_insight_id": selected_insight.insight_id,
            "action_type": "TBD", "details": {}
        }

        if selected_insight.type == InsightType.TOOL_BUG_SUSPECTED or selected_insight.type == InsightType.TOOL_ENHANCEMENT_SUGGESTED:
            if selected_insight.related_tool_name:
                proposed_action["action_type"] = "PROPOSE_TOOL_MODIFICATION"
                proposed_action["details"] = {
                    "module_path": selected_insight.metadata.get("module_path"),
                    "function_name": selected_insight.metadata.get("function_name"),
                    "tool_name": selected_insight.related_tool_name,
                    "suggested_change_description": selected_insight.description,
                    "suggested_code_change": selected_insight.suggested_code_change,
                    "reason": f"Based on insight {selected_insight.insight_id}",
                    "original_reflection_entry_ref_id": selected_insight.source_reflection_entry_ids[0] if selected_insight.source_reflection_entry_ids else None
                }
            else: # pragma: no cover
                proposed_action["action_type"] = "REVIEW_MANUALLY"

        elif selected_insight.type == InsightType.KNOWLEDGE_GAP_IDENTIFIED:
            if selected_insight.knowledge_to_learn:
                proposed_action["action_type"] = "ADD_LEARNED_FACT"
                proposed_action["details"] = {
                    "fact_to_learn": selected_insight.knowledge_to_learn,
                    "source": f"Based on insight {selected_insight.insight_id}"
                }
            else: # pragma: no cover
                proposed_action["action_type"] = "REVIEW_MANUALLY"
        else: # pragma: no cover
            proposed_action["action_type"] = "REVIEW_MANUALLY"

        execution_success = False
        if proposed_action["action_type"] == "REVIEW_MANUALLY" or proposed_action["action_type"] == "TBD":
            selected_insight.status = "PENDING_MANUAL_REVIEW"
            selected_insight.metadata["review_reason"] = f"Action type was {proposed_action['action_type']}."
        else:
            selected_insight.status = "ACTION_ATTEMPTED"
            selected_insight.metadata["action_attempt_timestamp"] = datetime.datetime.now(datetime.timezone.utc).isoformat()
            try:
                execution_success = await self.action_executor.execute_action(proposed_action)
                if execution_success: selected_insight.status = "ACTION_SUCCESSFUL"
                else: selected_insight.status = "ACTION_FAILED"
                selected_insight.metadata[f"action_{selected_insight.status.lower()}_timestamp"] = datetime.datetime.now(datetime.timezone.utc).isoformat()
            except Exception as e: # pragma: no cover
                selected_insight.status = "ACTION_EXCEPTION"
                selected_insight.metadata["action_exception_timestamp"] = datetime.datetime.now(datetime.timezone.utc).isoformat()
                selected_insight.metadata["exception_details"] = str(e)
                execution_success = False
        self._save_insights()
        return proposed_action, execution_success

if __name__ == '__main__': # pragma: no cover
    # import uuid # uuid is already imported at the top of the module
    # Removed local MockReflectionLogEntry, will use the actual one.
    # from ai_assistant.core.reflection import ReflectionLogEntry # Already imported at the top


    async def run_learning_tests():
        test_insights_file = "test_actionable_insights.json"
        if os.path.exists(test_insights_file): os.remove(test_insights_file)

        custom_tools_dir = os.path.join("ai_assistant", "custom_tools")
        os.makedirs(custom_tools_dir, exist_ok=True)
        dummy_tool_path = os.path.join(custom_tools_dir, "my_extra_tools.py")
        if not os.path.exists(dummy_tool_path):
            with open(dummy_tool_path, "w") as f:
                f.write("def subtract_numbers(a: float, b: float) -> float:\n    return a - b\n")
                f.write("def echo_message(message: str) -> str:\n    return message\n")

        agent = LearningAgent(insights_filepath=test_insights_file)

        # Test process_reflection_entry correctly uses entry.entry_id
        # Use the actual ReflectionLogEntry
        mock_entry_for_processing = ReflectionLogEntry(
            goal_description="Test entry_id propagation",
            status="FAILURE",
            error_type="TestError",
            plan=[], # Required non-optional field
            execution_results=[] # Required non-optional field
        )
        processed_insight = agent.process_reflection_entry(mock_entry_for_processing)
        assert processed_insight is not None
        assert len(processed_insight.source_reflection_entry_ids) == 1
        assert processed_insight.source_reflection_entry_ids[0] == mock_entry_for_processing.entry_id
        print(f"Verified insight source ID: {processed_insight.source_reflection_entry_ids[0]} matches entry ID: {mock_entry_for_processing.entry_id}")

        # Reset insights for review_and_propose_next_action tests
        agent.insights = []
        ts_now_iso = datetime.datetime.now(datetime.timezone.utc).isoformat()

        # Insight for ADD_LEARNED_FACT
        manual_add_fact_insight = ActionableInsight(
            insight_id="manual_af_002", type=InsightType.KNOWLEDGE_GAP_IDENTIFIED,
            description="Manual insight for testing add fact.",
            source_reflection_entry_ids=[str(uuid.uuid4())], # Give it a source ID
            knowledge_to_learn="Python is a dynamically-typed language.",
            priority=1, status="NEW", creation_timestamp=ts_now_iso
        )
        agent.insights.append(manual_add_fact_insight)

        # Insight for PROPOSE_TOOL_MODIFICATION
        manual_tool_mod_insight = ActionableInsight(
            insight_id="manual_ttm_001", type=InsightType.TOOL_BUG_SUSPECTED,
            description="Manual insight for testing tool modification proposal.",
            source_reflection_entry_ids=[str(uuid.uuid4())], # Give it a source ID
            related_tool_name="subtract_numbers", priority=2, status="NEW",
            creation_timestamp=(datetime.datetime.fromisoformat(ts_now_iso) + datetime.timedelta(seconds=1)).isoformat(),
            suggested_code_change="def subtract_numbers(a: float, b: float) -> float:\n    # Modified by test\n    return float(a) - float(b) - 1.0",
            metadata={
                "module_path": "ai_assistant.custom_tools.my_extra_tools",
                "function_name": "subtract_numbers",
                "original_reflection_entry_ref_id": str(uuid.uuid4()) # Mock original ref ID
            }
        )
        agent.insights.append(manual_tool_mod_insight)
        agent._save_insights()

        print("\n--- Testing review_and_propose_next_action (with entry_id logic) ---")

        action_result_tuple_1 = await agent.review_and_propose_next_action()
        if action_result_tuple_1:
            proposed_action_1, exec_success_1 = action_result_tuple_1
            print(f"Proposed Action 1: {proposed_action_1}")
            print(f"Execution Success 1: {exec_success_1}")
            acted_insight_1 = next((inst for inst in agent.insights if inst.insight_id == proposed_action_1.get("source_insight_id")), None)
            if acted_insight_1:
                print(f"Insight {acted_insight_1.insight_id} status is now {acted_insight_1.status}")
                assert acted_insight_1.status in ["ACTION_SUCCESSFUL", "ACTION_FAILED", "ACTION_EXCEPTION"], f"Unexpected status: {acted_insight_1.status}"

        action_result_tuple_2 = await agent.review_and_propose_next_action()
        if action_result_tuple_2:
            proposed_action_2, exec_success_2 = action_result_tuple_2
            print(f"Proposed Action 2: {proposed_action_2}")
            print(f"Execution Success 2: {exec_success_2}")
            acted_insight_2 = next((inst for inst in agent.insights if inst.insight_id == proposed_action_2.get("source_insight_id")), None)
            if acted_insight_2:
                print(f"Insight {acted_insight_2.insight_id} status is now {acted_insight_2.status}")
                assert acted_insight_2.status in ["ACTION_SUCCESSFUL", "ACTION_FAILED", "ACTION_EXCEPTION"], f"Unexpected status: {acted_insight_2.status}"

        action_result_tuple_3 = await agent.review_and_propose_next_action()
        assert action_result_tuple_3 is None, f"Expected no action on 3rd attempt, but got {action_result_tuple_3}"
        print("No action proposed on 3rd attempt, as expected.")

        if os.path.exists(test_insights_file):
            print(f"Test file {test_insights_file} can be manually inspected or removed.")

    asyncio.run(run_learning_tests())

# ### END FILE: ai_assistant/learning/learning.py ###

# ### START FILE: ai_assistant/llm_interface/__init__.py ###
# This file marks llm_interface as a package.

# ### END FILE: ai_assistant/llm_interface/__init__.py ###

# ### START FILE: ai_assistant/llm_interface/ollama_client.py ###
# ai_assistant/llm_interface/ollama_client.py
import requests
import json
from typing import Optional, Dict, Union, Tuple
import asyncio
import aiohttp # For asynchronous HTTP requests

from ai_assistant.config import (
    DEFAULT_MODEL as CFG_DEFAULT_MODEL,
    is_debug_mode,
    ENABLE_THINKING,
    THINKING_SUPPORTED_MODELS,
    ENABLE_CHAIN_OF_THOUGHT,
    DEFAULT_TEMPERATURE_THINKING,
    DEFAULT_TEMPERATURE_RESPONSE,
    THINKING_CONFIG
)
from ai_assistant.debugging.resilience import retry_with_backoff # Import the retry decorator

# Define Constants
#OLLAMA_API_ENDPOINT = "http://localhost:11434/api/generate"
#OLLAMA_CHAT_API_ENDPOINT = "http://localhost:11434/api/chat"
OLLAMA_API_ENDPOINT = "http://192.168.86.30:11434/api/generate"
OLLAMA_CHAT_API_ENDPOINT = "http://192.168.86.30:11434/api/chat"
DEFAULT_OLLAMA_MODEL = CFG_DEFAULT_MODEL

# Chain of thought prompt templates
THINKING_PROMPT_TEMPLATE = """You are a highly capable AI assistant with strong analytical and problem-solving abilities. Let's solve this problem step by step.

Original prompt: {user_prompt}

Before providing the final answer, I want you to think through this carefully. Break down your thought process:
1. Understand what's being asked
2. Identify the key elements and requirements
3. Consider potential approaches
4. Plan your response
5. Think about edge cases or potential issues

Do not give the final answer yet. Instead, walk me through your thinking process step by step.
Think it through..."""

RESPONSE_WITH_THINKING_PROMPT_TEMPLATE = """Now that you've thought it through, use your analysis to provide a clear, concise, and accurate response.

Your previous thinking process:
{thinking_process}

Original prompt: {user_prompt}

Provide your final response now, using your thought process to ensure accuracy and completeness."""

def process_llm_response(response_data: Dict) -> Optional[Tuple[str, Optional[str]]]:
    """Process LLM response and extract content and thinking."""
    if not response_data:
        return None
        
    thinking = None
    content = None
    
    if "message" in response_data:
        message = response_data["message"]
        if isinstance(message, dict):
            thinking = message.get("thinking")
            content = message.get("content")
    
    # Fallback to basic response if not in message format
    if content is None:
        content = response_data.get("response", "").strip()

    if not content:
        return None
        
    return (content, thinking)

@retry_with_backoff(retries=3, base_delay=1.0, max_delay=10.0, jitter=True)
def invoke_ollama_model(
    prompt: str,
    model_name: str = DEFAULT_OLLAMA_MODEL,
    temperature: float = 0.7,
    max_tokens: int = 1500
) -> Optional[str]:
    """
    Invokes the specified Ollama model with the given prompt, with retry logic.

    Args:
        prompt: The input prompt for the LLM.
        model_name: The name of the Ollama model to use (e.g., "qwen3:latest").
        temperature: Controls randomness. Lower is more deterministic.
        max_tokens: Maximum number of tokens to generate.

    Returns:
        The LLM's response string, or None if an error occurs.
        When thinking is enabled and supported, the response will include both 
        the thinking process and final answer separated by "...done thinking\\n\\n"
    """
    enable_thinking = ENABLE_THINKING and model_name in THINKING_SUPPORTED_MODELS
    enable_chain_of_thought = ENABLE_CHAIN_OF_THOUGHT and not enable_thinking
    use_chat_api = enable_thinking

    if enable_chain_of_thought:
        # Step 1: Generate thinking process
        thinking_prompt = THINKING_PROMPT_TEMPLATE.format(user_prompt=prompt)
        thinking_payload = {
            "model": model_name,
            "prompt": thinking_prompt,
            "stream": False,
            "options": {
                "temperature": DEFAULT_TEMPERATURE_THINKING,
                "num_predict": max_tokens
            }
        }

        if is_debug_mode():
            print(f"[DEBUG] Chain of thought - Thinking phase starting for model {model_name}")
            print(f"[DEBUG] Thinking prompt: {thinking_prompt[:200]}...")

        try:
            thinking_response = requests.post(OLLAMA_API_ENDPOINT, json=thinking_payload, timeout=600)
            thinking_response.raise_for_status()
            thinking_result = thinking_response.json().get("response", "").strip()
            
            # Display thinking process based on configuration
            if thinking_result:
                if is_debug_mode() and THINKING_CONFIG["display"]["show_working"]:
                    print(f"[DEBUG] {THINKING_CONFIG['display']['prefix'].strip()} {thinking_result} {THINKING_CONFIG['display']['suffix'].strip()}")
                elif not is_debug_mode() and THINKING_CONFIG["display"]["show_in_release"]:
                    print(f"{THINKING_CONFIG['display']['prefix'].strip()} {thinking_result} {THINKING_CONFIG['display']['suffix'].strip()}")
            elif is_debug_mode() and THINKING_CONFIG["display"]["show_working"]:
                 print(f"[DEBUG] CoT: No thinking process generated.")

            # Step 2: Generate final response incorporating the thinking
            # Note: The original prompt is included again here, which is standard for this CoT template.
            response_prompt = RESPONSE_WITH_THINKING_PROMPT_TEMPLATE.format(
                thinking_process=thinking_result,
                user_prompt=prompt
            )

            final_payload = {
                "model": model_name,
                "prompt": response_prompt,
                "stream": False,
                "options": {
                    "temperature": DEFAULT_TEMPERATURE_RESPONSE,
                    "num_predict": max_tokens
                }
            }

            if is_debug_mode():
                print(f"[DEBUG] Chain of thought - Response phase starting")
                print(f"[DEBUG] Response prompt: {response_prompt[:200]}...")

            final_response = requests.post(OLLAMA_API_ENDPOINT, json=final_payload, timeout=600)
            final_response.raise_for_status()
            final_result = final_response.json().get("response", "").strip()

            if is_debug_mode() and THINKING_CONFIG["display"]["show_working"]: # Also show final result in debug
                print(f"[DEBUG] CoT Final Response: {final_result[:200]}...")

            return final_result # Return only the final result for history

        except requests.exceptions.RequestException as e:
            print(f"Error during chain of thought process: {e}")
            return None

    # Regular processing for thinking-supported models or when chain of thought is disabled
    payload = {
        "model": model_name,
        "messages": [{"role": "user", "content": prompt}] if use_chat_api else None,
        "prompt": "" if use_chat_api else prompt,
        "stream": False,
        "options": {
            "temperature": temperature,
            "num_predict": max_tokens
        }
    }

    if use_chat_api:
        payload["think"] = True

    api_endpoint = OLLAMA_CHAT_API_ENDPOINT if use_chat_api else OLLAMA_API_ENDPOINT

    try:
        if is_debug_mode():
            print(f"[DEBUG] Sending request to Ollama with model: {model_name}, prompt: '{prompt[:100]}...'")
            if enable_thinking:
                print(f"[DEBUG] Native thinking enabled for model {model_name}")
        else:
            print(f"Sending request to Ollama with model: {model_name}, prompt: '{prompt[:50]}...'")

        response = requests.post(api_endpoint, json=payload, timeout=600)
        response.raise_for_status()

    except requests.exceptions.HTTPError as e:
        print(f"HTTP error occurred: {e}")
        if is_debug_mode() and e.response is not None:
            print(f"[DEBUG] Status code: {e.response.status_code}")
            try:
                print(f"[DEBUG] Response body: {e.response.json()}")
            except Exception:
                print(f"[DEBUG] Response body could not be parsed as JSON.")
        return None
    except requests.exceptions.RequestException as e:
        print(f"Error invoking Ollama model '{model_name}': {e}")
        print("Please ensure the Ollama service is running and accessible.")
        return None
    except Exception as e:
        print(f"An unexpected error occurred during the request: {e}")
        return None

    try:
        parsed_response = response.json()
        result = process_llm_response(parsed_response)
        
        if not result:
            return None

        content, thinking = result
        
        if enable_thinking: # Only if native thinking is generally enabled
            if thinking: # If the model actually provided thinking
                if is_debug_mode() and THINKING_CONFIG["display"]["show_working"]:
                    print(f"[DEBUG] {THINKING_CONFIG['display']['prefix'].strip()} {thinking} {THINKING_CONFIG['display']['suffix'].strip()}")
                elif not is_debug_mode() and THINKING_CONFIG["display"]["show_in_release"]:
                    print(f"{THINKING_CONFIG['display']['prefix'].strip()} {thinking} {THINKING_CONFIG['display']['suffix'].strip()}")
            elif is_debug_mode() and THINKING_CONFIG["display"]["show_working"]: # Thinking enabled, but model didn't provide it
                print(f"[DEBUG] Native thinking enabled for {model_name}, but no thinking process was returned by the model.")
        
        if is_debug_mode():
            print(f"[DEBUG] Final content being returned: {content[:200]}...")

        return content # Return only the content for history

    except json.JSONDecodeError:
        print("Error: Failed to parse JSON response from Ollama.")
        print(f"Raw response text: {response.text}")
        return None

async def invoke_ollama_model_async(
    prompt: str,
    model_name: str = DEFAULT_OLLAMA_MODEL,
    temperature: float = 0.7,
    max_tokens: int = 1500,
    api_endpoint: str = OLLAMA_API_ENDPOINT
) -> Optional[str]:
    """
    Invokes the specified Ollama model asynchronously with the given prompt, with retry logic.
    When thinking is enabled and supported, the response will include both 
    the thinking process and final answer separated by "...done thinking\\n\\n"
    """
    enable_thinking = ENABLE_THINKING and model_name in THINKING_SUPPORTED_MODELS
    enable_chain_of_thought = ENABLE_CHAIN_OF_THOUGHT and not enable_thinking
    use_chat_api = enable_thinking

    if enable_chain_of_thought:
        # Step 1: Generate thinking process
        thinking_prompt = THINKING_PROMPT_TEMPLATE.format(user_prompt=prompt)
        thinking_payload = {
            "model": model_name,
            "prompt": thinking_prompt,
            "stream": False,
            "options": {
                "temperature": DEFAULT_TEMPERATURE_THINKING,
                "num_predict": max_tokens
            }
        }

        if is_debug_mode():
            print(f"[DEBUG] Chain of thought - Thinking phase starting for model {model_name}")
            print(f"[DEBUG] Thinking prompt: {thinking_prompt[:200]}...")

        async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=600.0)) as session:
            try:
                async with session.post(OLLAMA_API_ENDPOINT, json=thinking_payload) as thinking_response:
                    thinking_response.raise_for_status()
                    thinking_data = await thinking_response.json()
                    thinking_result = thinking_data.get("response", "").strip()
                    
                    if thinking_result:
                        if is_debug_mode() and THINKING_CONFIG["display"]["show_working"]:
                            print(f"[DEBUG] {THINKING_CONFIG['display']['prefix'].strip()} {thinking_result} {THINKING_CONFIG['display']['suffix'].strip()}")
                        elif not is_debug_mode() and THINKING_CONFIG["display"]["show_in_release"]:
                            print(f"{THINKING_CONFIG['display']['prefix'].strip()} {thinking_result} {THINKING_CONFIG['display']['suffix'].strip()}")
                    elif is_debug_mode() and THINKING_CONFIG["display"]["show_working"]:
                        print(f"[DEBUG] Async CoT: No thinking process generated.")
                    # Step 2: Generate final response incorporating the thinking
                    response_prompt = RESPONSE_WITH_THINKING_PROMPT_TEMPLATE.format(
                        thinking_process=thinking_result,
                        user_prompt=prompt
                    )

                    final_payload = {
                        "model": model_name,
                        "prompt": response_prompt,
                        "stream": False,
                        "options": {
                            "temperature": DEFAULT_TEMPERATURE_RESPONSE,
                            "num_predict": max_tokens
                        }
                    }

                    if is_debug_mode():
                        print(f"[DEBUG] Chain of thought - Response phase starting")
                        print(f"[DEBUG] Response prompt: {response_prompt[:200]}...")

                    async with session.post(OLLAMA_API_ENDPOINT, json=final_payload) as final_response:
                        final_response.raise_for_status()
                        final_data = await final_response.json()
                        final_result = final_data.get("response", "").strip()

                        if is_debug_mode() and THINKING_CONFIG["display"]["show_working"]:
                             print(f"[DEBUG] Async CoT Final Response: {final_result[:200]}...")

                        return final_result # Return only the final result for history

            except aiohttp.ClientError as e:
                print(f"HTTP error occurred in async CoT: {e}")
                return None
            except json.JSONDecodeError as e:
                print(f"Error decoding JSON in async CoT: {e}")
                return None
            except Exception as e:
                print(f"An unexpected error occurred in async CoT: {e}")
                return None

    if use_chat_api:
        api_endpoint = OLLAMA_CHAT_API_ENDPOINT

    payload = {
        "model": model_name,
        "messages": [{"role": "user", "content": prompt}] if use_chat_api else None,
        "prompt": "" if use_chat_api else prompt,
        "stream": False,
        "options": {
            "temperature": temperature,
            "num_predict": max_tokens
        }
    }

    if use_chat_api:
        payload["think"] = True

    if is_debug_mode():
        print(f"[DEBUG] Sending async request to Ollama with model: {model_name}, prompt: '{prompt[:100]}...'")
        if enable_thinking:
            print(f"[DEBUG] Native thinking enabled for model {model_name}")
    else:
        print(f"Sending async request to Ollama with model: {model_name}, prompt: '{prompt[:50]}...'")

    async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=600.0)) as session:
        try:
            async with session.post(api_endpoint, json=payload) as response:
                response.raise_for_status()
                response_data = await response.json()

                if is_debug_mode():
                    print(f"[DEBUG] Ollama async response JSON: {str(response_data)[:500]}")

                result = process_llm_response(response_data)
                
                if not result:
                    return None

                content, thinking = result
                
                if enable_thinking: # Only if native thinking is generally enabled
                    if thinking: # If the model actually provided thinking
                        if is_debug_mode() and THINKING_CONFIG["display"]["show_working"]:
                            print(f"[DEBUG] {THINKING_CONFIG['display']['prefix'].strip()} {thinking} {THINKING_CONFIG['display']['suffix'].strip()}")
                        elif not is_debug_mode() and THINKING_CONFIG["display"]["show_in_release"]:
                            print(f"{THINKING_CONFIG['display']['prefix'].strip()} {thinking} {THINKING_CONFIG['display']['suffix'].strip()}")
                    elif is_debug_mode() and THINKING_CONFIG["display"]["show_working"]: # Thinking enabled, but model didn't provide it
                        print(f"[DEBUG] Async native thinking enabled for {model_name}, but no thinking process was returned by the model.")

                if is_debug_mode():
                    print(f"[DEBUG] Async final content being returned: {content[:200]}...")

                return content # Return only the content for history

        except aiohttp.ClientError as e:
            print(f"HTTP error occurred in async call: {e}")
            return None
        except json.JSONDecodeError:
            print("Error: Failed to parse JSON response from Ollama (async).")
            return None
        except Exception as e:
            print(f"An unexpected error occurred during the async request: {e}")
            return None

# Applying the decorator to the async function
invoke_ollama_model_async = retry_with_backoff(retries=3, base_delay=1.0, max_delay=10.0, jitter=True)(invoke_ollama_model_async)


async def main_async_test():
    print("\n--- Testing Asynchronous Ollama Client (with retries) ---")
    print(f"Attempting to invoke model: {DEFAULT_OLLAMA_MODEL} via {OLLAMA_API_ENDPOINT} (async with retries)")
    print("Please ensure your Ollama service is running and the model is available.")
    
    test_prompt = "Why is the sky blue? Explain very concisely using async."
    
    # Simulate failure for retry testing (e.g., by stopping Ollama temporarily)
    # For now, we'll just call it and see if normal operation or retries occur.
    try:
        response = await invoke_ollama_model_async(test_prompt)
        
        if response:
            print(f"\n--- Ollama Response (async) ---")
            print(response)
            print("-----------------------------")
        else:
            print("\n--- Failed to get response from Ollama (async) after retries ---")
            print("Check console for specific errors (HTTP, connection, timeout, etc.).")
            print("Possible reasons:")
            print("1. Ollama service is not running or not accessible at the endpoint.")
            print(f"2. The model '{DEFAULT_OLLAMA_MODEL}' is not available. Try 'ollama pull {DEFAULT_OLLAMA_MODEL}'.")
    except Exception as e:
        print(f"\n--- An error occurred during async test after potential retries: {e} ---")
        print("This might be the final exception after all retries failed.")


if __name__ == '__main__':
    print("--- Testing Ollama Client (with retries) ---")
    # Test synchronous version
    print(f"Attempting to invoke model: {DEFAULT_OLLAMA_MODEL} via {OLLAMA_API_ENDPOINT} (sync with retries)")
    print(f"Please ensure your Ollama service is running and the model is available (e.g., run 'ollama pull {DEFAULT_OLLAMA_MODEL}').")
    
    sync_test_prompt = "Why is the sun hot? Explain concisely."
    
    try:
        sync_response_content = invoke_ollama_model(sync_test_prompt)
        if sync_response_content:
            print("\n--- Ollama Response (sync) ---")
            print(sync_response_content)
            print("-----------------------")
        else:
            print("\n--- Failed to get response from Ollama (sync) after retries ---")
    except Exception as e:
        print(f"\n--- An error occurred during sync test after potential retries: {e} ---")

    # Run asynchronous test
    asyncio.run(main_async_test())

    print("\n--- Ollama Client Test Finished ---")

# ### END FILE: ai_assistant/llm_interface/ollama_client.py ###

# ### START FILE: ai_assistant/main.py ###
import sys
import os
import shutil # For deleting directory contents

# Add the project root directory (which is one level up from the 'ai_assistant' directory)
# to Python's module search path. This allows Python to find the 'ai_assistant' package.
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# Now, your original imports should work correctly
from ai_assistant.communication.cli import start_cli
# Import the consolidated background service management functions
from ai_assistant.core.background_service import start_background_services, stop_background_services
# Import config settings
from ai_assistant.config import CLEAR_EXISTING_KNOWLEDGE_ON_STARTUP, get_data_dir
# Imports for core services (ReflectionLog).
# Their persisted data (e.g., JSON files in the data_dir) is targeted by
# 'clear_knowledge_if_configured', which directly deletes files.
# These classes are likely used by other components initialized via main.py.
from ai_assistant.core.reflection import ReflectionLog # Manages reflection_log.json
import asyncio
import logging

# Setup basic logging configuration for the application
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def clear_knowledge_if_configured():
    """
    Clears stored knowledge (files and subdirectories in the data directory)
    if the CLEAR_EXISTING_KNOWLEDGE_ON_STARTUP flag is set to True.
    """
    if CLEAR_EXISTING_KNOWLEDGE_ON_STARTUP:
        data_dir = get_data_dir() # This function also creates the dir if it doesn't exist
        logger.info(f"CLEAR_EXISTING_KNOWLEDGE_ON_STARTUP is True. Attempting to clear knowledge in: {data_dir}")

        # Ask for user confirmation before proceeding
        confirm = input(f"WARNING: CLEAR_EXISTING_KNOWLEDGE_ON_STARTUP is set to True. "
                        f"This will delete all data in '{data_dir}'.\n"
                        f"Are you sure you want to proceed? (yes/no): ").strip().lower()

        if confirm == 'yes':
            logger.info(f"User confirmed. Proceeding with clearing knowledge in: {data_dir}")
            if os.path.exists(data_dir) and os.path.isdir(data_dir):
                for item_name in os.listdir(data_dir):
                    item_path = os.path.join(data_dir, item_name)
                    try:
                        if os.path.isfile(item_path) or os.path.islink(item_path):
                            os.unlink(item_path)
                            logger.info(f"Deleted file: {item_path}")
                        elif os.path.isdir(item_path):
                            shutil.rmtree(item_path) # Recursively delete directory and its contents
                            logger.info(f"Deleted directory and its contents: {item_path}")
                    except Exception as e:
                        logger.error(f"Failed to delete {item_path}. Reason: {e}")
                logger.info(f"Knowledge clearing process complete for {data_dir}.")
            else:
                logger.warning(f"Data directory {data_dir} was not found or is not a directory. No knowledge to clear.")
        else:
            logger.info("User aborted knowledge clearing process. No data will be deleted.")
            # Optionally, you might want to exit or set CLEAR_EXISTING_KNOWLEDGE_ON_STARTUP to False programmatically
            # For now, it just logs and continues, respecting the original flag for this session if not cleared.
    else:
        logger.info("CLEAR_EXISTING_KNOWLEDGE_ON_STARTUP is False. Skipping knowledge clearing.")

async def async_main_runner():
    """
    Asynchronous main function to orchestrate application startup and shutdown.
    """
    try:
        # Perform knowledge clearing first, if configured
        clear_knowledge_if_configured()

        logger.info("Main: Starting application and background services...")
        # start_background_services() is synchronous but relies on a running event loop
        # to create its async task. asyncio.run() provides this loop.
        start_background_services()

        logger.info("Main: Starting CLI...")
        await start_cli() # start_cli must be an async function
    finally:
        logger.info("Main (async_main_runner finally): Cleaning up background services...")
        await stop_background_services() # stop_background_services is async
        logger.info("Main (async_main_runner finally): Background services cleanup attempt complete.")

if __name__ == "__main__":
    try:
        asyncio.run(async_main_runner())
    except KeyboardInterrupt:
        logger.info("\nMain: Keyboard interrupt received by top-level handler. Application will exit.")
    except Exception as e:
        logger.error(f"Main: An unexpected error occurred at the top level: {e}", exc_info=True)
    finally:
        logger.info("Main: Application shutdown sequence finished.")
# ### END FILE: ai_assistant/main.py ###

# ### START FILE: ai_assistant/memory/__init__.py ###
# This file marks memory as a package.

# ### END FILE: ai_assistant/memory/__init__.py ###

# ### START FILE: ai_assistant/memory/awareness.py ###
# Code for contextual awareness, including tool-goal associations.
from typing import Dict, List, Set

# _tool_goal_links: Maps tool_name to a SET of goal_descriptions for which it was successfully used.
# Using a set for goal_descriptions to automatically handle duplicates.
_tool_goal_links: Dict[str, Set[str]] = {}

def record_tool_goal_association(tool_name: str, goal_description: str) -> None:
    """
    Records that a tool was successfully used to achieve a given goal.
    Ensures that goal_description is unique for the given tool_name.

    Args:
        tool_name: The name of the tool.
        goal_description: The description of the goal achieved.
    """
    if tool_name not in _tool_goal_links:
        _tool_goal_links[tool_name] = set()
    
    # Add the goal description to the set for this tool.
    # Sets automatically handle uniqueness.
    _tool_goal_links[tool_name].add(goal_description)
    print(f"Awareness: Recorded association - Tool: '{tool_name}', Goal: '{goal_description}'")


def get_tool_associations(tool_name: str) -> List[str]:
    """
    Retrieves a list of goal descriptions associated with a tool.

    Args:
        tool_name: The name of the tool.

    Returns:
        A list of unique goal descriptions, or an empty list if none found.
    """
    # Convert set to list for the return type.
    return list(_tool_goal_links.get(tool_name, set()))

if __name__ == '__main__':
    print("--- Testing Tool-Goal Association ---")

    # Test recording
    record_tool_goal_association("add_numbers", "Calculate sum of 5 and 7")
    record_tool_goal_association("add_numbers", "Find total of 10 and 20")
    record_tool_goal_association("add_numbers", "Calculate sum of 5 and 7") # Duplicate, should be ignored by set

    record_tool_goal_association("greet_user", "Greet Alice")

    # Test retrieval
    print("\nAssociations for 'add_numbers':")
    add_assoc = get_tool_associations("add_numbers")
    print(add_assoc)
    assert len(add_assoc) == 2, "Should have 2 unique goals for add_numbers"
    assert "Calculate sum of 5 and 7" in add_assoc
    assert "Find total of 10 and 20" in add_assoc

    print("\nAssociations for 'greet_user':")
    greet_assoc = get_tool_associations("greet_user")
    print(greet_assoc)
    assert len(greet_assoc) == 1
    assert "Greet Alice" in greet_assoc

    print("\nAssociations for 'unknown_tool':")
    unknown_assoc = get_tool_associations("unknown_tool")
    print(unknown_assoc)
    assert len(unknown_assoc) == 0

    # Verify internal state
    print("\nInternal _tool_goal_links state:")
    print(_tool_goal_links)
    assert len(_tool_goal_links["add_numbers"]) == 2

    print("\n--- Tool-Goal Association Tests Finished ---")

# ### END FILE: ai_assistant/memory/awareness.py ###

# ### START FILE: ai_assistant/memory/event_logger.py ###
import json
import os
import uuid
from datetime import datetime, timezone
from typing import Dict, Any, List, Optional

from ai_assistant.config import get_data_dir # Import the centralized function

EVENT_LOG_FILENAME = "event_log.json"
EVENT_LOG_FILE = os.path.join(get_data_dir(), EVENT_LOG_FILENAME) # Use the centralized data directory
MAX_LOG_ENTRIES_IN_MEMORY = 100 # For get_recent_events, not a hard limit on file size

# Lock for file operations if we were in a threaded environment.
# For CLI, less critical but good to be aware of potential race conditions
# if the app becomes more complex. For now, direct file read/write is fine.
# import threading
# _file_lock = threading.Lock()

def _ensure_log_dir_exists():
    """Ensures the log directory exists."""
    # get_data_dir() called when EVENT_LOG_FILE is defined already ensures the base data directory exists.
    # This function can ensure the specific directory for the log file exists if it were in a subdirectory
    # of get_data_dir(), but since it's directly in get_data_dir(), this call is mostly redundant
    # but harmless with exist_ok=True.
    os.makedirs(os.path.dirname(EVENT_LOG_FILE), exist_ok=True)

def log_event(
    event_type: str,
    description: str,
    source: str,
    metadata: Optional[Dict[str, Any]] = None,
    correlation_id: Optional[str] = None
) -> Dict[str, Any]:
    """
    Logs a structured event to the event_log.json file.

    Args:
        event_type: Enum-like string for the type of event.
        description: Human-readable summary of the event.
        source: Module or component name where the event originated.
        metadata: Optional dictionary for event-specific data.
        correlation_id: Optional UUID string to link related events.

    Returns:
        The event dictionary that was logged.
    """
    _ensure_log_dir_exists()

    event_id = uuid.uuid4().hex
    timestamp = datetime.now(timezone.utc).isoformat()
    
    event = {
        "timestamp": timestamp,
        "event_id": event_id,
        "event_type": event_type,
        "description": description,
        "source": source,
        "metadata": metadata if metadata is not None else {},
    }
    if correlation_id:
        event["correlation_id"] = correlation_id

    # with _file_lock: # If threading becomes a concern
    try:
        if os.path.exists(EVENT_LOG_FILE):
            with open(EVENT_LOG_FILE, 'r', encoding='utf-8') as f:
                try:
                    # Handle empty or malformed file
                    content = f.read()
                    if not content:
                        events = []
                    else:
                        events = json.loads(content)
                    if not isinstance(events, list): # Ensure it's a list, not some other JSON type
                        print(f"Warning: Event log file '{EVENT_LOG_FILE}' contained non-list data. Reinitializing.")
                        events = []
                except json.JSONDecodeError:
                    print(f"Warning: Could not decode JSON from '{EVENT_LOG_FILE}'. Reinitializing event log.")
                    events = [] # If file is corrupted, start fresh to avoid losing new logs
        else:
            events = []

        events.append(event)

        with open(EVENT_LOG_FILE, 'w', encoding='utf-8') as f: # Corrected typo here
            json.dump(events, f, indent=4, ensure_ascii=False)
            
        # Optional: print a confirmation or a summary of the logged event for debugging
        # print(f"Event logged ({event_type}): {description[:50]}...")

    except IOError as e:
        print(f"IOError logging event to {EVENT_LOG_FILE}: {e}")
    except Exception as e:
        print(f"Unexpected error logging event: {e}")
        
    return event # Return the created event, even if logging failed, for potential in-memory use by caller

def get_recent_events(limit: int = MAX_LOG_ENTRIES_IN_MEMORY) -> List[Dict[str, Any]]:
    """
    Retrieves a list of the most recent events from the log file.

    Args:
        limit: The maximum number of recent events to return.

    Returns:
        A list of event dictionaries, newest first. Returns empty if log is empty or error.
    """
    # with _file_lock: # If threading
    try:
        if not os.path.exists(EVENT_LOG_FILE):
            return []
        with open(EVENT_LOG_FILE, 'r', encoding='utf-8') as f:
            try:
                events = json.load(f)
                if not isinstance(events, list):
                     return [] # Or handle error
            except json.JSONDecodeError:
                return [] # Or handle error
        
        # Return last 'limit' events, newest first (assuming append-only log)
        return events[-limit:][::-1] 
    except IOError as e:
        print(f"IOError reading event log {EVENT_LOG_FILE}: {e}")
        return []
    except Exception as e:
        print(f"Unexpected error reading event log: {e}")
        return []

if __name__ == '__main__':
    print("--- Testing Event Logger ---")
    
    # Clean up old log file for fresh test
    if os.path.exists(EVENT_LOG_FILE):
        os.remove(EVENT_LOG_FILE)

    # Log some sample events
    event1_meta = {"tool_name": "test_tool", "version": "1.0"}
    ev1 = log_event("TOOL_REGISTERED_MANUAL", "Test tool registered by user.", "cli.py", event1_meta)
    print(f"Logged event 1: {ev1.get('event_id')}")

    event2_meta = {"goal_id": "g123", "status": "completed"}
    ev2 = log_event("GOAL_STATUS_UPDATED", "Goal status changed.", "goal_management.py", event2_meta)
    print(f"Logged event 2: {ev2.get('event_id')}")
    
    ev3 = log_event("USER_INTERACTION", "User provided input.", "cli.py", {"input_length": 20})
    print(f"Logged event 3: {ev3.get('event_id')}")

    # Retrieve recent events
    print("\n--- Recent Events ---")
    recent = get_recent_events(limit=2)
    for item in recent:
        print(f"  {item['timestamp']} - {item['event_type']}: {item['description']}")
    
    assert len(recent) == 2, f"Expected 2 recent events, got {len(recent)}"
    if recent: # Check if recent is not empty before indexing
        assert recent[0]['event_id'] == ev3['event_id'], "Events not in newest-first order or wrong event"

    # Check full log content
    if os.path.exists(EVENT_LOG_FILE):
        with open(EVENT_LOG_FILE, 'r') as f:
            all_logged_events = json.load(f)
        print(f"\nTotal events in log file: {len(all_logged_events)}")
        assert len(all_logged_events) == 3, "Expected 3 events in the log file."
        assert all_logged_events[0]['event_id'] == ev1['event_id']
        assert all_logged_events[1]['event_id'] == ev2['event_id']
        assert all_logged_events[2]['event_id'] == ev3['event_id']
    else:
        print("ERROR: Log file not found after logging events.")
        assert False, "Log file not created."

    print("\n--- Testing with empty/corrupted log file (before next logs) ---")
    # Simulate corrupted log
    with open(EVENT_LOG_FILE, 'w') as f:
        f.write("this is not json")
    
    corrupt_read = get_recent_events(1)
    assert corrupt_read == [], f"Expected empty list from corrupted log, got {corrupt_read}"
    
    # Log another event - should reinitialize due to corruption
    ev4 = log_event("SYSTEM_WARNING", "Log file was corrupted, reinitialized.", "event_logger.py")
    print(f"Logged event 4 (after corruption): {ev4.get('event_id')}")
    
    recent_after_corruption = get_recent_events(5)
    assert len(recent_after_corruption) == 1, "Expected 1 event after reinitialization"
    if recent_after_corruption:
        assert recent_after_corruption[0]['event_id'] == ev4['event_id']
    
    print(f"Log content after reinit: {recent_after_corruption}")


    print("\n--- Event Logger Tests Finished ---")

# ### END FILE: ai_assistant/memory/event_logger.py ###

# ### START FILE: ai_assistant/memory/persistent_memory.py ###
# Code for persistent memory management.
import json
import os
from typing import Dict, Any, List
import datetime # Added for __main__ tests for ActionableInsights

from ai_assistant.config import get_data_dir # Import the centralized function

def save_goals_to_file(filepath: str, goals_db: Dict[str, Any]) -> bool:
    """
    Serializes the goals_db to JSON and writes it to the specified file.

    Args:
        filepath: The path to the JSON file (e.g., "data/goals.json").
        goals_db: The dictionary of goals to save.

    Returns:
        True on success, False on error.
    """
    try:
        # Ensure the directory exists
        dir_path = os.path.dirname(filepath)
        if dir_path: # Only create if there is a directory part
            os.makedirs(dir_path, exist_ok=True)
            
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(goals_db, f, indent=4, ensure_ascii=False)
        # print(f"Successfully saved goals to {filepath}") # CLI will provide user feedback
        return True
    except IOError as e:
        print(f"IOError saving goals to {filepath}: {e}")
        return False
    except TypeError as e: # For issues with non-serializable content in goals_db
        print(f"TypeError during JSON serialization for {filepath}: {e}")
        return False
    except Exception as e:
        print(f"Unexpected error saving goals to {filepath}: {e}")
        return False

def load_goals_from_file(filepath: str) -> Dict[str, Any]:
    """
    Reads JSON data from the file and deserializes it into a dictionary.

    Args:
        filepath: The path to the JSON file.

    Returns:
        The loaded dictionary of goals. Returns an empty dictionary if the file
        doesn't exist, is invalid JSON, or another error occurs.
    """
    if not os.path.exists(filepath):
        # print(f"Info: Goals file '{filepath}' not found. Starting with an empty goals database.") # Handled by caller
        return {}
        
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            goals_db = json.load(f)
        # print(f"Successfully loaded goals from {filepath}") # CLI will provide user feedback
        return goals_db
    except FileNotFoundError: 
        # print(f"Info: Goals file '{filepath}' not found. Starting with an empty goals database.") # Handled by caller
        return {}
    except json.JSONDecodeError as e:
        print(f"JSONDecodeError loading goals from {filepath}: {e}. Returning empty goals database.")
        return {}
    except IOError as e:
        print(f"IOError loading goals from {filepath}: {e}. Returning empty goals database.")
        return {}
    except Exception as e:
        print(f"Unexpected error loading goals from {filepath}: {e}. Returning empty goals database.")
        return {}

# --- Learned Facts Persistence Functions ---

LEARNED_FACTS_FILENAME = "learned_facts.json"
LEARNED_FACTS_FILEPATH = os.path.join(get_data_dir(), LEARNED_FACTS_FILENAME)

ACTIONABLE_INSIGHTS_FILENAME = "actionable_insights.json"
ACTIONABLE_INSIGHTS_FILEPATH = os.path.join(get_data_dir(), ACTIONABLE_INSIGHTS_FILENAME)


def save_learned_facts(facts: list[str], filepath: str = LEARNED_FACTS_FILEPATH) -> bool:
    """
    Serializes the list of learned facts to JSON and writes it to the specified file.

    Args:
        facts: The list of learned facts (strings).
        filepath: The path to the JSON file. Defaults to LEARNED_FACTS_FILEPATH.

    Returns:
        True on success, False on error.
    """
    try:
        # Ensure the directory exists
        dir_path = os.path.dirname(filepath)
        if dir_path: # Only create if there is a directory part
            os.makedirs(dir_path, exist_ok=True)
            
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(facts, f, indent=4, ensure_ascii=False)
        return True
    except IOError as e:
        print(f"IOError saving learned facts to {filepath}: {e}")
        return False
    except TypeError as e: # For issues with non-serializable content
        print(f"TypeError during JSON serialization for learned facts at {filepath}: {e}")
        return False
    except Exception as e:
        print(f"Unexpected error saving learned facts to {filepath}: {e}")
        return False

def load_learned_facts(filepath: str = LEARNED_FACTS_FILEPATH) -> list[str]:
    """
    Reads JSON data from the file and deserializes it into a list of learned facts.

    Args:
        filepath: The path to the JSON file. Defaults to LEARNED_FACTS_FILEPATH.

    Returns:
        The loaded list of facts. Returns an empty list if the file
        doesn't exist, is invalid JSON, or another error occurs.
    """
    if not os.path.exists(filepath):
        return []
        
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            facts = json.load(f)
            if not isinstance(facts, list) or not all(isinstance(fact, str) for fact in facts):
                print(f"Warning: Data in '{filepath}' is not a list of strings. Returning empty list.")
                return []
        return facts
    except FileNotFoundError: 
        return []
    except json.JSONDecodeError as e:
        print(f"JSONDecodeError loading learned facts from {filepath}: {e}. Returning empty list.")
        return []
    except IOError as e:
        print(f"IOError loading learned facts from {filepath}: {e}. Returning empty list.")
        return []
    except Exception as e:
        print(f"Unexpected error loading learned facts from {filepath}: {e}. Returning empty list.")
        return []

# --- Actionable Insights Persistence Functions --- Added section
def save_actionable_insights(insights: List[Dict[str, Any]], filepath: str = ACTIONABLE_INSIGHTS_FILEPATH) -> bool:
    """
    Serializes a list of ActionableInsight objects (as dictionaries) to JSON and writes to file.

    Args:
        insights: A list of dictionaries, where each dict is a serializable ActionableInsight.
        filepath: The path to the JSON file. Defaults to ACTIONABLE_INSIGHTS_FILEPATH.

    Returns:
        True on success, False on error.
    """
    try:
        dir_path = os.path.dirname(filepath)
        if dir_path:
            os.makedirs(dir_path, exist_ok=True)

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(insights, f, indent=4, ensure_ascii=False)
        return True
    except IOError as e: # pragma: no cover
        print(f"IOError saving actionable insights to {filepath}: {e}")
        return False
    except TypeError as e: # pragma: no cover
        print(f"TypeError during JSON serialization for actionable insights at {filepath}: {e}")
        return False
    except Exception as e: # pragma: no cover
        print(f"Unexpected error saving actionable insights to {filepath}: {e}")
        return False

def load_actionable_insights(filepath: str = ACTIONABLE_INSIGHTS_FILEPATH) -> List[Dict[str, Any]]:
    """
    Reads a list of ActionableInsight objects (as dictionaries) from a JSON file.

    Args:
        filepath: The path to the JSON file. Defaults to ACTIONABLE_INSIGHTS_FILEPATH.

    Returns:
        The loaded list of dictionaries. Returns an empty list if the file
        doesn't exist, is invalid JSON, or another error occurs.
    """
    if not os.path.exists(filepath):
        return []

    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
            if not content.strip(): # Handle empty file
                return []
            loaded_insights = json.loads(content)
        if not isinstance(loaded_insights, list): # pragma: no cover
            print(f"Warning: Data in actionable insights file '{filepath}' is not a list. Returning empty list.")
            return []
        return loaded_insights
    except FileNotFoundError: # pragma: no cover
        return []
    except json.JSONDecodeError as e: # pragma: no cover
        print(f"JSONDecodeError loading actionable insights from {filepath}: {e}. Returning empty list.")
        return []
    except IOError as e: # pragma: no cover
        print(f"IOError loading actionable insights from {filepath}: {e}. Returning empty list.")
        return []
    except Exception as e: # pragma: no cover
        print(f"Unexpected error loading actionable insights from {filepath}: {e}. Returning empty list.")
        return []

if __name__ == '__main__':
    print("--- Testing Persistent Memory for Goals ---")
    TEST_FILE_DIR = "test_data_pm" 
    TEST_GOALS_FILE = os.path.join(TEST_FILE_DIR, "test_goals.json") # Renamed for clarity

    # Ensure the test directory is clean before starting
    # Simplified cleanup for tests - more robust cleanup might be needed for complex scenarios
    if os.path.exists(TEST_GOALS_FILE):
        os.remove(TEST_GOALS_FILE)
    # Clean up other specific test files if they exist
    _test_invalid_goals_file = os.path.join(TEST_FILE_DIR, "invalid_goals.json")
    if os.path.exists(_test_invalid_goals_file):
        os.remove(_test_invalid_goals_file)
    # Attempt to remove the directory if empty
    if os.path.exists(TEST_FILE_DIR) and not os.listdir(TEST_FILE_DIR):
        try:
            os.rmdir(TEST_FILE_DIR)
        except OSError:
            print(f"Warning: Could not remove test directory {TEST_FILE_DIR} as it might not be empty or in use.")
    elif not os.path.exists(TEST_FILE_DIR):
        os.makedirs(TEST_FILE_DIR, exist_ok=True)


    sample_goals = {
        "goal1": {"id": "goal1", "description": "Test saving goals", "status": "pending", "priority": 1},
        "goal2": {"id": "goal2", "description": "Test loading goals", "status": "in_progress", "priority": 2},
    }

    # Test saving
    print(f"\nAttempting to save sample goals to {TEST_GOALS_FILE}...")
    save_success_goals = save_goals_to_file(TEST_GOALS_FILE, sample_goals)
    assert save_success_goals, "save_goals_to_file failed during test."
    print(f"Save operation result for goals: {save_success_goals}")
    assert os.path.exists(TEST_GOALS_FILE), f"File {TEST_GOALS_FILE} was not created after save."

    # Test loading
    print(f"\nAttempting to load goals from {TEST_GOALS_FILE}...")
    loaded_goals = load_goals_from_file(TEST_GOALS_FILE)
    assert loaded_goals == sample_goals, "Loaded goals do not match saved goals."
    print(f"Loaded goals: {loaded_goals}")

    # Test loading non-existent file
    print("\nAttempting to load goals from a non-existent file...")
    NON_EXISTENT_GOALS_FILE = os.path.join(TEST_FILE_DIR, "non_existent_goals.json")
    loaded_empty_goals = load_goals_from_file(NON_EXISTENT_GOALS_FILE)
    assert loaded_empty_goals == {}, "Loading non-existent goals file did not return an empty dict."
    print(f"Result from loading non-existent goals file: {loaded_empty_goals} (should be empty)")

    # Test loading invalid JSON
    print("\nAttempting to load goals from an invalid JSON file...")
    INVALID_JSON_GOALS_FILE = os.path.join(TEST_FILE_DIR, "invalid_goals.json")
    os.makedirs(os.path.dirname(INVALID_JSON_GOALS_FILE), exist_ok=True) 
    with open(INVALID_JSON_GOALS_FILE, 'w') as f:
        f.write("This is not valid JSON {")
    loaded_invalid_goals = load_goals_from_file(INVALID_JSON_GOALS_FILE)
    assert loaded_invalid_goals == {}, "Loading invalid JSON for goals did not return an empty dict."
    print(f"Result from loading invalid JSON goals file: {loaded_invalid_goals} (should be empty)")


    # Cleanup for goals tests
    print("\nCleaning up goals test files...")
    if os.path.exists(TEST_GOALS_FILE):
        os.remove(TEST_GOALS_FILE)
    if os.path.exists(INVALID_JSON_GOALS_FILE):
        os.remove(INVALID_JSON_GOALS_FILE)
    # Attempt to remove the directory if empty - this might be shared with other tests below
    # So, cleanup of TEST_FILE_DIR itself is deferred until the very end of __main__

    print("\n--- Goals Persistent Memory Tests Finished ---")

    print("\n--- Testing Persistent Memory for Learned Facts ---")
    TEST_FACTS_FILE = os.path.join(TEST_FILE_DIR, "test_learned_facts.json")
    _test_invalid_facts_file = os.path.join(TEST_FILE_DIR, "invalid_facts.json")

    sample_facts = ["fact1: the sky is blue", "fact2: elephants are large", "fact3: Python is a programming language"]

    # Test saving facts
    print(f"\nAttempting to save sample facts to {TEST_FACTS_FILE}...")
    save_success_facts = save_learned_facts(sample_facts, TEST_FACTS_FILE)
    assert save_success_facts, "save_learned_facts failed during test."
    print(f"Save operation result for facts: {save_success_facts}")
    assert os.path.exists(TEST_FACTS_FILE), f"File {TEST_FACTS_FILE} was not created after save."

    # Test loading facts
    print(f"\nAttempting to load facts from {TEST_FACTS_FILE}...")
    loaded_facts = load_learned_facts(TEST_FACTS_FILE)
    assert loaded_facts == sample_facts, "Loaded facts do not match saved facts."
    print(f"Loaded facts: {loaded_facts}")

    # Test loading facts from a non-existent file
    print("\nAttempting to load facts from a non-existent file...")
    NON_EXISTENT_FACTS_FILE = os.path.join(TEST_FILE_DIR, "non_existent_facts.json")
    loaded_empty_facts = load_learned_facts(NON_EXISTENT_FACTS_FILE)
    assert loaded_empty_facts == [], "Loading non-existent facts file did not return an empty list."
    print(f"Result from loading non-existent facts file: {loaded_empty_facts} (should be empty list)")

    # Test loading facts from an invalid JSON file
    print("\nAttempting to load facts from an invalid JSON file...")
    INVALID_JSON_FACTS_FILE = os.path.join(TEST_FILE_DIR, "invalid_facts.json")
    os.makedirs(os.path.dirname(INVALID_JSON_FACTS_FILE), exist_ok=True) 
    with open(INVALID_JSON_FACTS_FILE, 'w') as f:
        f.write("This is not a valid JSON list of strings") # Malformed JSON
    loaded_invalid_facts = load_learned_facts(INVALID_JSON_FACTS_FILE)
    assert loaded_invalid_facts == [], "Loading invalid JSON for facts did not return an empty list."
    print(f"Result from loading invalid JSON facts file: {loaded_invalid_facts} (should be empty list)")

    # Test loading facts from a file with correct JSON type but incorrect inner type (e.g. list of dicts)
    print("\nAttempting to load facts from a file with list of dicts (should fail type check)...")
    INVALID_TYPE_FACTS_FILE = os.path.join(TEST_FILE_DIR, "invalid_type_facts.json")
    with open(INVALID_TYPE_FACTS_FILE, 'w') as f:
        json.dump([{"fact": "is_a_dict"}, {"fact": "not_a_string"}], f)
    loaded_invalid_type_facts = load_learned_facts(INVALID_TYPE_FACTS_FILE)
    assert loaded_invalid_type_facts == [], "Loading facts with invalid inner type did not return an empty list."
    print(f"Result from loading facts with invalid inner type: {loaded_invalid_type_facts} (should be empty list)")


    # Cleanup for facts tests
    print("\nCleaning up facts test files...")
    if os.path.exists(TEST_FACTS_FILE):
        os.remove(TEST_FACTS_FILE)
    if os.path.exists(INVALID_JSON_FACTS_FILE):
        os.remove(INVALID_JSON_FACTS_FILE)
    if os.path.exists(INVALID_TYPE_FACTS_FILE):
        os.remove(INVALID_TYPE_FACTS_FILE) # pragma: no cover
    print("--- Learned Facts Persistent Memory Tests Finished ---")

    # --- Testing Persistent Memory for Actionable Insights ---
    print("\n--- Testing Persistent Memory for Actionable Insights ---")
    TEST_INSIGHTS_FILE = os.path.join(TEST_FILE_DIR, "test_actionable_insights.json")
    INVALID_JSON_INSIGHTS_FILE = os.path.join(TEST_FILE_DIR, "invalid_insights.json")
    NON_EXISTENT_INSIGHTS_FILE = os.path.join(TEST_FILE_DIR, "non_existent_insights.json")

    if os.path.exists(TEST_INSIGHTS_FILE): os.remove(TEST_INSIGHTS_FILE) # pragma: no cover
    if os.path.exists(INVALID_JSON_INSIGHTS_FILE): os.remove(INVALID_JSON_INSIGHTS_FILE) # pragma: no cover

    sample_insights_data = [
        {
            "insight_id": "TOOL_BUG_SUSPECTED_test1", "type": "TOOL_BUG_SUSPECTED",
            "description": "Tool X failed with ValueError", "source_reflection_entry_ids": ["entry1"],
            "related_tool_name": "ToolX", "priority": 3, "status": "NEW",
            "creation_timestamp": datetime.datetime.now(datetime.timezone.utc).isoformat(), "metadata": {}
        },
        {
            "insight_id": "KNOWLEDGE_GAP_IDENTIFIED_test2", "type": "KNOWLEDGE_GAP_IDENTIFIED",
            "description": "Agent needs to learn about topic Y", "source_reflection_entry_ids": ["entry2"],
            "knowledge_to_learn": "Topic Y is important.", "priority": 5, "status": "NEW",
            "creation_timestamp": datetime.datetime.now(datetime.timezone.utc).isoformat(), "metadata": {}
        }
    ]

    print(f"\nAttempting to save {len(sample_insights_data)} sample insights to {TEST_INSIGHTS_FILE}...")
    assert save_actionable_insights(sample_insights_data, TEST_INSIGHTS_FILE), "save_actionable_insights failed."
    print(f"Save operation for insights successful.")
    assert os.path.exists(TEST_INSIGHTS_FILE), f"{TEST_INSIGHTS_FILE} was not created."

    print(f"\nAttempting to load insights from {TEST_INSIGHTS_FILE}...")
    loaded_insights = load_actionable_insights(TEST_INSIGHTS_FILE)
    assert loaded_insights == sample_insights_data, "Loaded insights do not match saved insights."
    print(f"Loaded {len(loaded_insights)} insights successfully.")

    print("\nAttempting to load insights from a non-existent file...")
    loaded_empty_insights = load_actionable_insights(NON_EXISTENT_INSIGHTS_FILE)
    assert loaded_empty_insights == [], "Loading non-existent insights file did not return an empty list."
    print(f"Loading non-existent insights file test successful: {loaded_empty_insights}")

    print("\nAttempting to load insights from an invalid JSON file...")
    with open(INVALID_JSON_INSIGHTS_FILE, 'w') as f: f.write("This is not valid JSON [")
    loaded_invalid_insights = load_actionable_insights(INVALID_JSON_INSIGHTS_FILE)
    assert loaded_invalid_insights == [], "Loading invalid JSON for insights did not return an empty list."
    print(f"Loading invalid JSON insights file test successful: {loaded_invalid_insights}")

    if os.path.exists(TEST_INSIGHTS_FILE): os.remove(TEST_INSIGHTS_FILE) # pragma: no cover
    if os.path.exists(INVALID_JSON_INSIGHTS_FILE): os.remove(INVALID_JSON_INSIGHTS_FILE) # pragma: no cover
    print("--- Actionable Insights Persistent Memory Tests Finished ---")
    
    # General cleanup for the test directory at the very end
    print("\nFinal cleanup of test directory...")
    # Clean up any other specific test files that might have been missed if tests failed early
    if os.path.exists(TEST_GOALS_FILE): os.remove(TEST_GOALS_FILE) # pragma: no cover
    if os.path.exists(_test_invalid_goals_file): os.remove(_test_invalid_goals_file) # pragma: no cover
    if os.path.exists(TEST_FACTS_FILE): os.remove(TEST_FACTS_FILE) # pragma: no cover

    if os.path.exists(_test_invalid_facts_file): os.remove(_test_invalid_facts_file)
    if os.path.exists(INVALID_TYPE_FACTS_FILE): os.remove(INVALID_TYPE_FACTS_FILE) # pragma: no cover
    if os.path.exists(TEST_INSIGHTS_FILE): os.remove(TEST_INSIGHTS_FILE) # pragma: no cover
    if os.path.exists(INVALID_JSON_INSIGHTS_FILE): os.remove(INVALID_JSON_INSIGHTS_FILE) # pragma: no cover

    if os.path.exists(TEST_FILE_DIR) and not os.listdir(TEST_FILE_DIR): # pragma: no cover
        try:
            os.rmdir(TEST_FILE_DIR)
            print(f"Test directory {TEST_FILE_DIR} removed.")
        except OSError: # pragma: no cover
            print(f"Warning: Could not remove test directory {TEST_FILE_DIR}. It might not be empty or is in use.")
    elif os.path.exists(TEST_FILE_DIR): # pragma: no cover
        remaining_files = os.listdir(TEST_FILE_DIR)
        print(f"Warning: Test directory {TEST_FILE_DIR} still contains files: {remaining_files}. Manual cleanup may be needed.")
    
    print("\n--- All Persistent Memory Tests Finished ---")
    
    

# --- Tool Persistence Functions ---

def save_tools_to_file(filepath: str, tool_registry_data: Dict[str, Dict[str, Any]]) -> bool:
    """
    Serializes the tool_registry_data (metadata only) to JSON and writes it to file.

    Args:
        filepath: The path to the JSON file (e.g., "data/tools.json").
        tool_registry_data: The dictionary of tool metadata to save.
                            Assumes 'callable_cache' has been removed.

    Returns:
        True on success, False on error.
    """
    try:
        dir_path = os.path.dirname(filepath)
        if dir_path:
            os.makedirs(dir_path, exist_ok=True)
            
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(tool_registry_data, f, indent=4, ensure_ascii=False)
        # print(f"Successfully saved tools to {filepath}") # Feedback handled by caller
        return True
    except IOError as e:
        print(f"IOError saving tools to {filepath}: {e}")
        return False
    except TypeError as e:
        print(f"TypeError during JSON serialization for tools at {filepath}: {e}")
        return False
    except Exception as e:
        print(f"Unexpected error saving tools to {filepath}: {e}")
        return False

def load_tools_from_file(filepath: str) -> Dict[str, Dict[str, Any]]:
    """
    Reads tool metadata JSON from the file and deserializes it.

    Args:
        filepath: The path to the JSON file.

    Returns:
        The loaded dictionary of tool metadata. Returns an empty dictionary
        if the file doesn't exist, is invalid JSON, or another error occurs.
    """
    if not os.path.exists(filepath):
        # print(f"Info: Tools file '{filepath}' not found. Starting with no persisted tools.") # Feedback by caller
        return {}
        
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            tool_data = json.load(f)
        # print(f"Successfully loaded tools from {filepath}") # Feedback by caller
        return tool_data
    except FileNotFoundError:
        # print(f"Info: Tools file '{filepath}' not found.") # Feedback by caller
        return {}
    except json.JSONDecodeError as e:
        print(f"JSONDecodeError loading tools from {filepath}: {e}. Returning empty toolset.")
        return {}
    except IOError as e:
        print(f"IOError loading tools from {filepath}: {e}. Returning empty toolset.")
        return {}
    except Exception as e:
        print(f"Unexpected error loading tools from {filepath}: {e}. Returning empty toolset.")
        return {}

REFLECTION_LOG_FILENAME = "reflection_log.json"
REFLECTION_LOG_FILEPATH = os.path.join(get_data_dir(), REFLECTION_LOG_FILENAME)

def save_reflection_log_entries(filepath: str, entries_as_dicts: List[Dict[str, Any]]) -> bool:
    """
    Serializes a list of reflection log entries (as dictionaries) to JSON and writes to file.

    Args:
        filepath: The path to the JSON file.
        entries_as_dicts: A list of dictionaries, where each dict is a serializable reflection log entry.

    Returns:
        True on success, False on error.
    """
    try:
        dir_path = os.path.dirname(filepath)
        if dir_path:
            os.makedirs(dir_path, exist_ok=True)

        temp_filepath = filepath + ".tmp"
        with open(temp_filepath, 'w', encoding='utf-8') as f:
            json.dump(entries_as_dicts, f, indent=4, ensure_ascii=False)

        # Atomically replace the old file with the new one
        try:
            os.replace(temp_filepath, filepath)
        except OSError:  # Fallback for systems where os.replace might not be atomic (e.g., some network filesystems)
            os.remove(filepath) # pragma: no cover
            os.rename(temp_filepath, filepath) # pragma: no cover

        return True

    except IOError as e:
        print(f"IOError saving reflection log to {filepath}: {e}")
        return False
    except TypeError as e:
        print(f"TypeError during JSON serialization for reflection log at {filepath}: {e}")
        return False
    except Exception as e:
        print(f"Unexpected error saving reflection log to {filepath}: {e}")
        return False

def load_reflection_log_entries(filepath: str) -> List[Dict[str, Any]]:
    """
    Reads a list of reflection log entries (as dictionaries) from a JSON file.

    Args:
        filepath: The path to the JSON file.

    Returns:
        The loaded list of dictionaries. Returns an empty list if the file
        doesn't exist, is invalid JSON, or another error occurs.
    """
    if not os.path.exists(filepath):
        return []
        
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
            if not content.strip(): # Handle empty file
                return []
            loaded_entries = json.loads(content) # Use json.loads on the read content
            if not isinstance(loaded_entries, list):
                print(f"Warning: Data in reflection log '{filepath}' is not a list. Returning empty log.")
                return []
            # Further validation could ensure each item is a dict, but ReflectionLog.from_dict will handle that.
        # print(f"Successfully loaded reflection log from {filepath}") # Optional: for debugging
        return loaded_entries
    except FileNotFoundError: 
        return []
    except json.JSONDecodeError as e:
        print(f"JSONDecodeError loading reflection log from {filepath}: {e}. Returning empty log.")
        return []
    except IOError as e:
        print(f"IOError loading reflection log from {filepath}: {e}. Returning empty log.")
        return []
    except Exception as e:
        print(f"Unexpected error loading reflection log from {filepath}: {e}. Returning empty log.")
        return []
# ### END FILE: ai_assistant/memory/persistent_memory.py ###

# ### START FILE: ai_assistant/planning/__init__.py ###
# This file marks planning as a package.

# ### END FILE: ai_assistant/planning/__init__.py ###

# ### START FILE: ai_assistant/planning/execution.py ###
# Code for task execution.
import traceback
from typing import List, Dict, Any, Optional, Tuple
import ai_assistant.tools.tool_system as ts_module_type
import re
import asyncio
from ..core.reflection import global_reflection_log, analyze_last_failure
from ai_assistant.memory.awareness import record_tool_goal_association
from ai_assistant.memory.event_logger import log_event
from ai_assistant.learning.learning import LearningAgent # Import LearningAgent
from ai_assistant.planning.planning import PlannerAgent # Added for re-planning

class ExecutionAgent:
    """
    Responsible for executing a plan (a sequence of tool invocations)
    generated by the PlannerAgent.
    """
    MAX_RETRIES_PER_STEP = 1 # Results in 1 initial attempt + 1 retry = 2 total attempts
    MAX_REPLAN_ATTEMPTS = 1  # Max number of times to attempt re-planning

    async def execute_plan(
        self, 
        goal_description: str, 
        initial_plan: List[Dict[str, Any]], 
        tool_system: Any,
        planner_agent: PlannerAgent, # Added for re-planning
        learning_agent: LearningAgent, # Added LearningAgent
        ollama_model_name: Optional[str] = None
    ) -> Tuple[List[Dict[str, Any]], List[Any]]: # MODIFIED: Return final plan and its results
        """
        Executes each step in the provided plan with retries and logs the execution.
        Includes logic for re-planning if critical failures occur.
        """
        current_plan = list(initial_plan) # Make a mutable copy
        replan_attempts = 0

        while replan_attempts <= self.MAX_REPLAN_ATTEMPTS:
            if not current_plan:
                print("ExecutionAgent: Plan is empty. Nothing to execute.")
                if replan_attempts == 0: # Only log empty plan if it was the initial plan
                    reflection_entry_obj_empty = global_reflection_log.log_execution(
                        goal_description=goal_description, plan=current_plan, execution_results=[],
                        overall_success=False, notes="Initial plan was empty."
                    )
                    if learning_agent:
                        learning_agent.process_reflection_entry(reflection_entry_obj_empty) # type: ignore
                return [], [] # MODIFIED: Return empty plan and results

            print(f"\nExecutionAgent: Starting execution of {'re-plan' if replan_attempts > 0 else 'plan'} for goal '{goal_description}' (Attempt {replan_attempts + 1}) with {len(current_plan)} steps.")
            
            # Reset per-plan state for each new plan (or re-plan)
            plan_results: List[Any] = []
            plan_step_notes: List[str] = []
            # plan_step_errors_details: List[Dict[str, Any]] = [] # Not directly used for re-planning logic, but for logging
            
            first_critical_error_details: Dict[str, Optional[str]] = {
                "error_type": None, "error_message": None, "traceback_snippet": None
            }
            
            plan_failed_critically = False

            for i, step in enumerate(current_plan):
                tool_name = step.get("tool_name")
                args = step.get("args", ())
                kwargs = step.get("kwargs", {})

                step_result: Any = None
                current_step_error_details: Dict[str, Any] = {}
                step_attempt_note: str = ""

                if not tool_name:
                    err_msg = f"Step {i+1} is missing 'tool_name'. Skipping."
                    print(f"ExecutionAgent: {err_msg}")
                    step_result = RuntimeError(err_msg)
                    current_step_error_details = {'error_type': type(step_result).__name__, 'error_message': err_msg, 'traceback_snippet': None}
                    step_attempt_note = "Skipped due to missing tool name."
                else:
                    if not isinstance(args, tuple): args = tuple(args) if isinstance(args, list) else (args,)
                    if not isinstance(kwargs, dict): kwargs = {}

                    processed_args = list(args)
                    for i_arg, arg_val in enumerate(processed_args):
                        if isinstance(arg_val, str):
                            match = re.fullmatch(r"\[\[step_(\d+)_output\]\]", arg_val)
                            if match:
                                ref_step_num = int(match.group(1))
                                if 1 <= ref_step_num <= len(plan_results):
                                    plan_result_to_sub = plan_results[ref_step_num - 1]
                                    if not isinstance(plan_result_to_sub, str):
                                        print(f"Warning: Step {ref_step_num} result is not a string ('{type(plan_result_to_sub).__name__}'). Using its string representation for placeholder '{arg_val}'.")
                                        processed_args[i_arg] = str(plan_result_to_sub)
                                    else:
                                        processed_args[i_arg] = plan_result_to_sub
                                else:
                                    print(f"ExecutionAgent: Warning - Invalid step reference {arg_val} for step {i+1}. Placeholder passed as is.")
                    final_args_for_tool = tuple(processed_args)

                    processed_kwargs = kwargs.copy()
                    for kw_key, kw_val in processed_kwargs.items():
                        if isinstance(kw_val, str):
                            match = re.fullmatch(r"\[\[step_(\d+)_output\]\]", kw_val)
                            if match:
                                ref_step_num = int(match.group(1))
                                if 1 <= ref_step_num <= len(plan_results):
                                    kw_plan_result_to_sub = plan_results[ref_step_num-1]
                                    if not isinstance(kw_plan_result_to_sub, str):
                                        print(f"Warning: Step {ref_step_num} result for kwarg '{kw_key}' is not str. Using str().")
                                        processed_kwargs[kw_key] = str(kw_plan_result_to_sub)
                                    else:
                                        processed_kwargs[kw_key] = kw_plan_result_to_sub
                                else:
                                    print(f"ExecutionAgent: Warning - Invalid step reference {kw_val} for kwarg '{kw_key}' in step {i+1}. Placeholder passed as is.")
                    final_kwargs_for_tool = processed_kwargs

                    for attempt in range(self.MAX_RETRIES_PER_STEP + 1):
                        try:
                            print(f"ExecutionAgent: Executing step {i+1}/{len(current_plan)} - Tool: {tool_name} (Args: {final_args_for_tool}, Kwargs: {final_kwargs_for_tool}), Attempt: {attempt+1}/{self.MAX_RETRIES_PER_STEP + 1}")
                            step_result = await tool_system.execute_tool(tool_name, args=final_args_for_tool, kwargs=final_kwargs_for_tool)
                            current_step_error_details = {} 
                            if attempt > 0:
                                step_attempt_note = f"Succeeded on retry (attempt {attempt+1})."
                            print(f"ExecutionAgent: Step {i+1} completed. Result: {str(step_result)[:200] + '...' if len(str(step_result)) > 200 else step_result}")
                            break 
                        except Exception as e:
                            step_result = e 
                            tb_snippet = traceback.format_exc(limit=3)
                            current_step_error_details = {'error_type': type(e).__name__, 'error_message': str(e), 'traceback_snippet': tb_snippet}
                            
                            if attempt < self.MAX_RETRIES_PER_STEP:
                                print(f"ExecutionAgent: Tool '{tool_name}' failed (Attempt {attempt+1}). Error: {str(e)}. Retrying...")
                            else: 
                                step_attempt_note = f"Failed after {self.MAX_RETRIES_PER_STEP + 1} attempt(s). Last error: {str(e)}"
                                print(f"ExecutionAgent: Tool '{tool_name}' also failed on last retry (Attempt {attempt+1}). Error: {str(e)}")

                plan_results.append(step_result)
                plan_step_notes.append(step_attempt_note)
                # plan_step_errors_details.append(current_step_error_details) # For detailed step-by-step logging if needed

                # Check for failure: either an exception or a dictionary indicating failure
                step_failed = False
                if isinstance(step_result, Exception):
                    step_failed = True
                elif isinstance(step_result, dict):
                    # Check for common failure indicators in dictionary results
                    if step_result.get("ran_successfully") is False or step_result.get("error") is not None:
                        step_failed = True
                        # Populate current_step_error_details if it's a dict-reported error and not already set by an exception
                        if not current_step_error_details:
                            current_step_error_details = {'error_type': 'ToolReportedError', 'error_message': step_result.get("error", str(step_result.get("stderr","Unknown tool error"))), 'traceback_snippet': None}

                if step_failed:
                    if first_critical_error_details["error_type"] is None: # Capture first critical error of this plan attempt
                        first_critical_error_details = current_step_error_details
                    
                    # Log this specific plan attempt's failure before trying to re-plan
                    reflection_entry_obj_fail = global_reflection_log.log_execution(
                        goal_description=goal_description,
                        plan=current_plan, # Log the plan that just failed
                        execution_results=plan_results, 
                        overall_success=False, # This specific plan attempt failed
                        notes=f"Plan attempt {replan_attempts + 1} failed at step {i+1} ({tool_name}). {step_attempt_note}",
                        first_error_type=first_critical_error_details["error_type"],
                        first_error_message=first_critical_error_details["error_message"],
                        first_traceback_snippet=first_critical_error_details["traceback_snippet"]
                    )
                    if learning_agent:
                        learning_agent.process_reflection_entry(reflection_entry_obj_fail)
                    plan_failed_critically = True # Mark that this plan attempt had a critical failure

                    if replan_attempts < self.MAX_REPLAN_ATTEMPTS:
                        print(f"ExecutionAgent: Critical failure in plan attempt {replan_attempts + 1}. Attempting to analyze failure and re-plan...")
                        tool_registry = tool_system.list_tools()
                        failure_analysis = analyze_last_failure(tool_registry, ollama_model_name=ollama_model_name)

                        if failure_analysis and failure_analysis.strip():
                            print(f"ExecutionAgent: Failure analysis obtained:\n{failure_analysis}")
                            new_plan = await planner_agent.replan_after_failure(
                                original_goal=goal_description,
                                failure_analysis=failure_analysis,
                                available_tools=tool_registry,
                                ollama_model_name=ollama_model_name
                            )
                            if new_plan:
                                print(f"ExecutionAgent: Successfully re-planned. New plan has {len(new_plan)} steps. Resetting and retrying.")
                                current_plan = new_plan
                                replan_attempts += 1 # Increment before breaking to restart loop
                                break # Break from step loop to restart with new plan in the outer while loop
                            else:
                                print("ExecutionAgent: Re-planning attempt failed to produce a new plan. Proceeding with original failure.")
                                # Fall through to normal failure handling outside the step loop as plan_failed_critically is True
                        else:
                            print("ExecutionAgent: Failure analysis did not yield significant results. Proceeding with original failure.")
                            # Fall through
                    else:
                        print(f"ExecutionAgent: Maximum re-plan attempts ({self.MAX_REPLAN_ATTEMPTS}) reached. Plan execution failed.")
                        # Fall through
                    
                    break # Break from step loop (current plan execution stops due to critical error)

            # After iterating through all steps of the current_plan or breaking due to critical failure
            if not plan_failed_critically: # Plan completed all steps without critical error
                print(f"ExecutionAgent: Plan attempt {replan_attempts + 1} completed successfully.")
                # Log this successful plan attempt
                reflection_entry_obj_success = global_reflection_log.log_execution(
                    goal_description=goal_description,
                    plan=current_plan,
                    execution_results=plan_results,
                    overall_success=True, # This specific plan attempt was successful
                    notes=f"Plan attempt {replan_attempts + 1} succeeded. " + ". ".join(filter(None, plan_step_notes)),
                    first_error_type=None, first_error_message=None, first_traceback_snippet=None
                )
                if learning_agent:
                    learning_agent.process_reflection_entry(reflection_entry_obj_success) # type: ignore
                # Record tool-goal associations only if this final plan was successful
                for step in current_plan:
                    tool_name = step.get("tool_name")
                    if tool_name: record_tool_goal_association(tool_name, goal_description)
                
                # Log GOAL_EXECUTION_COMPLETED for overall success
                tools_used_in_final_plan = list(set(step.get("tool_name") for step in current_plan if step.get("tool_name")))
                log_event(
                    event_type="GOAL_EXECUTION_COMPLETED",
                    description=f"Goal execution completed successfully for: {goal_description} (possibly after re-planning).",
                    source="ExecutionAgent.execute_plan",
                    metadata={
                        "goal_description": goal_description,
                        "final_plan_summary": [{"tool": step.get("tool_name"), "args_preview": str(step.get("args",()))[:50]} for step in current_plan],
                        "overall_success": True,
                        "num_steps_in_final_plan": len(current_plan),
                        "tools_used_in_final_plan": tools_used_in_final_plan,
                        "replan_attempts_made": replan_attempts,
                    }
                )
                return current_plan, plan_results # MODIFIED: Return successful plan and its results

            # If plan_failed_critically is True and we are here, it means either re-planning didn't happen,
            # or re-planning failed to produce a new plan, or re-plan limit was reached.
            if replan_attempts >= self.MAX_REPLAN_ATTEMPTS or not new_plan: # Check if we should stop trying
                if plan_failed_critically : # ensure this is only if the last attempt also failed.
                    print(f"ExecutionAgent: Plan execution failed for goal '{goal_description}' after {replan_attempts} re-plan attempt(s).")
                    # The final failure was already logged by global_reflection_log inside the loop.
                    # Log GOAL_EXECUTION_COMPLETED for overall failure
                    tools_used_in_last_plan = list(set(step.get("tool_name") for step in current_plan if step.get("tool_name"))) # current_plan is the one that failed last
                    log_event(
                        event_type="GOAL_EXECUTION_COMPLETED",
                        description=f"Goal execution failed for: {goal_description}",
                        source="ExecutionAgent.execute_plan",
                        metadata={
                            "goal_description": goal_description,
                            "last_attempted_plan_summary": [{"tool": step.get("tool_name"), "args_preview": str(step.get("args",()))[:50]} for step in current_plan],
                            "overall_success": False,
                            "num_steps_in_last_plan": len(current_plan),
                            "tools_used_in_last_plan": tools_used_in_last_plan,
                            "replan_attempts_made": replan_attempts,
                            "first_error_type_in_last_plan": first_critical_error_details.get("error_type"),
                            "first_error_message_in_last_plan": first_critical_error_details.get("error_message")
                        }
                    )
                    return current_plan, plan_results # MODIFIED: Return last attempted plan and its (failed) results
            # If we are here and plan_failed_critically is true, but replan_attempts < MAX_REPLAN_ATTEMPTS and new_plan was generated,
            # the outer while loop will continue with the new_plan.
        
        # Should ideally be covered by returns inside the loop.
        # This path implies MAX_REPLAN_ATTEMPTS was 0 and the first plan failed, or some other edge case.
        print(f"ExecutionAgent: Exiting execute_plan for goal '{goal_description}' after exhausting plan attempts.")
        return current_plan, plan_results # MODIFIED: Return the last plan and its results, even if loop exhausted


if __name__ == '__main__':
    # Example Usage and Test (assuming global_reflection_log is available for testing its effect)
    # Needs to be updated to include PlannerAgent and ollama_model_name for full testing.

    async def main_test():
        class MockToolSystemForExecutionTest:
            def __init__(self, succeed_after_n_failures=0):
                self.tools_called = []
                self.call_counts = {}
                self.succeed_after_n_failures = succeed_after_n_failures

            def list_tools(self): # Mock for analyze_last_failure
                return {
                    "greet_user": "Greets the user. Args: name (str)",
                    "add_numbers": "Adds two numbers. Args: a (str), b (str)",
                    "faulty_tool": "A tool that always fails initially.",
                    "sometimes_faulty_tool": "A tool that fails N times then succeeds.",
                    "none_tool": "A tool that returns None.",
                    "tool_not_found_in_mock": "This tool does not exist here."
                }

            async def execute_tool(self, name, args=(), kwargs=None):
                if kwargs is None: kwargs = {}
                call_info = {'name': name, 'args': args, 'kwargs': kwargs}
                self.tools_called.append(call_info)
                self.call_counts[name] = self.call_counts.get(name, 0) + 1
                
                print(f"MockToolSystem: Executing {name} (Call #{self.call_counts[name]}) with {args} and {kwargs}")
                
                if name == "greet_user":
                    return f"Hello, {args[0]}!" if args else "Hello, World!"
                elif name == "add_numbers":
                    # Ensure args are actual numbers for mock, not strings like "5"
                    actual_args = []
                    for arg in args:
                        try: actual_args.append(int(arg)) # Simple int conversion for test
                        except ValueError: return f"Error: add_numbers requires parsable integer arguments. Got '{arg}'."
                    
                    if not all(isinstance(arg, int) for arg in actual_args): # Redundant due to above, but good check
                        return "Error: add_numbers requires integer arguments."
                    return actual_args[0] + actual_args[1]
                elif name == "faulty_tool":
                    raise ValueError("Intentionally faulty tool error on every call.")
                elif name == "sometimes_faulty_tool":
                    if self.call_counts.get(name, 0) <= self.succeed_after_n_failures:
                        raise ValueError(f"Sometimes_faulty_tool error (call #{self.call_counts[name]})")
                    return f"Success from sometimes_faulty_tool after {self.succeed_after_n_failures} failures."
                elif name == "none_tool":
                    return None
                elif name == "tool_not_found_in_mock":
                    raise ts_module_type.ToolNotFoundError(f"Mock: Tool '{name}' not found.")
                await asyncio.sleep(0) 
                return f"Mock success for {name}"

        # Mock PlannerAgent for testing ExecutionAgent's re-plan logic
        class MockPlannerAgentForExecutionTest(PlannerAgent):
            def __init__(self, new_plan_on_replan=None, fail_replan=False):
                super().__init__() # Not strictly needed if only overriding replan
                self.new_plan_on_replan = new_plan_on_replan if new_plan_on_replan else []
                self.fail_replan = fail_replan
                self.replan_calls = 0

            async def replan_after_failure(self, original_goal: str, failure_analysis: str, available_tools: Dict[str, str], ollama_model_name: Optional[str] = None) -> List[Dict[str, Any]]:
                self.replan_calls += 1
                print(f"MockPlannerAgent: replan_after_failure called for goal '{original_goal}'. Analysis: '{failure_analysis[:50]}...'")
                if self.fail_replan:
                    print("MockPlannerAgent: Simulating re-plan failure (returning empty list).")
                    return []
                print(f"MockPlannerAgent: Returning new plan: {self.new_plan_on_replan}")
                return self.new_plan_on_replan

        # Mock LearningAgent for testing ExecutionAgent's calls
        class MockLearningAgent(LearningAgent): # Inherit from LearningAgent
            def process_reflection_entry(self, entry):
                print(f"MockLearningAgent: process_reflection_entry called for entry ID {entry.entry_id}, Goal: '{entry.goal_description[:30]}...'")

        mock_learning_agent_instance = MockLearningAgent()

        executor_test = ExecutionAgent()
        mock_ollama_model = "mock_model_for_testing" # Passed but not used by mocks here

        print("--- ExecutionAgent Tests with Re-planning ---")
        initial_log_count = len(global_reflection_log.get_entries(limit=1000))

        # Test 1: Simple successful plan (no re-planning needed)
        print("\n--- Test 1: Successful Plan (No Re-plan) ---")
        mock_ts_test1 = MockToolSystemForExecutionTest()
        mock_planner_test1 = MockPlannerAgentForExecutionTest() # Won't be called
        plan1 = [{"tool_name": "greet_user", "args": ("Alice",), "kwargs": {}}, {"tool_name": "add_numbers", "args": ("5", "3"), "kwargs": {}}]
        results1 = await executor_test.execute_plan(
            "Test Goal 1", 
            plan1, 
            mock_ts_test1, 
            mock_planner_test1, 
            mock_learning_agent_instance, # Pass mock learning agent
            ollama_model_name=mock_ollama_model)
        print(f"Test 1 Results: {results1}")
        assert mock_planner_test1.replan_calls == 0
        assert any(isinstance(r, Exception) for r in results1) == False # Should be all success
        mock_ts_test1.tools_called = []; mock_ts_test1.call_counts = {}

        # Test 2: Plan fails, re-planning provides a successful new plan
        print("\n--- Test 2: Failure, Re-plan to Success ---")
        mock_ts_test2 = MockToolSystemForExecutionTest(succeed_after_n_failures=0) # faulty_tool will always fail
                                                                                # sometimes_faulty_tool will succeed on first call if used in new plan
        successful_replan = [{"tool_name": "greet_user", "args": ("Replanned User",), "kwargs": {}}, {"tool_name": "add_numbers", "args": ("10", "10"), "kwargs": {}}]
        mock_planner_test2 = MockPlannerAgentForExecutionTest(new_plan_on_replan=successful_replan)
        
        plan2_initial = [{"tool_name": "faulty_tool", "args": (), "kwargs": {}}] # This will fail
        results2 = await executor_test.execute_plan(
            "Test Goal 2", 
            plan2_initial, 
            mock_ts_test2, 
            mock_planner_test2, 
            mock_learning_agent_instance, 
            ollama_model_name=mock_ollama_model)
        print(f"Test 2 Results: {results2}")
        assert mock_planner_test2.replan_calls == 1
        # Check if final results correspond to the successful_replan
        assert "Hello, Replanned User!" in results2
        assert 20 in results2 # 10 + 10
        assert any("Re-planning attempt 1 initiated" in str(r) for r in results2)
        mock_ts_test2.tools_called = []; mock_ts_test2.call_counts = {}

        # Test 3: Plan fails, re-planning also provides a failing plan (or empty) -> overall failure
        print("\n--- Test 3: Failure, Re-plan also Fails ---")
        mock_ts_test3 = MockToolSystemForExecutionTest() # faulty_tool will always fail
        failing_replan = [{"tool_name": "faulty_tool", "args": ("still_fails",), "kwargs": {}}] # This will also fail
        mock_planner_test3 = MockPlannerAgentForExecutionTest(new_plan_on_replan=failing_replan)
        
        plan3_initial = [{"tool_name": "faulty_tool", "args": (), "kwargs": {}}]
        results3 = await executor_test.execute_plan(
            "Test Goal 3", 
            plan3_initial, 
            mock_ts_test3, 
            mock_planner_test3, 
            mock_learning_agent_instance, 
            ollama_model_name=mock_ollama_model)
        print(f"Test 3 Results: {results3}")
        assert mock_planner_test3.replan_calls == 1 # Called once for the first failure
        # ExecutionAgent.MAX_REPLAN_ATTEMPTS is 1, so after the re-plan fails, it stops.
        # The results should contain results from the first failed plan, the re-plan note, and then the second failed plan.
        assert any(isinstance(r, ValueError) and "Intentionally faulty tool error" in str(r) for r in results3)
        assert any("Re-planning attempt 1 initiated" in str(r) for r in results3)
        mock_ts_test3.tools_called = []; mock_ts_test3.call_counts = {}

        # Test 4: Plan fails, re-planning fails to produce any plan (returns empty list)
        print("\n--- Test 4: Failure, Re-plan returns Empty Plan ---")
        mock_ts_test4 = MockToolSystemForExecutionTest()
        mock_planner_test4 = MockPlannerAgentForExecutionTest(fail_replan=True) # Simulate planner returning []
        
        plan4_initial = [{"tool_name": "faulty_tool", "args": (), "kwargs": {}}]
        results4 = await executor_test.execute_plan(
            "Test Goal 4", 
            plan4_initial, 
            mock_ts_test4, 
            mock_planner_test4, 
            mock_learning_agent_instance, 
            ollama_model_name=mock_ollama_model)
        print(f"Test 4 Results: {results4}")
        assert mock_planner_test4.replan_calls == 1
        assert any(isinstance(r, ValueError) and "Intentionally faulty tool error" in str(r) for r in results4)
        # Should not contain "Re-planning attempt" note if planner returns empty, but the fact that it failed is logged.
        # The overall_plan_execution_results will contain the error from the first plan.
        mock_ts_test4.tools_called = []; mock_ts_test4.call_counts = {}


        # Test 5: Initial plan is empty
        print("\n--- Test 5: Initial Empty Plan ---")
        mock_ts_test5 = MockToolSystemForExecutionTest()
        mock_planner_test5 = MockPlannerAgentForExecutionTest()
        plan5_empty = []
        results5 = await executor_test.execute_plan(
            "Test Goal 5", 
            plan5_empty, 
            mock_ts_test5, 
            mock_planner_test5, 
            mock_learning_agent_instance, 
            ollama_model_name=mock_ollama_model)
        print(f"Test 5 Results: {results5}")
        assert results5 == []
        assert mock_planner_test5.replan_calls == 0

        # Test 6: Max re-plan attempts reached
        print("\n--- Test 6: Max Re-plan Attempts Reached ---")
        ExecutionAgent.MAX_REPLAN_ATTEMPTS = 1 # Ensure it's 1 for this test
        mock_ts_test6 = MockToolSystemForExecutionTest() # faulty_tool always fails
        failing_replan_again = [{"tool_name": "faulty_tool", "args": ("attempt_2_fail",), "kwargs": {}}]
        # Planner will keep providing a failing plan
        mock_planner_test6 = MockPlannerAgentForExecutionTest(new_plan_on_replan=failing_replan_again)
        
        plan6_initial = [{"tool_name": "faulty_tool", "args": ("attempt_1_fail",), "kwargs": {}}]
        results6 = await executor_test.execute_plan(
            "Test Goal 6", 
            plan6_initial, 
            mock_ts_test6, 
            mock_planner_test6, 
            mock_learning_agent_instance, 
            ollama_model_name=mock_ollama_model)
        print(f"Test 6 Results: {results6}")
        assert mock_planner_test6.replan_calls == ExecutionAgent.MAX_REPLAN_ATTEMPTS 
        # Results should show two sets of failures + re-plan note
        assert len([r for r in results6 if isinstance(r, ValueError)]) == 2 # Two faulty_tool errors
        assert any("Re-planning attempt 1 initiated" in str(r) for r in results6)
        mock_ts_test6.tools_called = []; mock_ts_test6.call_counts = {}
        ExecutionAgent.MAX_REPLAN_ATTEMPTS = 1 # Reset for other tests if any

        final_log_count = len(global_reflection_log.get_entries(limit=1000))
        print(f"\nNumber of entries added to global_reflection_log during these tests: {final_log_count - initial_log_count}")
        print("\n--- ExecutionAgent Tests with Re-planning Finished ---")

    if __name__ == '__main__':
        asyncio.run(main_test())

# ### END FILE: ai_assistant/planning/execution.py ###

# ### START FILE: ai_assistant/planning/llm_argument_parser.py ###
# ai_assistant/planning/llm_argument_parser.py
from typing import Tuple, List, Dict, Any, Optional
from ai_assistant.llm_interface.ollama_client import invoke_ollama_model
from ai_assistant.config import get_model_for_task # Added import
import json
import re

LLM_ARG_POPULATION_PROMPT_TEMPLATE = """Given the user's overall goal: "{goal_description}"
And the specific tool selected:
  Tool Name: "{tool_name}"
  Tool Description: "{tool_description}" (This description may include details about expected arguments like names, types, and if they are positional or keyword-based.)

Your task is to identify and extract the arguments for the tool "{tool_name}" from the user's goal.
Respond ONLY with a JSON object containing two keys:
- "args": A list of strings, representing the positional arguments in the correct order.
- "kwargs": A dictionary where keys are argument names (strings) and values are the corresponding argument values (strings).

If no suitable value for an argument is found in the goal, represent it as an empty string "" or omit it if appropriate for keyword arguments.
If the tool description specifies argument names (e.g., "requires 'filename' and 'content'"), use those as keys in "kwargs" if they seem like keyword arguments.
If the tool description implies positional arguments (e.g., "takes two numbers"), fill the "args" list.

Example Response (tool expects two positional args and one kwarg):
{{"args": ["10", "blue"], "kwargs": {{"output_file": "results.txt"}}}}
Another Example (no kwargs):
{{"args": ["some_value"], "kwargs": {{}}}}
Another Example (no args):
{{"args": [], "kwargs": {{"param1": "value1"}}}}
If an argument is mentioned in the description but not found in the goal for the tool, you can represent it as an empty string in "args" or omit from "kwargs".

JSON object:
"""

def populate_tool_arguments_with_llm(
    goal_description: str,
    tool_name: str,
    tool_description: str,
    ollama_model_name: Optional[str] = None 
) -> Tuple[List[str], Dict[str, str]]:
    """
    Uses an LLM to populate arguments for a given tool based on a goal description.
    """
    model_to_use = ollama_model_name if ollama_model_name is not None else get_model_for_task("argument_population")
    formatted_prompt = LLM_ARG_POPULATION_PROMPT_TEMPLATE.format(
        goal_description=goal_description,
        tool_name=tool_name,
        tool_description=tool_description
    )
    
    print(f"\nLLMArgParser: Sending prompt to populate args for '{tool_name}' using model '{model_to_use}' (Goal: '{goal_description[:50]}...'):\nPrompt (first 300 chars): {formatted_prompt[:300]}...")

    llm_response_str = invoke_ollama_model(formatted_prompt, model_name=model_to_use)

    if not llm_response_str:
        print(f"LLMArgParser: Received no response from LLM ({model_to_use}) for argument population.")
        return ([], {})

    print(f"LLMArgParser: Raw response from LLM for args:\n---\n{llm_response_str}\n---")

    json_str_to_parse = llm_response_str
    # Sanitize: remove markdown and potential "JSON object:" prefix
    match = re.search(r"```json\s*([\s\S]*?)\s*```", json_str_to_parse)
    if match:
        json_str_to_parse = match.group(1)
    json_str_to_parse = re.sub(r"^\s*JSON object:?\s*", "", json_str_to_parse.strip(), flags=re.IGNORECASE).strip()

    try:
        parsed_json = json.loads(json_str_to_parse)
    except json.JSONDecodeError as e:
        print(f"LLMArgParser: Failed to parse JSON response for arguments. Error: {e}")
        print(f"LLMArgParser: Attempted to parse: '{json_str_to_parse}'")
        return ([], {})

    if not isinstance(parsed_json, dict):
        print(f"LLMArgParser: Parsed JSON is not a dictionary. Got: {type(parsed_json)}")
        return ([], {})

    raw_args = parsed_json.get("args")
    raw_kwargs = parsed_json.get("kwargs")

    # Validate and sanitize args
    final_args: List[str] = []
    if isinstance(raw_args, list):
        final_args = [str(arg) for arg in raw_args]
    elif raw_args is not None: # If it's present but not a list
        print(f"LLMArgParser: Warning - 'args' from LLM was not a list (got {type(raw_args)}). Using empty list.")
    
    # Validate and sanitize kwargs
    final_kwargs: Dict[str, str] = {}
    if isinstance(raw_kwargs, dict):
        final_kwargs = {str(k): str(v) for k, v in raw_kwargs.items()}
    elif raw_kwargs is not None: # If it's present but not a dict
        print(f"LLMArgParser: Warning - 'kwargs' from LLM was not a dictionary (got {type(raw_kwargs)}). Using empty dict.")

    print(f"LLMArgParser: Successfully parsed args: {final_args}, kwargs: {final_kwargs} for tool '{tool_name}'")
    return (final_args, final_kwargs)


if __name__ == '__main__':
    print("--- Testing LLM Argument Parser ---")
    
    # Mock invoke_ollama_model for testing this module directly
    # Store original function to restore later
    original_invoke_ollama = invoke_ollama_model
    
    def mock_invoke_ollama(prompt: str, model_name: str, **kwargs) -> Optional[str]:
        print(f"\n--- MOCK OLLAMA CALL ---")
        print(f"Model: {model_name}")
        print(f"Prompt (first 150 chars for test): {prompt[:150]}...")
        
        # Simulate different LLM responses based on prompt content for testing
        if "add 75 and 20" in prompt and "add_numbers" in prompt:
            return """```json
            {
                "args": ["75", "20"],
                "kwargs": {}
            }
            ```"""
        elif "greet User" in prompt and "greet_user" in prompt:
             return """JSON object:
             {
                 "args": ["User"],
                 "kwargs": {"title": "Esteemed"}
             }"""
        elif "subtract 10 from 30" in prompt and "subtract_tool" in prompt:
             return """{
                 "args": ["30", "10"],
                 "kwargs": {}
             }"""
        elif "no_real_args_here" in prompt and "test_tool_no_args" in prompt:
             return """{
                 "args": [],
                 "kwargs": {}
             }"""
        elif "bad_json_response" in prompt:
            return "This is not JSON { definitely not"
        elif "not_dict_response" in prompt:
            return "[\"just_a_list\"]" # Valid JSON, but not a dict
        elif "bad_args_type" in prompt:
            return """{
                "args": "not_a_list", 
                "kwargs": {"key": "value"}
            }"""
        elif "bad_kwargs_type" in prompt:
            return """{
                "args": ["valid_arg"], 
                "kwargs": "not_a_dict"
            }"""
        return None # Default to no response

    # Replace the actual function with the mock
    from ai_assistant.llm_interface import ollama_client
    ollama_client.invoke_ollama_model = mock_invoke_ollama


    # Test cases
    print("\n--- Test Case 1: Add numbers ---")
    args, kwargs = populate_tool_arguments_with_llm(
        goal_description="Can you add 75 and 20 for me?",
        tool_name="add_numbers",
        tool_description="Adds two numbers a and b."
    )
    print(f"Result: args={args}, kwargs={kwargs}")
    assert args == ["75", "20"]
    assert kwargs == {}

    print("\n--- Test Case 2: Greet user with kwargs ---")
    args, kwargs = populate_tool_arguments_with_llm(
        goal_description="Please greet User",
        tool_name="greet_user",
        tool_description="Greets a person. Takes name as positional arg, and optional 'title' as kwarg."
    )
    print(f"Result: args={args}, kwargs={kwargs}")
    assert args == ["User"]
    assert kwargs == {"title": "Esteemed"}
    
    print("\n--- Test Case 3: Tool with no real args in goal ---")
    args, kwargs = populate_tool_arguments_with_llm(
        goal_description="no_real_args_here",
        tool_name="test_tool_no_args",
        tool_description="A test tool that takes no specific args from this goal."
    )
    print(f"Result: args={args}, kwargs={kwargs}")
    assert args == []
    assert kwargs == {}

    print("\n--- Test Case 4: Bad JSON response ---")
    args, kwargs = populate_tool_arguments_with_llm(
        goal_description="bad_json_response", 
        tool_name="test_bad_json", 
        tool_description="Tool that will get bad JSON."
    )
    print(f"Result: args={args}, kwargs={kwargs}")
    assert args == []
    assert kwargs == {}
    
    print("\n--- Test Case 5: LLM returns JSON list instead of dict ---")
    args, kwargs = populate_tool_arguments_with_llm(
        goal_description="not_dict_response", 
        tool_name="test_not_dict", 
        tool_description="Tool that will get a JSON list."
    )
    print(f"Result: args={args}, kwargs={kwargs}")
    assert args == []
    assert kwargs == {}

    print("\n--- Test Case 6: LLM returns args not as list ---")
    args, kwargs = populate_tool_arguments_with_llm(
        goal_description="bad_args_type", 
        tool_name="test_bad_args_type", 
        tool_description="Tool that will get args not as list."
    )
    print(f"Result: args={args}, kwargs={kwargs}")
    assert args == [] # Should default to empty list
    assert kwargs == {"key": "value"}


    print("\n--- Test Case 7: LLM returns kwargs not as dict ---")
    args, kwargs = populate_tool_arguments_with_llm(
        goal_description="bad_kwargs_type", 
        tool_name="test_bad_kwargs_type", 
        tool_description="Tool that will get kwargs not as dict."
    )
    print(f"Result: args={args}, kwargs={kwargs}")
    assert args == ["valid_arg"]
    assert kwargs == {} # Should default to empty dict
    
    print("\n--- Test Case 8: No response from LLM ---")
    # For this, the mock needs to return None. Our mock returns None by default if no conditions met.
    args, kwargs = populate_tool_arguments_with_llm(
        goal_description="This goal won't match any mock conditions",
        tool_name="any_tool",
        tool_description="Any description"
    )
    print(f"Result: args={args}, kwargs={kwargs}")
    assert args == []
    assert kwargs == {}

    # Restore original function
    ollama_client.invoke_ollama_model = original_invoke_ollama
    print("\n--- LLM Argument Parser Tests Finished (mocked Ollama) ---")

# ### END FILE: ai_assistant/planning/llm_argument_parser.py ###

# ### START FILE: ai_assistant/planning/planning.py ###
# Code for task planning.
from typing import Optional, Dict, Any, List
import re
import json # For parsing LLM plan string
from ai_assistant.planning.llm_argument_parser import populate_tool_arguments_with_llm
from ai_assistant.config import get_model_for_task
from ai_assistant.llm_interface.ollama_client import invoke_ollama_model_async # For re-planning

class PlannerAgent:
    """
    Responsible for creating a sequence of tool invocations (a plan)
    to achieve a given goal.
    """

    def _extract_numbers(self, text: str, count: int = 2) -> List[str]:
        """Extracts up to 'count' numbers from the text using regex."""
        numbers = re.findall(r'\d+(?:\.\d+)?', text) # Supports integers and decimals
        return numbers[:count]

    def _extract_name_for_greeting(self, text: str) -> str:
        """Extracts a name for greeting, looking for capitalized words after keywords."""
        match = re.search(
            r'(?:greet|hello to|hi to|say hello to|say hi to)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)', 
            text, 
            re.IGNORECASE
        )
        if match:
            return match.group(1)
        
        if "greet" in text.lower():
            words = text.split()
            for i, word in enumerate(words):
                if word.istitle() and word.lower() not in ["greet", "hello", "hi", "say", "to"]:
                    if i > 0 and words[i-1].lower() in ["greet", "to"]:
                        # Check for multi-word names like "John Doe"
                        name_parts = [word]
                        for j in range(i + 1, len(words)):
                            if words[j].istitle():
                                name_parts.append(words[j])
                            else:
                                break
                        return " ".join(name_parts)
                    elif i > 0 and not words[i-1].istitle():
                        return word 
        return "User"

    def _plan_single_segment(self, segment: str, available_tools: Dict[str, str]) -> Optional[Dict[str, Any]]:
        """
        Attempts to plan a single tool invocation for a given text segment.
        This encapsulates the previous single-step planning logic and integrates LLM for arg population.
        """
        segment_lower = segment.lower()
        
        selected_tool_name: Optional[str] = None
        extracted_args: tuple = ()
        extracted_kwargs: Dict[str, Any] = {}
        
        # 1. Rule-based tool selection
        if "greet_user" in available_tools and \
           any(kw in segment_lower for kw in ["greet", "hello", "hi", "say hi", "say hello"]):
            selected_tool_name = "greet_user"
            name_to_greet = self._extract_name_for_greeting(segment)
            if name_to_greet and name_to_greet != "User": # If a specific name was found
                extracted_args = (name_to_greet,)
            # If name_to_greet is "User" (default), regex was weak. LLM might do better.
        
        elif "add_numbers" in available_tools and \
             any(kw in segment_lower for kw in ["add", "sum", "plus", "total of"]):
            selected_tool_name = "add_numbers"
            numbers = self._extract_numbers(segment, 2)
            if len(numbers) == 2:
                extracted_args = tuple(numbers)
            # If not 2 numbers, regex was weak. LLM might do better.

        elif "multiply_numbers" in available_tools and \
             any(kw in segment_lower for kw in ["multiply", "times", "product of"]):
            selected_tool_name = "multiply_numbers"
            numbers = self._extract_numbers(segment, 2)
            if len(numbers) == 2:
                extracted_args = tuple(numbers)
            # If not 2 numbers, regex was weak. LLM might do better.
        
        # Add other rule-based tool selections here...

        if selected_tool_name:
            tool_description = available_tools[selected_tool_name]
            
            # 2. Decide if LLM should be used for argument population
            # Strategy: Use LLM if regex extraction was weak (e.g., no args found for tools that expect them)
            # or for tools where regex is inherently difficult for args.
            use_llm_for_args = False
            if selected_tool_name in ["add_numbers", "multiply_numbers"] and not extracted_args:
                use_llm_for_args = True
                print(f"PlannerAgent: Rule-based arg extraction for '{selected_tool_name}' yielded no args. Trying LLM.")
            elif selected_tool_name == "greet_user" and (not extracted_args or extracted_args[0] == "User"):
                # If regex found default "User" or nothing, LLM might find a specific name.
                use_llm_for_args = True
                print(f"PlannerAgent: Rule-based arg extraction for '{selected_tool_name}' was weak. Trying LLM.")
            # Add other conditions for use_llm_for_args if needed for other tools

            if use_llm_for_args:
                llm_args_list, llm_kwargs_dict = populate_tool_arguments_with_llm(
                    goal_description=segment, # Use the current segment as the goal for arg population
                    tool_name=selected_tool_name,
                    tool_description=tool_description
                )
                
                # Merge strategy: LLM overrides if it provides something substantial.
                # For positional args, if LLM provides any, it usually has better context.
                if llm_args_list: # If LLM found any positional args
                    extracted_args = tuple(llm_args_list) 
                # For kwargs, merge or override. Here, simple override if LLM provides them.
                if llm_kwargs_dict:
                    extracted_kwargs = llm_kwargs_dict
            
            # Fallback for tools where regex failed and LLM also didn't provide args
            if selected_tool_name == "add_numbers" and not extracted_args:
                extracted_args = ("0", "0")
                extracted_kwargs["note"] = f"Could not infer numbers for 'add_numbers' from '{segment}'. Using defaults."
            elif selected_tool_name == "multiply_numbers" and not extracted_args:
                extracted_args = ("1", "1")
                extracted_kwargs["note"] = f"Could not infer numbers for 'multiply_numbers' from '{segment}'. Using defaults."
            elif selected_tool_name == "greet_user" and not extracted_args:
                 extracted_args = ("User",) # Default if LLM also fails for greet_user

            return {
                "tool_name": selected_tool_name,
                "args": extracted_args,
                "kwargs": extracted_kwargs
            }

        return None # No tool matched for this segment by rule-based selection

    def create_plan(self, main_goal_description: str, available_tools: Dict[str, str]) -> List[Dict[str, Any]]:
        """
        Creates a multi-step plan to achieve the main_goal_description using available_tools.
        Splits the goal into segments and processes each.
        """
        full_plan: List[Dict[str, Any]] = []
        
        # Split by "and then" or "then" first, as these are strong indicators of sequence.
        # Using a regex that captures the delimiters to re-insert them for context or complex parsing later if needed,
        # but for now, we just split and process.
        # We use non-capturing groups for the delimiters for simpler splitting.
        segments = re.split(r'\s+(?:and then|then)\s+', main_goal_description, flags=re.IGNORECASE)
        
        processed_segments = []
        for segment in segments:
            # Further split by "and" if it seems to connect distinct actions.
            # This is heuristic. "add 5 and 7" should not be split.
            # "greet Alice and add 5 and 7" -> "greet Alice", "add 5 and 7" by the outer split.
            # "multiply 2 by 3 and greet Bob" -> needs "and" splitting.
            
            # Avoid splitting "and" if it's likely part of a number phrase like "add 5 and 7"
            # This check is very basic.
            if ' and ' in segment.lower() and not any(num_kw in segment.lower() for num_kw in ["add", "sum", "plus", "multiply", "times", "product of"]):
                 # Split only once by "and" to separate into two main actions if "and" is a primary conjunction
                sub_segments = re.split(r'\s+and\s+', segment, maxsplit=1, flags=re.IGNORECASE)
                processed_segments.extend(sub_segments)
            else:
                processed_segments.append(segment)

        for seg_idx, segment_text in enumerate(processed_segments):
            if not segment_text.strip(): # Skip empty segments
                continue
            
            print(f"PlannerAgent: Processing segment {seg_idx+1}/{len(processed_segments)}: '{segment_text}'")
            step = self._plan_single_segment(segment_text, available_tools)
            if step:
                full_plan.append(step)
            else:
                print(f"PlannerAgent: No specific tool action planned for segment: '{segment_text}'")

        # If no steps were generated at all from any segment, and no_op_tool is available, use it.
        if not full_plan and "no_op_tool" in available_tools:
            full_plan.append({
                "tool_name": "no_op_tool",
                "args": (),
                "kwargs": {} # Ensure no_op_tool is called without unexpected keyword arguments
            })
        elif not full_plan:
            print(f"Planner: Could not find any suitable tool or create a plan for the goal: '{main_goal_description}'")

        print(f"PlannerAgent: Generated plan for '{main_goal_description}': {full_plan}")
        return full_plan

    async def create_plan_with_llm(
        self, 
        goal_description: str, 
        available_tools: Dict[str, str],
        project_context_summary: Optional[str] = None, # NEW: For providing project file contents
        project_name_for_context: Optional[str] = None # NEW: Name of the project for context
    ) -> List[Dict[str, Any]]:
        """ (Async)
        Creates a plan to achieve the goal_description using an LLM to generate the plan steps.
        Optionally includes project context if provided.
        """
        import json 
        # No longer need sync invoke_ollama_model here, will use invoke_ollama_model_async

        MAX_CORRECTION_ATTEMPTS = 1 # 1 initial attempt + 1 correction attempt
        current_attempt = 0
        llm_response_str: Optional[str] = None
        parsed_plan: Optional[List[Dict[str, Any]]] = None
        last_error_description: str = "No response from LLM."

        print(f"\nPlannerAgent (LLM): Attempting to create plan for goal: '{goal_description}'")
        tools_json_string = json.dumps(available_tools, indent=2)

        # --- NEW: Prepare project context section for the prompt ---
        PROJECT_CONTEXT_SECTION_TEMPLATE = """
Current Project Context for '{project_name}':
---
{project_context_summary}
---
When generating the plan, consider this existing project context. For example, if the goal is to "fix a bug in function X of file Y.py", your plan should likely involve reading or modifying Y.py. If the goal is to "add a feature that uses existing function Z", your plan should reflect knowledge of Z if it's in the context.
"""
        project_context_section_str = ""
        if project_context_summary and project_name_for_context:
            project_context_section_str = PROJECT_CONTEXT_SECTION_TEMPLATE.format(
                project_name=project_name_for_context,
                project_context_summary=project_context_summary
            )
        # --- END NEW ---

        # Refined Prompt Template
        LLM_PLANNING_PROMPT_TEMPLATE = """Given the user's goal: "{goal}"
{project_context_section}
And the following available tools (tool_name: description):
{tools_json_string}

Generate a plan to achieve this goal. The plan *MUST* be a JSON list of step dictionaries.
Each step dictionary *MUST* contain the following keys:
- "tool_name": string (must be one of the available tools listed above)
- "args": list of strings (positional arguments for the tool). If an argument value cannot be inferred from the goal, use an empty string "" or a placeholder like "TODO_infer_arg_value".
- "kwargs": dictionary (key-value pairs of strings for keyword arguments, e.g., {{"key": "value"}}). If no keyword arguments, use an empty dictionary {{}}.

**Preferred Project Management Tools:**
For tasks related to software project creation, code generation for specific files within a project, or building out a project based on a plan, please PREFER the following tools:
1.  `initiate_ai_project(project_name: str, project_description: str)`:
    *   Use when the user wants to start a new software project.
    *   `project_name` should be a concise, descriptive name derived from the user's goal (e.g., "MyWebApp", "DataAnalyzer").
    *   `project_description` should be the user's stated goal or a clear summary of the project's purpose.
2.  `generate_code_for_project_file(project_name: str, filename: str)`:
    *   Use when the user wants to generate code for a specific file within an *existing* project.
    *   Identify the `project_name` and the target `filename` (e.g., "main.py", "utils/helpers.js") from the user's request.
3.  `execute_project_coding_plan(project_name: str)`:
    *   Use when the user wants to generate all remaining planned code for an *existing* project according to its coding plan.
    *   Identify the `project_name` from the user's request.

**IMPORTANT DIRECTIVE FOR TOOL CREATION:**
If the user's goal is to "create a tool", "make a tool", "generate a tool", or a similar request implying the creation of new functionality that is not met by existing tools, your primary plan *MUST* be to use the "generate_new_tool_from_description" tool.
The 'tool_description' argument for this tool should be the user's stated requirements for the new tool.
Example for tool creation:
  User goal: "Make a tool that tells me the current moon phase."
  Correct Plan:
  [
    {{"tool_name": "generate_new_tool_from_description", "args": ["a tool that tells me the current moon phase"], "kwargs": {{}}}}
  ]
Do NOT attempt to fulfill the *functionality* of a requested new tool using other existing tools if the user explicitly asks to *create* a tool. Your task in such a scenario is to initiate the tool creation process.

Example of a valid JSON plan (list with one step using a general tool):
[
  {{"tool_name": "add_numbers", "args": ["10", "20"], "kwargs": {{}}}}
]

Examples using Project Management Tools:
*   User goal: "start a new python project called 'MyWebApp' to manage a to-do list"
    Plan: `[{{"tool_name": "initiate_ai_project", "args": ["MyWebApp", "A project to manage a to-do list"], "kwargs": {{}}}}]`
*   User goal: "generate the main.py file for the MyWebApp project"
    Plan: `[{{"tool_name": "generate_code_for_project_file", "args": ["MyWebApp", "main.py"], "kwargs": {{}}}}]`
*   User goal: "build the rest of the MyWebApp project"
    Plan: `[{{"tool_name": "execute_project_coding_plan", "args": ["MyWebApp"], "kwargs": {{}}}}]`

**Important Instructions for Search and Knowledge Retrieval:**
Use tools like 'search_google_custom_search' (if available and appropriate) primarily when the goal requires CURRENT information (e.g., recent news, rapidly changing facts) or specific external knowledge that your internal knowledge base is unlikely to cover. Do NOT use search for general knowledge, creative tasks, or if the answer is likely static and well-known.
When a search is needed and both 'search_google_custom_search' and 'search_duckduckgo' are available, generally prefer 'search_google_custom_search' for comprehensive results, unless DuckDuckGo is specifically requested or more appropriate for privacy-sensitive queries.

If you determine 'search_google_custom_search' is necessary:
1.  Formulate a clear and concise search query as the first argument for the 'search_google_custom_search' tool.
2.  Optionally, you can specify the number of results by providing a 'num_results' integer (between 1 and 10) in the 'kwargs' dictionary (e.g., `{{"num_results": "5"}}`). If omitted, it defaults to 5.
3.  You *MUST* add a subsequent step in the plan to call a tool named 'process_search_results'.

The 'process_search_results' tool takes the following arguments:
    - `search_query` (string): The original search query you provided to the search tool.
    - `search_results_json` (string): The JSON output from the preceding search tool (e.g., 'search_google_custom_search' or 'search_duckduckgo'). Use "[[step_X_output]]" where X is the 1-based index of the search tool step.
    - `processing_instruction` (string, optional kwargs): Describes the desired processing. Examples:
        - `"answer_query"` (default): Generate a direct natural language answer to the original query.
        - `"summarize_results"`: Provide a concise summary of the information found.
        - `"extract_entities"`: List key entities (people, places, organizations, dates) relevant to the query found in the results.
        - `"custom_instruction:<your specific request>"`: For more specific extraction tasks, e.g., "custom_instruction:Extract the main arguments for and against the proposal."
      If omitted, the default is "answer_query".

Example of a plan involving search with Google (default processing):
```json
[
  {{
    "tool_name": "search_duckduckgo",
    "args": ["latest developments in AI regulation"],
    "kwargs": {{}}
  }},
  {{
    "tool_name": "process_search_results",
    "args": ["latest developments in AI regulation", "[[step_1_output]]"],
    "kwargs": {{}} // Defaults to "answer_query"
  }}
]
```

Example of a plan involving search (custom processing - summarization):
```json
[
  {{
    "tool_name": "search_duckduckgo",
    "args": ["recent papers on climate change impact on agriculture"],
    "kwargs": {{}}
  }},
  {{
    "tool_name": "process_search_results",
    "args": ["recent papers on climate change impact on agriculture", "[[step_1_output]]"],
    "kwargs": {{"processing_instruction": "summarize_results"}}
  }}
]
```
If the goal cannot be achieved with the available tools, or if it's unclear, return an empty JSON list [].

Respond ONLY with the JSON plan. Do not include any other text, comments, or explanations outside the JSON structure.
The entire response must be a single, valid JSON object (a list of steps).
JSON Plan:
"""
        
        CORRECTION_PROMPT_TEMPLATE = """Your previous attempt to generate a JSON plan had issues.
Original Goal: "{goal}"
Available Tools:
{tools_json_string}

Your Previous Incorrect Response:
---
{previous_llm_response}
---
Error Description: {error_description}

Please try again. Generate a plan as a JSON list of step dictionaries.
Each step *MUST* be a dictionary with "tool_name" (string from available tools), "args" (list of strings, use "" or "TODO_infer_arg_value" for missing values), and "kwargs" (dictionary of string:string, use {{}} if none).
Respond ONLY with the corrected JSON plan. The entire response must be a single, valid JSON list.
JSON Plan:
"""

        current_prompt = LLM_PLANNING_PROMPT_TEMPLATE.format(
            goal=goal_description, 
            project_context_section=project_context_section_str, # NEW
            tools_json_string=tools_json_string
        )

        while current_attempt <= MAX_CORRECTION_ATTEMPTS:
            model_for_planning = get_model_for_task("planning")
            print(f"PlannerAgent (LLM): Attempt {current_attempt + 1}/{MAX_CORRECTION_ATTEMPTS + 1}. Sending prompt to LLM (model: {model_for_planning})...")
            if current_attempt > 0 : # Only print full prompt for corrections, initial is too long
                 print(f"PlannerAgent (LLM): Correction prompt (first 500 chars):\n{current_prompt[:500]}...\n")
            
            llm_response_str = await invoke_ollama_model_async(current_prompt, model_name=model_for_planning)

            if not llm_response_str:
                last_error_description = f"Received no response or empty response from LLM ({model_for_planning})."
                print(f"PlannerAgent (LLM): {last_error_description}")
                current_attempt += 1
                if current_attempt <= MAX_CORRECTION_ATTEMPTS:
                    current_prompt = CORRECTION_PROMPT_TEMPLATE.format(
                        goal=goal_description, 
                        tools_json_string=tools_json_string, 
                        # Note: Correction prompt doesn't explicitly re-add project_context_section for brevity, assumes LLM remembers context from first fail.
                        previous_llm_response=llm_response_str or "", 
                        error_description=last_error_description
                    )
                continue # Try correction if attempts left

            print(f"PlannerAgent (LLM): Raw response from LLM (Attempt {current_attempt + 1}):\n---\n{llm_response_str}\n---")
            
            json_str_to_parse = llm_response_str
            # Sanitize the response
            match = re.search(r"```json\s*([\s\S]*?)\s*```", json_str_to_parse)
            if match:
                json_str_to_parse = match.group(1)
            
            json_str_to_parse = re.sub(r"^\s*JSON Plan:?\s*", "", json_str_to_parse.strip(), flags=re.IGNORECASE).strip()

            try:
                parsed_plan = json.loads(json_str_to_parse)
            except json.JSONDecodeError as e:
                last_error_description = f"Failed to parse JSON response. Error: {e}. Response: '{json_str_to_parse}'"
                print(f"PlannerAgent (LLM): {last_error_description}")
                current_attempt += 1
                if current_attempt <= MAX_CORRECTION_ATTEMPTS:
                    current_prompt = CORRECTION_PROMPT_TEMPLATE.format(
                        goal=goal_description, 
                        tools_json_string=tools_json_string, 
                        # Correction prompt doesn't explicitly re-add project_context_section
                        previous_llm_response=llm_response_str, 
                        error_description=f"Response was not valid JSON. Error: {e}"
                    )
                continue # Try correction

            # Validate plan structure
            if not isinstance(parsed_plan, list):
                last_error_description = f"LLM returned an invalid plan format - not a list. Got: {type(parsed_plan)}"
                print(f"PlannerAgent (LLM): {last_error_description}")
                current_attempt += 1
                if current_attempt <= MAX_CORRECTION_ATTEMPTS:
                     current_prompt = CORRECTION_PROMPT_TEMPLATE.format(
                        goal=goal_description, 
                        tools_json_string=tools_json_string, 
                        # Correction prompt doesn't explicitly re-add project_context_section
                        previous_llm_response=llm_response_str, 
                        error_description=last_error_description
                    )
                parsed_plan = None # Invalidate plan
                continue # Try correction

            validated_plan: List[Dict[str, Any]] = []
            valid_plan_overall = True
            for i, step in enumerate(parsed_plan):
                if not isinstance(step, dict):
                    last_error_description = f"Step {i+1} is not a dictionary. Content: {step}"
                    print(f"PlannerAgent (LLM): {last_error_description}")
                    valid_plan_overall = False; break
                
                tool_name = step.get("tool_name")
                args = step.get("args", []) 
                kwargs = step.get("kwargs", {}) 

                if not tool_name or not isinstance(tool_name, str):
                    last_error_description = f"Step {i+1} has missing or invalid 'tool_name'. Content: {step}"
                    print(f"PlannerAgent (LLM): {last_error_description}")
                    valid_plan_overall = False; break
                if tool_name not in available_tools:
                    last_error_description = f"Step {i+1} uses unavailable tool '{tool_name}'. Content: {step}"
                    print(f"PlannerAgent (LLM): {last_error_description}")
                    valid_plan_overall = False; break
                if not isinstance(args, list):
                    print(f"PlannerAgent (LLM): Warning - Step {i+1} 'args' for tool '{tool_name}' is not a list. Using empty list instead. Original: {args}")
                    args = []
                if not isinstance(kwargs, dict):
                    print(f"PlannerAgent (LLM): Warning - Step {i+1} 'kwargs' for tool '{tool_name}' is not a dictionary. Using empty dict instead. Original: {kwargs}")
                    kwargs = {}
                
                validated_args = [str(arg) for arg in args]
                validated_kwargs = {str(k): str(v) for k, v in kwargs.items()}

                validated_plan.append({
                    "tool_name": tool_name,
                    "args": tuple(validated_args), 
                    "kwargs": validated_kwargs
                })
            
            if valid_plan_overall:
                print(f"PlannerAgent (LLM): Successfully parsed and validated LLM plan (Attempt {current_attempt + 1}): {validated_plan}")
                return validated_plan
            else: # Plan was structurally okay as JSON list, but content failed validation
                current_attempt += 1
                if current_attempt <= MAX_CORRECTION_ATTEMPTS:
                    current_prompt = CORRECTION_PROMPT_TEMPLATE.format(
                        goal=goal_description, 
                        tools_json_string=tools_json_string, 
                        # Correction prompt doesn't explicitly re-add project_context_section
                        previous_llm_response=llm_response_str, 
                        error_description=last_error_description
                    )
                parsed_plan = None # Invalidate plan due to content errors
                continue # Try correction
        
        # All attempts failed
        print(f"PlannerAgent (LLM): All {MAX_CORRECTION_ATTEMPTS + 1} attempts to generate a valid plan failed. Last error: {last_error_description}")
        return [] # Return empty plan if all attempts fail

    async def replan_after_failure(self, original_goal: str, failure_analysis: str, available_tools: Dict[str, str], ollama_model_name: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        Attempts to create a new plan after a previous plan execution failed.
        Uses an LLM to generate the new plan based on the failure analysis.
        """
        
        LLM_REPLANNING_PROMPT_TEMPLATE = """The previous attempt to achieve a goal failed. You need to create a new plan.
Original Goal: "{original_goal}"

Analysis of the previous failure:
---
{failure_analysis}
---

Available Tools (tool_name: description):
{tools_json_string}

Based on the original goal and the failure analysis, generate a new plan to achieve the goal.
The plan *MUST* be a JSON list of step dictionaries.
Each step dictionary *MUST* contain "tool_name" (string), "args" (list of strings), and "kwargs" (dictionary of string:string).
If an argument value cannot be inferred, use an empty string "" or a placeholder like "TODO_infer_arg_value".
If you use 'search_duckduckgo', you *MUST* add a subsequent step 'process_search_results_for_answer' with "[[step_X_output]]" as an argument.

Consider the failure analysis carefully. Try to use different tools or different arguments if the previous attempt failed due to tool misuse.
If the goal seems unachievable with the available tools even considering the failure, return an empty JSON list [].

Respond ONLY with the JSON plan. Do not include any other text, comments, or explanations outside the JSON structure.
The entire response must be a single, valid JSON object (a list of steps).
JSON Plan:
"""
        MAX_CORRECTION_ATTEMPTS = 1 # 1 initial attempt + 1 correction attempt
        current_attempt = 0
        llm_response_str: Optional[str] = None
        last_error_description: str = "No response from LLM for re-planning."

        print(f"\nPlannerAgent (Re-plan): Attempting to re-plan for goal: '{original_goal}'")
        tools_json_string = json.dumps(available_tools, indent=2)
        
        model_for_replan = ollama_model_name or get_model_for_task("planning") # Use provided or default

        current_prompt = LLM_REPLANNING_PROMPT_TEMPLATE.format(
            original_goal=original_goal,
            failure_analysis=failure_analysis,
            tools_json_string=tools_json_string
        )

        # Using a simplified correction prompt for re-planning as the main prompt is already contextual.
        CORRECTION_PROMPT_TEMPLATE_REPLAN = """Your previous attempt to generate a JSON re-plan had issues.
Original Goal: "{goal}"
Failure Analysis: {failure_analysis}
Available Tools: {tools_json_string}
Your Previous Incorrect Response: --- {previous_llm_response} ---
Error Description: {error_description}
Please try again. Respond ONLY with the corrected JSON plan.
JSON Plan:
"""

        while current_attempt <= MAX_CORRECTION_ATTEMPTS:
            print(f"PlannerAgent (Re-plan): Attempt {current_attempt + 1}/{MAX_CORRECTION_ATTEMPTS + 1}. Sending prompt to LLM (model: {model_for_replan})...")
            if current_attempt > 0:
                 print(f"PlannerAgent (Re-plan): Correction prompt (first 500 chars):\n{current_prompt[:500]}...\n")

            llm_response_str = await invoke_ollama_model_async(current_prompt, model_name=model_for_replan)

            if not llm_response_str:
                last_error_description = f"Received no response or empty response from LLM ({model_for_replan}) during re-planning."
                print(f"PlannerAgent (Re-plan): {last_error_description}")
                current_attempt += 1
                if current_attempt <= MAX_CORRECTION_ATTEMPTS:
                    current_prompt = CORRECTION_PROMPT_TEMPLATE_REPLAN.format(
                        goal=original_goal,
                        failure_analysis=failure_analysis,
                        tools_json_string=tools_json_string,
                        previous_llm_response=llm_response_str or "",
                        error_description=last_error_description
                    )
                continue

            print(f"PlannerAgent (Re-plan): Raw response from LLM (Attempt {current_attempt + 1}):\n---\n{llm_response_str}\n---")
            
            json_str_to_parse = llm_response_str
            match = re.search(r"```json\s*([\s\S]*?)\s*```", json_str_to_parse)
            if match:
                json_str_to_parse = match.group(1)
            json_str_to_parse = re.sub(r"^\s*JSON Plan:?\s*", "", json_str_to_parse.strip(), flags=re.IGNORECASE).strip()

            try:
                parsed_plan = json.loads(json_str_to_parse)
            except json.JSONDecodeError as e:
                last_error_description = f"Failed to parse JSON response for re-plan. Error: {e}. Response: '{json_str_to_parse}'"
                print(f"PlannerAgent (Re-plan): {last_error_description}")
                current_attempt += 1
                if current_attempt <= MAX_CORRECTION_ATTEMPTS:
                    current_prompt = CORRECTION_PROMPT_TEMPLATE_REPLAN.format(
                        goal=original_goal,
                        failure_analysis=failure_analysis,
                        tools_json_string=tools_json_string,
                        previous_llm_response=llm_response_str,
                        error_description=f"Response was not valid JSON. Error: {e}"
                    )
                continue

            if not isinstance(parsed_plan, list):
                last_error_description = f"LLM returned an invalid re-plan format - not a list. Got: {type(parsed_plan)}"
                print(f"PlannerAgent (Re-plan): {last_error_description}")
                current_attempt += 1
                if current_attempt <= MAX_CORRECTION_ATTEMPTS:
                     current_prompt = CORRECTION_PROMPT_TEMPLATE_REPLAN.format(
                        goal=original_goal,
                        failure_analysis=failure_analysis,
                        tools_json_string=tools_json_string,
                        previous_llm_response=llm_response_str,
                        error_description=last_error_description
                    )
                parsed_plan = None 
                continue

            validated_plan: List[Dict[str, Any]] = []
            valid_plan_overall = True
            for i, step in enumerate(parsed_plan):
                if not isinstance(step, dict) or \
                   not step.get("tool_name") or not isinstance(step.get("tool_name"), str) or \
                   step.get("tool_name") not in available_tools:
                    last_error_description = f"Re-plan step {i+1} is invalid (not a dict, missing/invalid tool_name, or tool not available). Content: {step}"
                    print(f"PlannerAgent (Re-plan): {last_error_description}")
                    valid_plan_overall = False; break
                
                args = step.get("args", [])
                kwargs = step.get("kwargs", {})
                if not isinstance(args, list): args = []
                if not isinstance(kwargs, dict): kwargs = {}
                
                validated_args = [str(arg) for arg in args]
                validated_kwargs = {str(k): str(v) for k, v in kwargs.items()}

                validated_plan.append({
                    "tool_name": step["tool_name"],
                    "args": tuple(validated_args),
                    "kwargs": validated_kwargs
                })
            
            if valid_plan_overall:
                print(f"PlannerAgent (Re-plan): Successfully parsed and validated LLM re-plan (Attempt {current_attempt + 1}): {validated_plan}")
                return validated_plan
            else:
                current_attempt += 1
                if current_attempt <= MAX_CORRECTION_ATTEMPTS:
                    current_prompt = CORRECTION_PROMPT_TEMPLATE_REPLAN.format(
                        goal=original_goal,
                        failure_analysis=failure_analysis,
                        tools_json_string=tools_json_string,
                        previous_llm_response=llm_response_str,
                        error_description=last_error_description
                    )
                parsed_plan = None
                continue
        
        print(f"PlannerAgent (Re-plan): All {MAX_CORRECTION_ATTEMPTS + 1} attempts to generate a valid re-plan failed. Last error: {last_error_description}")
        return []


if __name__ == '__main__':
    # Example Usage and Test
    class MockToolSystem:
        def list_tools(self):
            return {
                "greet_user": "Greets the user. Args: name (str)",
                "add_numbers": "Adds two numbers. Args: a (str), b (str)", # Assume tools handle string conversion
                "multiply_numbers": "Multiplies two numbers. Args: x (str), y (str)",
                "no_op_tool": "Does nothing."
            }

    mock_ts = MockToolSystem()
    planner = PlannerAgent()

    print("\n--- Testing PlannerAgent with Argument Extraction ---")

    test_cases = [
        ("Please greet John", [{'tool_name': 'greet_user', 'args': ('John',), 'kwargs': {}}]),
        ("Say hello to Alice", [{'tool_name': 'greet_user', 'args': ('Alice',), 'kwargs': {}}]),
        ("Hi to Bob", [{'tool_name': 'greet_user', 'args': ('Bob',), 'kwargs': {}}]),
        ("greet user", [{'tool_name': 'greet_user', 'args': ('User',), 'kwargs': {}}]), # Default
        ("just greet", [{'tool_name': 'greet_user', 'args': ('User',), 'kwargs': {}}]), # Default
        ("Can you add 15 and 30 for me?", [{'tool_name': 'add_numbers', 'args': ('15', '30'), 'kwargs': {}}]),
        ("what is 25 plus 102.5?", [{'tool_name': 'add_numbers', 'args': ('25', '102.5'), 'kwargs': {}}]),
        ("sum of 7 and 3", [{'tool_name': 'add_numbers', 'args': ('7', '3'), 'kwargs': {}}]),
        ("add 5", [{'tool_name': 'add_numbers', 'args': ('0', '0'), 'kwargs': {'note': "Could not infer numbers for 'add_numbers' from 'add 5'. Using defaults."}}]), # Fallback, updated expected
        ("What is 5 times 3?", [{'tool_name': 'multiply_numbers', 'args': ('5', '3'), 'kwargs': {}}]),
        ("multiply 6 by 8.2", [{'tool_name': 'multiply_numbers', 'args': ('6', '8.2'), 'kwargs': {}}]),
        ("product of 10 and 4", [{'tool_name': 'multiply_numbers', 'args': ('10', '4'), 'kwargs': {}}]),
        ("multiply 9", [{'tool_name': 'multiply_numbers', 'args': ('1', '1'), 'kwargs': {'note': "Could not infer numbers for 'multiply_numbers' from 'multiply 9'. Using defaults."}}]), # Fallback, updated expected
        ("Do something complex", [{'tool_name': 'no_op_tool', 'args': (), 'kwargs': {"note": "No specific actions identified in the main goal."}}]), # Fallback to no_op, updated expected
        ("This is a test", [{'tool_name': 'no_op_tool', 'args': (), 'kwargs': {"note": "No specific actions identified in the main goal."}}]), # Fallback if no keywords match, updated expected
    ]

    all_tests_passed = True
    for i, (goal_desc, expected_plan) in enumerate(test_cases):
        print(f"\nTest Case {i+1}: '{goal_desc}'")
        generated_plan = planner.create_plan(goal_desc, mock_ts.list_tools())
        if generated_plan == expected_plan:
            print(f"PASS: Expected {expected_plan}")
        else:
            print(f"FAIL: Expected {expected_plan}, Got {generated_plan}")
            all_tests_passed = False
            
    # Test with a tool not existing for a keyword
    print("\nTest Case: Tool not available")
    planner_no_greet = PlannerAgent()
    mock_ts_no_greet_tools = mock_ts.list_tools().copy()
    del mock_ts_no_greet_tools["greet_user"] # Remove greet_user tool
    
    goal_desc_greet = "Greet the team"
    # Updated expected plan when a tool is missing and no_op_tool is available.
    expected_plan_no_greet = [{'tool_name': 'no_op_tool', 'args': (), 'kwargs': {"note": "No specific actions identified in the main goal."}}] 
    generated_plan_no_greet = planner_no_greet.create_plan(goal_desc_greet, mock_ts_no_greet_tools)
    if generated_plan_no_greet == expected_plan_no_greet:
        print(f"PASS (Tool not available): Expected {expected_plan_no_greet}")
    else:
        print(f"FAIL (Tool not available): Expected {expected_plan_no_greet}, Got {generated_plan_no_greet}")
        all_tests_passed = False

    print(f"\n--- PlannerAgent Tests Finished. All Passed: {all_tests_passed} ---")
    
    # Minimal async test for replan_after_failure (rudimentary)
    # Note: This requires a running Ollama instance.
    # import asyncio
    # async def test_replan():
    #     print("\n--- Testing replan_after_failure (requires Ollama) ---")
    #     test_planner = PlannerAgent()
    #     tools = mock_ts.list_tools()
    #     analysis = "The 'add_numbers' tool failed because the input 'apple' was not a number."
    #     goal = "add 5 and apple, then greet User"
    #     try:
    #         new_plan = await test_planner.replan_after_failure(goal, analysis, tools)
    #         print(f"Replan result for '{goal}': {new_plan}")
    #     except Exception as e:
    #         print(f"Error during replan_after_failure test: {e}")
    #         print("This test might fail if Ollama is not running or the model is not available.")

    # if __name__ == '__main__':
    #    asyncio.run(test_replan()) # Comment out if you don't want to run this test by default

# ### END FILE: ai_assistant/planning/planning.py ###

# ### START FILE: ai_assistant/project_management/manifest_schema.py ###
# Self-Evolving-Agent-feat-chat-history-context/ai_assistant/project_management/manifest_schema.py
from dataclasses import dataclass, field, asdict as dataclass_asdict # Import asdict
from typing import List, Dict, Any, Optional
import json # For the __main__ example

# Using Pydantic for actual validation would be better in a production system,
# but dataclasses are good for schema definition and type hinting.

@dataclass
class Dependency:
    name: str
    version: Optional[str] = None
    type: Optional[str] = None # e.g., "pip", "npm", "maven"
    notes: Optional[str] = None

@dataclass
class BuildConfig:
    build_command: Optional[str] = None
    output_directory: Optional[str] = None # Relative to project root
    source_directories: List[str] = field(default_factory=lambda: ["src"])

@dataclass
class TestConfig:
    test_command: Optional[str] = None
    test_directory: Optional[str] = field(default_factory=lambda: "tests") # Relative to project root

@dataclass
class DevelopmentTask:
    task_id: str
    task_type: str # e.g., "CREATE_FILE", "MODIFY_FILE", "ADD_FUNCTION", "INSTALL_DEPENDENCY"
    description: str
    details: Dict[str, Any] = field(default_factory=dict)
    status: str = "pending" # "pending", "in_progress", "completed", "failed"
    dependencies: List[str] = field(default_factory=list) # List of task_ids
    last_attempt_timestamp: Optional[str] = None
    error_message: Optional[str] = None
    notes: Optional[str] = None

@dataclass
class ProjectManifest:
    project_name: str
    sanitized_project_name: str # Filesystem-friendly
    project_directory: str # Relative to ai_generated_projects/
    project_description: str
    creation_timestamp: str # ISO format string
    last_modified_timestamp: str # ISO format string
    
    version: str = "0.1.0" # Project's own version
    manifest_version: str = "1.1.0" # Schema version (fixed for this definition)
    
    project_type: Optional[str] = None # e.g., "python", "javascript", "java", "general"
    entry_points: Dict[str, str] = field(default_factory=dict) # e.g., {"cli": "python src/main.py"}
    
    dependencies: List[Dependency] = field(default_factory=list)
    build_config: Optional[BuildConfig] = None
    test_config: Optional[TestConfig] = None
    
    project_goals: List[str] = field(default_factory=list) # More detailed objectives
    development_tasks: List[DevelopmentTask] = field(default_factory=list)
    
    project_notes: Optional[str] = None

    def to_json_dict(self) -> Dict[str, Any]:
        """Converts the manifest to a dictionary suitable for JSON serialization using dataclasses.asdict.
        This handles nested dataclasses correctly.
        """
        return dataclass_asdict(self)

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'ProjectManifest':
        """Creates a ProjectManifest instance from a dictionary.
        This needs to handle reconstruction of nested dataclasses.
        """
        # Basic reconstruction; more robust parsing might be needed for production
        # (e.g., handling date strings to datetime objects if they were stored as such,
        # or ensuring nested dicts are converted to their respective dataclasses).
        
        # Convert lists of dicts back to lists of dataclass instances
        dependencies_data = data.get('dependencies', [])
        data['dependencies'] = [Dependency(**dep_data) for dep_data in dependencies_data]

        build_config_data = data.get('build_config')
        if build_config_data:
            data['build_config'] = BuildConfig(**build_config_data)

        test_config_data = data.get('test_config')
        if test_config_data:
            data['test_config'] = TestConfig(**test_config_data)
            
        dev_tasks_data = data.get('development_tasks', [])
        data['development_tasks'] = [DevelopmentTask(**task_data) for task_data in dev_tasks_data]

        # Ensure manifest_version is correctly set if it was missing in data (though unlikely for new files)
        if 'manifest_version' not in data:
            data['manifest_version'] = "1.1.0"

        return cls(**data)

if __name__ == '__main__':
    # Example Usage:
    build_conf = BuildConfig(build_command="python setup.py build", source_directories=["app"])
    test_conf = TestConfig(test_command="pytest")
    
    dep1 = Dependency(name="requests", version="2.25.1", type="pip")
    
    task1 = DevelopmentTask(
        task_id="TASK001",
        task_type="CREATE_FILE",
        description="Create the main application file.",
        details={"filename": "app/main.py", "initial_code_prompt": "Create a basic Flask app."}
    )
    task2 = DevelopmentTask(
        task_id="TASK002",
        task_type="INSTALL_DEPENDENCY",
        description="Install Flask.",
        details={"name": "Flask", "version": "2.0.1"},
        dependencies=["TASK001"] # Example dependency
    )

    example_manifest = ProjectManifest(
        project_name="My Web App",
        sanitized_project_name="my_web_app",
        project_directory="ai_generated_projects/my_web_app",
        project_description="A simple web application.",
        creation_timestamp="2024-05-17T10:00:00Z",
        last_modified_timestamp="2024-05-17T10:00:00Z",
        project_type="python",
        entry_points={"web": "gunicorn app.main:app"},
        dependencies=[dep1],
        build_config=build_conf,
        test_config=test_conf,
        project_goals=["Provide a user-friendly interface.", "Ensure data privacy."],
        development_tasks=[task1, task2],
        project_notes="This is an example manifest."
    )

    # Convert to dictionary for JSON serialization
    manifest_dict_for_json = example_manifest.to_json_dict()
    print("--- Serialized Manifest (for JSON) ---")
    print(json.dumps(manifest_dict_for_json, indent=4))

    # Example of reconstructing from a dictionary (e.g., after json.load)
    # This assumes manifest_dict_for_json is what you'd get from json.loads(file_content)
    try:
        reloaded_manifest = ProjectManifest.from_dict(manifest_dict_for_json)
        print("\n--- Reloaded Manifest ---")
        print(f"Project Name: {reloaded_manifest.project_name}")
        print(f"Manifest Version: {reloaded_manifest.manifest_version}")
        if reloaded_manifest.dependencies:
            print(f"First Dependency Name: {reloaded_manifest.dependencies[0].name}")
        if reloaded_manifest.build_config:
            print(f"Build Command: {reloaded_manifest.build_config.build_command}")
        if reloaded_manifest.development_tasks:
            print(f"First Dev Task ID: {reloaded_manifest.development_tasks[0].task_id}")
    except Exception as e:
        print(f"Error during reconstruction: {e}")

# ### END FILE: ai_assistant/project_management/manifest_schema.py ###

# ### START FILE: ai_assistant/tools/__init__.py ###
# This file marks tools as a package.

# ### END FILE: ai_assistant/tools/__init__.py ###

# ### START FILE: ai_assistant/tools/tool_management_tools.py ###
# ai_assistant/tools/tool_management_tools.py
import asyncio
import os
import re
import importlib
import sys
from typing import Dict, Any

from ai_assistant.code_services.service import CodeService
from ai_assistant.llm_interface import ollama_client as default_llm_provider
from ai_assistant.core import self_modification as default_self_modification_service
from ai_assistant.core.fs_utils import write_to_file
from ai_assistant.tools.tool_system import tool_system_instance
from ai_assistant.core.reflection import global_reflection_log
from ai_assistant.config import is_debug_mode
from ai_assistant.utils.display_utils import CLIColors, color_text # For potential debug prints


async def generate_and_register_tool_backend(tool_description: str) -> Dict[str, Any]:
    """
    Handles the backend process of generating code via CodeService,
    saving it, and registering it as a tool. This version is less interactive
    than the CLI's _handle_code_generation_and_registration.
    """
    if is_debug_mode():
        print(color_text(f"[DEBUG TOOL_MGMT] Received request to generate tool: {tool_description}", CLIColors.DEBUG_MESSAGE))

    code_service = CodeService(
        llm_provider=default_llm_provider,
        self_modification_service=default_self_modification_service
    )
    generation_result = await code_service.generate_code(
        context="NEW_TOOL",
        prompt_or_description=tool_description,
        target_path=None  # CodeService might handle path suggestion or use defaults
    )

    if is_debug_mode():
        print(color_text(f"[DEBUG TOOL_MGMT] CodeService generation result: {generation_result}", CLIColors.DEBUG_MESSAGE))

    if generation_result.get("status") != "SUCCESS_CODE_GENERATED":
        error_msg = generation_result.get("error", "CodeService failed to generate code or parse metadata.")
        global_reflection_log.log_execution(
            goal_description=f"Automated tool generation failed (generation_error): {tool_description}",
            plan=[{'action_type': 'AUTO_TOOL_GEN_FAIL', 'description': tool_description}],
            execution_results=[f"CodeService failed. Status: {generation_result.get('status')}, Error: {error_msg}"],
            overall_success=False
        )
        return {"status": "error", "message": f"Tool generation failed: {error_msg}"}

    cleaned_code = generation_result.get("code_string")
    parsed_metadata = generation_result.get("metadata")

    if not cleaned_code or not parsed_metadata:
        global_reflection_log.log_execution(
            goal_description=f"Automated tool generation failed (missing_data): {tool_description}",
            plan=[{'action_type': 'AUTO_TOOL_GEN_FAIL', 'description': tool_description}],
            execution_results=["CodeService reported success but returned incomplete data (missing code/metadata)."],
            overall_success=False
        )
        return {"status": "error", "message": "Tool generation succeeded but metadata or code is missing."}

    # Simplified saving and registration using metadata or defaults
    function_name = parsed_metadata.get("suggested_function_name", "generated_tool_func")
    tool_name_from_meta = parsed_metadata.get("suggested_tool_name", "newly_generated_tool")
    description_for_reg = parsed_metadata.get("suggested_description", tool_description)

    safe_tool_name_base = re.sub(r'[^\w_]', '', tool_name_from_meta.lower())
    if not safe_tool_name_base: safe_tool_name_base = "custom_generated_tool"
    filename = f"{safe_tool_name_base}.py"
    filepath_to_save = os.path.join("ai_assistant", "custom_tools", filename)
    module_path_for_registration = f"ai_assistant.custom_tools.{safe_tool_name_base}"

    if not write_to_file(filepath_to_save, cleaned_code):
        return {"status": "error", "message": f"Failed to save generated code to {filepath_to_save}."}

    try:
        if module_path_for_registration in sys.modules:
            importlib.reload(sys.modules[module_path_for_registration])
        imported_module = importlib.import_module(module_path_for_registration)
        function_object = getattr(imported_module, function_name)
        tool_system_instance.register_tool(
            tool_name=tool_name_from_meta,
            description=description_for_reg,
            module_path=module_path_for_registration,
            function_name_in_module=function_name,
            tool_type="dynamic_generated", # Mark as generated by this process
            func_callable=function_object
        )
        return {"status": "success", "message": f"Tool '{tool_name_from_meta}' generated, saved to '{filepath_to_save}', and registered successfully."}
    except Exception as e:
        return {"status": "error", "message": f"Tool '{tool_name_from_meta}' generated and saved, but registration failed: {e}"}
# ### END FILE: ai_assistant/tools/tool_management_tools.py ###

# ### START FILE: ai_assistant/tools/tool_system.py ###
# ai_assistant/tools/tool_system.py
import importlib
import os
import sys
import json
import inspect
import asyncio
from typing import Callable, Dict, Any, Optional, Tuple, List
from ai_assistant.config import is_debug_mode, get_data_dir # Import get_data_dir
from ai_assistant.core.self_modification import get_function_source_code

# --- Constants ---
DEFAULT_TOOLS_FILE_DIR = get_data_dir() # Use centralized data directory from config
DEFAULT_TOOL_REGISTRY_FILE = os.path.join(DEFAULT_TOOLS_FILE_DIR, "tool_registry.json") # Standardized name


# --- Custom Exceptions ---
class ToolNotFoundError(Exception):
    """Raised when a tool is not found in the registry."""
    pass

class ToolAlreadyRegisteredError(Exception):
    """Raised when trying to register a tool that already exists."""
    pass

class ToolExecutionError(Exception):
    """Raised when a tool fails to load or execute."""
    pass

class ToolSystem:
    def __init__(self, tool_registry_file: Optional[str] = None):
        self._tool_registry: Dict[str, Dict[str, Any]] = {}
        # Import is_debug_mode here or ensure it's available if used in methods called by __init__
        self._persisted_tool_metadata_file = tool_registry_file or DEFAULT_TOOL_REGISTRY_FILE

        if is_debug_mode():
            print(f"ToolSystem: Initializing with registry file: {self._persisted_tool_metadata_file}")
        self.load_persisted_tools()
        self._register_system_tools()

        # Define a list of custom tool modules to discover
        # Each tuple is (module_import_path, friendly_filename_for_logging)
        custom_tool_modules_to_discover = [
            ("ai_assistant.custom_tools.my_extra_tools", "my_extra_tools.py"),
            ("ai_assistant.custom_tools.awareness_tools", "awareness_tools.py"),
            ("ai_assistant.custom_tools.config_management_tools", "config_management_tools.py"), # Added new module
            ("ai_assistant.custom_tools.conversational_tools", "conversational_tools.py"),
            ("ai_assistant.custom_tools.project_management_tools", "project_management_tools.py"),
            ("ai_assistant.custom_tools.project_execution_tools", "project_execution_tools.py"),
            ("ai_assistant.custom_tools.code_execution_tools", "code_execution_tools.py"),
            ("ai_assistant.custom_tools.file_system_tools", "file_system_tools.py"),
            ("ai_assistant.custom_tools.git_tools", "git_tools.py"),
            ("ai_assistant.custom_tools.knowledge_tools", "knowledge_tools.py"),
            ("ai_assistant.custom_tools.meta_programming_tools", "meta_programming_tools.py"), # For generate_new_tool_from_description
            ("ai_assistant.custom_tools.generated", "generated_tools_module"), # For tools created by generate_new_tool_from_description
        ]

        any_new_tools_registered_overall = False
        for module_import_path, module_filename in custom_tool_modules_to_discover:
            try:
                # Attempt to import the module
                module_to_inspect = importlib.import_module(module_import_path)
                # Discover and register tools from this module
                if self._discover_and_register_custom_tools(module_to_inspect, module_import_path):
                    if is_debug_mode():
                        print(f"ToolSystem: New custom tools discovered from {module_filename}. Triggering save.")
                    any_new_tools_registered_overall = True
            except ImportError:
                if is_debug_mode(): # Less noise if a custom module is optionally missing
                    print(f"ToolSystem: Could not import {module_filename} (path: {module_import_path}), skipping custom tool discovery from it.")
            except Exception as e:
                # Keep this as a regular print or change to logger.warning, as it's an unexpected error.
                print(f"ToolSystem: Warning - Error during custom tool discovery from {module_filename} (path: {module_import_path}): {e}")

        self.register_example_tools() # Register built-in example tools

        # Save the registry only if new tools were discovered and registered during this initialization
        if any_new_tools_registered_overall:
             self.save_registered_tools()
        if is_debug_mode():
            print(f"ToolSystem: Initialization complete. {len(self._tool_registry)} tools registered.")

    def _discover_and_register_custom_tools(self, module_to_inspect, module_path_str: str) -> bool:
        """
        Discovers and registers public functions from a given module as tools.
        Skips functions starting with '_' or not defined directly in the module.
        If a tool with the same name and module_path already exists, it's skipped.
        Returns True if any new tools were registered from this module, False otherwise.
        """
        new_tools_registered_in_this_module = False
        if is_debug_mode():
            print(f"ToolSystem: Discovering custom tools from module: {module_path_str}")
        for name, func_object in inspect.getmembers(module_to_inspect, inspect.isfunction):
            if name.startswith("_"): # Skip private/internal functions
                continue
            # Ensure the function is defined in the module being inspected, not imported into it
            if func_object.__module__ != module_to_inspect.__name__:
                continue

            # Check if this specific tool (name + module_path) is already registered (e.g., from persisted data)
            if name in self._tool_registry and self._tool_registry[name].get('module_path') == module_path_str:
                # Tool already loaded, likely from tools.json. No need to re-register from discovery.
                continue

            docstring = inspect.getdoc(func_object) or "No description available."
            # Ensure description is a single line for brevity in some displays, or take first line.
            first_line_of_docstring = docstring.splitlines()[0] if docstring else "No description available."

            if is_debug_mode():
                print(f"ToolSystem: Attempting to register discovered custom tool '{name}' from '{module_path_str}'.")
            try:
                self.register_tool(
                    tool_name=name,
                    description=first_line_of_docstring, # Use concise description
                    module_path=module_path_str, # Store the full Python import path
                    function_name_in_module=name, # The actual name of the function in its module
                    tool_type="custom_discovered", # Mark its origin
                    func_callable=func_object # Cache the callable for direct execution
                )
                new_tools_registered_in_this_module = True # Mark that a new tool was registered
                if is_debug_mode():
                    print(f"ToolSystem: Successfully registered custom tool '{name}'.")
            except ToolAlreadyRegisteredError as e:
                # This might happen if a tool with the same name but different origin (e.g. example tool) exists.
                # The actual warning about metadata difference is handled in register_tool.
                if is_debug_mode():
                    print(f"ToolSystem: Info during discovery for custom tool '{name}': {e}")
            except Exception as e: # Catch any other error during registration
                # This is an error, so it should probably always be visible or logged as an error.
                print(f"ToolSystem: Error - Failed to register discovered custom tool '{name}': {e}")

        if new_tools_registered_in_this_module:
            if is_debug_mode():
                print(f"ToolSystem: Finished discovery for '{module_path_str}'. New tools were registered in this pass.")
        else:
            if is_debug_mode():
                print(f"ToolSystem: Finished discovery for '{module_path_str}'. No new tools were registered in this pass. ")
        return new_tools_registered_in_this_module


    def _system_update_tool_metadata_impl(self, tool_name: str, new_description: Optional[str] = None) -> bool:
        """
        Implementation logic for updating a tool's metadata.
        Modifies self._tool_registry and persists changes.
        """
        if tool_name not in self._tool_registry:
            print(f"Error (system_update_tool_metadata): Tool '{tool_name}' not found in registry.")
            return False
        tool_entry = self._tool_registry[tool_name]
        updated = False
        if new_description is not None:
            if tool_entry.get('description') != new_description:
                tool_entry['description'] = new_description                
                if is_debug_mode():
                    print(f"SystemTool: Updated description for tool '{tool_name}'.")
                updated = True
            else:
                if is_debug_mode():
                    print(f"SystemTool: New description for '{tool_name}' is same as old; no update made.")
        # Placeholder for other metadata updates
        # if new_module_path is not None: tool_entry['module_path'] = new_module_path; updated = True
        # if new_function_name is not None: tool_entry['function_name'] = new_function_name; updated = True

        if updated:
            if self.save_registered_tools():
                if is_debug_mode():
                    print(f"SystemTool: Tool registry saved successfully after updating '{tool_name}'.")
                return True
            else: # pragma: no cover
                print(f"Error (system_update_tool_metadata): Failed to save tool registry after updating '{tool_name}'.")
                return False
        else:
            # No actual change was made, but operation is considered "successful" in terms of not erroring.
            return True


    def _register_system_tools(self):
        """Registers tools that are internal to the ToolSystem or for system management."""
        system_tool_entry = {
            'tool_name': "system_update_tool_metadata", # Ensure tool_name is part of the entry dict
            'description': "Updates the metadata of a registered tool, such as its description. For system use. Args: tool_name (str), new_description (str, optional)",
            'type': 'system_internal', # Mark as an internal system tool
            'module_path': self.__class__.__module__, # Points to this module (ai_assistant.tools.tool_system)
            'function_name': '_system_update_tool_metadata_impl', # The actual method name
            'callable_cache': self._system_update_tool_metadata_impl, # Cache the bound method
            'is_method_on_instance': True # Flag indicating it's a method of this ToolSystem instance
        }
        # Only register if not already present, or if re-registration is desired (e.g. to update cache)
        if "system_update_tool_metadata" not in self._tool_registry:
             self._tool_registry["system_update_tool_metadata"] = system_tool_entry
        # If it is already there, this ensures the callable_cache is for the current instance,
        # which is important if ToolSystem is re-instantiated.
        elif self._tool_registry["system_update_tool_metadata"].get('is_method_on_instance'):
            self._tool_registry["system_update_tool_metadata"]['callable_cache'] = self._system_update_tool_metadata_impl
        if is_debug_mode():
            print("ToolSystem: System tools registered.")


    def register_tool(
        self,
        tool_name: str,
        description: str,
        module_path: str,
        function_name_in_module: str,
        tool_type: str = "dynamic", # e.g., "builtin", "custom_discovered", "dynamic_llm_generated"
        func_callable: Optional[Callable] = None # Pre-resolved callable if available
    ) -> bool:
        """
        Registers a new tool or updates an existing one.
        If func_callable is provided, it's cached. Otherwise, it's loaded on first execution.
        """
        if tool_name in self._tool_registry:
            existing_tool = self._tool_registry[tool_name]
            # More robust check for re-registration: allow if all key metadata matches OR if it's an update.
            # For simplicity, if name exists, we'll treat it as an update if core details differ,
            # or just refresh the callable if provided.
            if (existing_tool['module_path'] != module_path or
                existing_tool['function_name'] != function_name_in_module or
                existing_tool['type'] != tool_type):
                if existing_tool['type'] == 'system_internal' and tool_type == 'system_internal' and func_callable is not None:
                    if is_debug_mode():
                        print(f"ToolSystem: Re-caching system tool '{tool_name}'.")
                elif is_debug_mode(): # pragma: no cover
                    # This error might be too strict if we want to allow overriding a builtin with a custom tool of same name.
                    # For now, let's log a warning and proceed with update.
                    print(
                        # Changed to print from logger.warning to avoid logger setup dependency here
                        f"ToolSystem: Info - Tool '{tool_name}' already registered with different metadata "
                        f"(module: {existing_tool['module_path']}, func: {existing_tool['function_name']}, type: {existing_tool['type']}). "
                        f"Updating with new details: (module: {module_path}, func: {function_name_in_module}, type: {tool_type})."
                    )
            # Always log if a tool is being "updated" (even if just re-registering with same info)
            if is_debug_mode():
                print(f"ToolSystem: Tool '{tool_name}' is being re-registered/updated. Description: '{description}'")
        tool_entry = {
            "tool_name": tool_name, # Added tool_name here for consistency
            "module_path": module_path,
            "function_name": function_name_in_module, # Name of the function within its module
            "description": description,
            "type": tool_type,
            "callable_cache": func_callable # Store the callable if provided, else None
        }
        self._tool_registry[tool_name] = tool_entry
        return True

    def remove_tool(self, name: str) -> bool:
        """Removes a registered tool. Returns True if successful."""
        if name in self._tool_registry:
            del self._tool_registry[name]
            if is_debug_mode():
                print(f"ToolSystem: Tool '{name}' removed from registry.")
            return True
        else:
            if is_debug_mode():
                print(f"ToolSystem: Tool '{name}' not found in registry. Cannot remove.")
            return False

    def get_tool(self, name: str) -> Optional[Dict[str, Any]]:
        """Retrieves tool metadata from the registry."""
        return self._tool_registry.get(name)

    async def execute_tool(self, name: str, args: Tuple = (), kwargs: Optional[Dict[str, Any]] = None) -> Any:
        """
        Executes a registered tool by its name.
        Loads the tool function dynamically if not already cached.
        Handles both synchronous and asynchronous tool functions.
        """
        if kwargs is None:
            kwargs = {}

        tool_info = self._tool_registry.get(name)
        if not tool_info:
            raise ToolNotFoundError(f"Tool '{name}' not found.")

        func_to_execute = tool_info.get('callable_cache')

        # If the function is not cached, load it dynamically
        if not func_to_execute:
            module_path = tool_info['module_path']
            function_name = tool_info['function_name']
            if is_debug_mode():
                print(f"ToolSystem: Tool '{name}': Function not cached. Attempting to load from {module_path}.{function_name}")
            try:
                # Dynamically import the module
                # Using asyncio.to_thread for the import part as importlib can sometimes block
                module = await asyncio.to_thread(importlib.import_module, module_path)
                # Get the function from the module
                func_to_execute = getattr(module, function_name)
                # Cache the loaded function for future calls
                self._tool_registry[name]['callable_cache'] = func_to_execute
                if is_debug_mode():
                    print(f"ToolSystem: Tool '{name}': Function loaded and cached successfully.")
            except ModuleNotFoundError: # pragma: no cover
                raise ToolExecutionError(f"Could not load function for tool '{name}': Module '{module_path}' not found.")
            except AttributeError: # pragma: no cover
                raise ToolExecutionError(f"Could not load function for tool '{name}': Function '{function_name}' not found in '{module_path}'.")
            except Exception as e: # pragma: no cover
                raise ToolExecutionError(f"Could not load function for tool '{name}': An unexpected error occurred - {e}")

        if not callable(func_to_execute): # pragma: no cover
             raise ToolExecutionError(f"Tool '{name}': Loaded attribute '{tool_info['function_name']}' is not callable.")

        # Execute the function
        try:
            if is_debug_mode():
                print(f"ToolSystem: Executing tool '{name}' with args={args}, kwargs={kwargs}")
            # Check if the function is an async function
            if inspect.iscoroutinefunction(func_to_execute):
                result = await func_to_execute(*args, **kwargs)
            else:
                # Run synchronous function in a separate thread to avoid blocking asyncio event loop
                result = await asyncio.to_thread(func_to_execute, *args, **kwargs)
            if is_debug_mode():
                print(f"ToolSystem: Tool '{name}' executed successfully. Result (first 200 chars): {str(result)[:200]}")
            return result
        except Exception as e: # pragma: no cover
            print(f"ToolSystem: Error during execution of tool '{name}': {type(e).__name__} - {e}")
            # Consider re-raising a more specific ToolExecutionError or the original error
            raise ToolExecutionError(f"Error during execution of tool '{name}': {e}") from e


    def list_tools(self) -> Dict[str, str]:
        """Returns a dictionary of tool names to their descriptions."""
        return {name: tool["description"] for name, tool in self._tool_registry.items()}


    def save_registered_tools(self) -> bool:
        """Saves the metadata of all registered tools (excluding callables) to a JSON file."""
        # Ensure the 'data' directory exists
        os.makedirs(DEFAULT_TOOLS_FILE_DIR, exist_ok=True)

        data_to_save = {}
        for name, tool_data in self._tool_registry.items():
            # Do not persist system_internal tools with bound methods in callable_cache
            if tool_data.get('type') == 'system_internal' and tool_data.get('is_method_on_instance') and is_debug_mode():
                print(f"ToolSystem: Debug - Skipping persistence of system_internal tool '{name}' with instance method.")
                continue

            # Create a copy and remove the non-serializable 'callable_cache'
            serializable_data = tool_data.copy()
            serializable_data.pop('callable_cache', None) # Remove if exists
            serializable_data.pop('is_method_on_instance', None) # Remove helper flag if exists
            data_to_save[name] = serializable_data

        # Specific debug print for the problematic tool before saving
        if "get_self_awareness_info_and_converse" in data_to_save:
            print(f"[DEBUG SAVE TOOL_SYSTEM] Saving 'get_self_awareness_info_and_converse' with module_path: {data_to_save['get_self_awareness_info_and_converse'].get('module_path')}")
        elif is_debug_mode(): # Only print if debug mode is on and the tool isn't in data_to_save
            print("[DEBUG SAVE TOOL_SYSTEM] 'get_self_awareness_info_and_converse' not in data_to_save (it might be a system_internal tool with instance method).")

        try:
            with open(self._persisted_tool_metadata_file, 'w', encoding='utf-8') as f:
                json.dump(data_to_save, f, indent=4)
            if is_debug_mode():
                print(f"ToolSystem: Tools saved successfully to {self._persisted_tool_metadata_file}")
            return True
        except IOError as e: # pragma: no cover
            print(f"ToolSystem: Error saving tools to {self._persisted_tool_metadata_file}: {e}")
            return False
        except Exception as e: # pragma: no cover
            print(f"ToolSystem: Unexpected error saving tools: {e}")
            return False


    def load_persisted_tools(self):
        """Loads tool metadata from the persisted JSON file and registers them."""
        if is_debug_mode():
            print(f"ToolSystem: Loading persisted tools from '{self._persisted_tool_metadata_file}'...")
        if not os.path.exists(self._persisted_tool_metadata_file):
            # This is a common scenario on first run, so make it less verbose if not in debug.
            if is_debug_mode():
                print(f"ToolSystem: Tools file '{self._persisted_tool_metadata_file}' not found. No tools loaded from persistence.")
            return

        try:
            with open(self._persisted_tool_metadata_file, 'r', encoding='utf-8') as f:
                content = f.read()
                if not content: # Handle empty file
                    if is_debug_mode():
                        print("ToolSystem: Tools file is empty. No tools loaded from persistence.")
                    return
                loaded_tool_metadata = json.loads(content)
        except json.JSONDecodeError as e: # pragma: no cover
            print(f"ToolSystem: Error decoding JSON from '{self._persisted_tool_metadata_file}': {e}. No tools loaded from persistence.")
            return
        except IOError as e: # pragma: no cover
            print(f"ToolSystem: Error reading tools file '{self._persisted_tool_metadata_file}': {e}. No tools loaded from persistence.")
            return

        # Specific debug print for the problematic tool after loading
        if loaded_tool_metadata and "get_self_awareness_info_and_converse" in loaded_tool_metadata:
            print(f"[DEBUG LOAD TOOL_SYSTEM] Loaded 'get_self_awareness_info_and_converse' with module_path: {loaded_tool_metadata['get_self_awareness_info_and_converse'].get('module_path')}")
        elif loaded_tool_metadata and is_debug_mode():
            print("[DEBUG LOAD TOOL_SYSTEM] 'get_self_awareness_info_and_converse' not found in loaded_tool_metadata from file.")
        elif is_debug_mode(): # Only print if debug and no metadata loaded
            print("[DEBUG LOAD TOOL_SYSTEM] loaded_tool_metadata is empty or None after attempting to read file.")

        if not loaded_tool_metadata: # Should be caught by empty content check, but good failsafe
            if is_debug_mode():
                print("ToolSystem: No tools data found in file. Skipping load from persistence.")
            return

        loaded_count = 0
        for tool_name, tool_data in loaded_tool_metadata.items():
            # Skip system_internal tools that are instance methods, as they are registered by _register_system_tools
            if tool_data.get('type') == 'system_internal' and tool_data.get('is_method_on_instance'):
                if is_debug_mode():
                    print(f"ToolSystem: Skipping load of system_internal tool '{tool_name}' from file; will be instance-registered.")
                continue

            try:
                # Ensure all necessary keys are present for registration
                # 'tool_name' is the key in loaded_tool_metadata, so it's implicitly present.
                # We use tool_data.get('tool_name', tool_name) to be safe if tool_name wasn't stored inside tool_data itself.
                self.register_tool(
                    tool_name=tool_data.get('tool_name', tool_name), # Use key if internal 'tool_name' is missing
                    description=tool_data['description'],
                    module_path=tool_data['module_path'],
                    function_name_in_module=tool_data['function_name'],
                    tool_type=tool_data.get('type', 'dynamic'), # Default to 'dynamic' if type is missing
                    func_callable=None # Callables are loaded on demand for persisted tools
                )
                loaded_count +=1
            except ToolAlreadyRegisteredError as e: # pragma: no cover
                # This is expected if example tools are also in the JSON file, allow update.
                if is_debug_mode():
                    print(f"ToolSystem: Info while loading persisted tool '{tool_name}': {e}")
            except KeyError as e: # pragma: no cover
                print(f"ToolSystem: Error - Persisted tool '{tool_name}' has missing metadata: {e}. Skipping.")
            except Exception as e: # pragma: no cover
                print(f"ToolSystem: Error loading persisted tool '{tool_name}': {e}. Skipping.")
        if is_debug_mode() or loaded_count > 0 : # Print if debug or if any tools were actually loaded
            print(f"ToolSystem: Successfully processed {loaded_count} persisted tools from file.")

    # Import is_debug_mode at the top of the file
    def register_example_tools(self):
        """Registers a set of example tools. Idempotent."""
        current_module_obj = sys.modules[self.__class__.__module__]

        example_tools_data = [
            ("greet_user", "Greets the user. Args: name (str)", "_example_greet_user"),
            ("add_numbers", "Adds two integers. Args: a (int), b (int)", "_example_add_numbers"),
            ("multiply_numbers", "Multiplies two floats. Args: x (float), y (float)", "_example_multiply_numbers"),
            ("no_op_tool", "Does nothing, useful for default plans.", "_example_no_op_tool"),
            ("view_function_code", "Retrieves the source code of a specified function. Inputs: module_path (str), function_name (str).", "_tool_view_function_code"),
            ("simulate_edit_function_code", "Simulates editing source code. Inputs: module_path (str), function_name (str), new_code_block (str).", "_tool_simulate_edit_function_code"),
            ("maybe_fail_tool", "A tool that fails on its 1st, 3rd, etc. call and succeeds on its 2nd, 4th, etc. call.", "_example_maybe_fail_tool"),
        ]
        for tool_name, description, func_name_str in example_tools_data:
            try:
                func_callable = getattr(current_module_obj, func_name_str, None)
                if not func_callable or not callable(func_callable): # pragma: no cover
                    print(f"ToolSystem: Error - Example tool function '{func_name_str}' not found or not callable in {current_module_obj.__name__}. Skipping.")
                    continue

                self.register_tool(
                    tool_name=tool_name,
                    description=description,
                    module_path=current_module_obj.__name__,
                    function_name_in_module=func_name_str,
                    tool_type="builtin", # Mark as a built-in example tool
                    func_callable=func_callable # Cache the callable
                )
            except ToolAlreadyRegisteredError as e: # pragma: no cover
                # This is expected if tools were loaded from persistence first
                if is_debug_mode():
                    print(f"ToolSystem: Info while registering example tool '{tool_name}': {e}")
            except Exception as e: # pragma: no cover
                print(f"ToolSystem: Error registering example tool {tool_name}: {e}")
        if is_debug_mode():
            print(f"ToolSystem: Example tools registration attempt finished.")
            
# --- Example Tool Function Definitions ---
# These must be at the module level for getattr to find them.
def _example_greet_user(name: str) -> str:
    return f"Hello, {name}!"
def _example_add_numbers(a: int, b: int) -> int:
    try: 
        return int(a) + int(b)
    except ValueError:
        raise ValueError("'a' and 'b' must be integers.")
def _example_multiply_numbers(x: float, y: float) -> float:
    try:
        return float(x) * float(y)
    except ValueError:
        raise ValueError("'x' and 'y' must be floats.")
def _example_no_op_tool() -> str:
    return "No-op tool executed successfully."
_maybe_fail_tool_counter = 0
def _example_maybe_fail_tool() -> str:
    global _maybe_fail_tool_counter
    _maybe_fail_tool_counter += 1
    if _maybe_fail_tool_counter % 2 != 0:
        raise ValueError(f"Intentional failure from maybe_fail_tool on call #{_maybe_fail_tool_counter}!")
    return f"maybe_fail_tool succeeded on call #{_maybe_fail_tool_counter}."
def _tool_view_function_code(module_path: str, function_name: str) -> str:
    source_code = get_function_source_code(module_path, function_name)
    if source_code is None:
        return "Error: Could not retrieve source code. Module or function not found, or source not available."
    return source_code
def _tool_simulate_edit_function_code(module_path: str, function_name: str, new_code_block: str) -> str:
    # This is a placeholder. Actual edit_function_source_code might do more.
    print(f"Simulating edit for {module_path}.{function_name} with:\n{new_code_block}")
    return f"Simulation of code edit for '{module_path}.{function_name}' completed. No actual changes made by this simulation tool."

# Global instance, initialized once when the module is first imported.
tool_system_instance = ToolSystem()

# --- Module-level convenience functions (wrappers around the instance methods) ---
def register_tool(
    tool_name: str, description: str, module_path: str, function_name_in_module: str,
    tool_type: str = "dynamic", func_callable: Optional[Callable] = None
) -> bool:
    return tool_system_instance.register_tool(
        tool_name, description, module_path, function_name_in_module, tool_type, func_callable
    )

def remove_tool(name: str) -> bool:
    """Removes a registered tool. Returns True if successful."""
    return tool_system_instance.remove_tool(name)

def get_tool(name: str) -> Optional[Dict[str, Any]]:
    return tool_system_instance.get_tool(name)

async def execute_tool(name: str, args: Tuple = (), kwargs: Optional[Dict[str, Any]] = None) -> Any:
    return await tool_system_instance.execute_tool(name, args, kwargs)
def list_tools() -> Dict[str, str]:
    return tool_system_instance.list_tools()
def save_registered_tools() -> bool: # Should primarily be called internally by ToolSystem
    return tool_system_instance.save_registered_tools()
def load_persisted_tools(): # For explicit reload if needed, __init__ handles initial
    tool_system_instance.load_persisted_tools()
def register_example_tools(): # For explicit re-registration if needed
    tool_system_instance.register_example_tools()

async def main_test(): # pragma: no cover
    print("\n--- ToolSystem Direct Execution Test (using global instance) ---")
    print("Listing tools from the globally initialized instance:")
    all_tools = list_tools()
    for t_name, t_desc in all_tools.items():
        print(f"  - {t_name}: {t_desc[:70]}...")

    print("\nTesting execution of 'greet_user' tool:")
    try:
        greeting = await execute_tool("greet_user", args=("ModuleTester",))
        print(f"Greeting result: {greeting}")
    except Exception as e:
        print(f"Error executing greet_user: {e}")

    print("\nTesting execution of 'add_numbers' tool:")
    try:
        sum_result = await execute_tool("add_numbers", args=(5, "7")) # "7" to test type conversion
        print(f"Sum result: {sum_result}")
    except Exception as e:
        print(f"Error executing add_numbers: {e}")

    print("\nTesting new 'manage_auto_approve_list' tool (if registered):")
    if "manage_auto_approve_list" in all_tools:
        try:
            list_result = await execute_tool("manage_auto_approve_list", args=("list",))
            print(f"Manage auto-approve list result: {list_result}")
            add_result = await execute_tool("manage_auto_approve_list", args=("add", "greet_user"))
            print(f"Add 'greet_user' to auto-approve: {add_result}")
            list_after_add = await execute_tool("manage_auto_approve_list", args=("list",))
            print(f"List after add: {list_after_add}")
        except Exception as e:
            print(f"Error executing manage_auto_approve_list: {e}")
    else:
        print("'manage_auto_approve_list' not found in registered tools for this test run.")


    print("\n--- ToolSystem Direct Execution Test Finished ---")

if __name__ == '__main__': # pragma: no cover
    # This ensures that if the script is run directly, the async main_test runs.
    # Note: ToolSystem is instantiated globally, so its __init__ runs on import.
    # This __main__ block then calls functions on that global instance.
    asyncio.run(main_test())

# ### END FILE: ai_assistant/tools/tool_system.py ###

# ### START FILE: ai_assistant/utils/__init__.py ###

# ### END FILE: ai_assistant/utils/__init__.py ###

# ### START FILE: ai_assistant/utils/display_utils.py ###
from prompt_toolkit.formatted_text import ANSI

class CLIColors:
    # Base colors with better contrast
    USER_INPUT = '\033[38;5;111m'     # Soft blue for user input
    AI_RESPONSE = '\033[38;5;156m'    # Mint green for AI responses
    SYSTEM_MESSAGE = '\033[38;5;222m'  # Warm yellow for system messages
    ERROR_MESSAGE = '\033[38;5;203m'   # Coral red for errors
    DEBUG_MESSAGE = '\033[38;5;245m'   # Medium grey for debug
    TOOL_OUTPUT = '\033[38;5;123m'     # Bright cyan for tool output
    SUCCESS = '\033[38;5;149m'         # Soft green for success
    FAILURE = '\033[38;5;196m'         # Deep red for failures
    WARNING = '\033[38;5;214m'         # Orange for warnings
    
    # Special formatting
    BOLD = '\033[1m'
    DIM = '\033[2m'
    ITALIC = '\033[3m'
    UNDERLINE = '\033[4m'
    
    # Status indicators
    THINKING = '\033[38;5;147m'        # Purple for thinking/processing
    INPUT_PROMPT = '\033[38;5;117m'    # Light blue for input prompt
    COMMAND = '\033[38;5;208m'         # Orange for commands
    BORDER = '\033[38;5;240m'          # Dark grey for borders/separators
    TOOL_NAME = "\033[94m"     # Bright Blue (same as SYSTEM_MESSAGE, can be distinct e.g. Light Blue \033[94m)
    TOOL_ARGS = "\033[36m"     # Cyan (regular, distinct from TOOL_OUTPUT if desired)
    
    BLUE = '\033[34m'          # Standard ANSI blue for the new prompt
    # Component-specific colors (for debug mode)
    PLANNER = '\033[38;5;105m'    # Purple for planner
    REVIEWER = '\033[38;5;208m'   # Orange for reviewer
    THINKER = '\033[38;5;39m'     # Blue for thinking process
    EXECUTOR = '\033[38;5;34m'    # Green for executor
    ACTION_PLAN = '\033[38;5;226m' # Yellow for final action plan
    
    END_COLOR = '\033[0m'         # Resets the color

def color_text(text: str, color_code: str) -> str:
    """Add color to text with proper reset"""
    return f"{color_code}{text}{CLIColors.END_COLOR}"

def format_header(text: str) -> ANSI:
    """Format a section header with borders"""
    width = 60
    border = color_text('─' * width, CLIColors.BORDER)
    padded_text = text.center(width - 2)
    final_str = f"\n{border}\n{color_text(f' {padded_text} ', CLIColors.BOLD + CLIColors.SYSTEM_MESSAGE)}\n{border}"
    return ANSI(final_str)

def format_message(prefix: str, message: str, color: str, show_prefix: bool = True) -> ANSI:
    """Format a message with proper indentation and optional prefix"""
    prefix_str = f"{color_text(prefix, color)} " if show_prefix else ""
    indented_message = message.replace('\n', f'\n{"  " if show_prefix else ""}')
    final_str = f"{prefix_str}{color_text(indented_message, color)}"
    return ANSI(final_str)

def format_input_prompt() -> ANSI:
    """Format the input prompt with a visually appealing indicator"""
    # Original prompt:
    # arrow = color_text("→", CLIColors.INPUT_PROMPT + CLIColors.BOLD)
    # return f"\n{arrow} {color_text('', CLIColors.USER_INPUT)}"
    # New prompt: A blue ">" followed by a space
    return ANSI(f"{CLIColors.BLUE}>{CLIColors.END_COLOR} ")

def format_thinking() -> ANSI:
    """Format the 'thinking' indicator"""
    final_str = color_text("⋯ thinking", CLIColors.THINKING + CLIColors.ITALIC)
    return ANSI(final_str)

def format_tool_execution(tool_name: str) -> ANSI:
    """Format tool execution message"""
    final_str = color_text(f"[{tool_name}]", CLIColors.TOOL_OUTPUT + CLIColors.BOLD)
    return ANSI(final_str)

def format_status(status: str, success: bool = True) -> ANSI:
    """Format a status message"""
    icon = "✓" if success else "✗"
    color = CLIColors.SUCCESS if success else CLIColors.FAILURE
    final_str = color_text(f"{icon} {status}", color + CLIColors.BOLD)
    return ANSI(final_str)

def draw_separator() -> ANSI:
    """Draw a subtle separator line"""
    final_str = color_text("─" * 60, CLIColors.BORDER + CLIColors.DIM)
    return ANSI(final_str)

def format_component_output(component: str, message: str, is_thinking: bool = False) -> ANSI:
    """Format output from different AI components (planner, reviewer, etc)
    
    Args:
        component: The name of the component (planner, reviewer, etc)
        message: The message to format
        is_thinking: Whether this is part of the thinking process
    """
    from ai_assistant.config import is_debug_mode, THINKING_CONFIG
    
    if not is_debug_mode():
        if is_thinking and not THINKING_CONFIG["display"]["show_in_release"]:
            return ANSI("")
        return ANSI(message)
        
    component_colors = {
        "planner": CLIColors.PLANNER,
        "reviewer": CLIColors.REVIEWER,
        "executor": CLIColors.EXECUTOR,
        "thinker": CLIColors.THINKER
    }
    
    prefix = THINKING_CONFIG["components"].get(component.lower(), "")
    color = component_colors.get(component.lower(), CLIColors.AI_RESPONSE)
    
    # Special handling for thinking process
    if is_thinking:
        if not THINKING_CONFIG["display"]["show_working"]:
            return ANSI("")
        steps = message.split("\n")
        formatted_steps = []
        for step in steps:
            formatted_steps.append(color_text(f"{THINKING_CONFIG['display']['step_prefix']}{step}", color))
        final_str = "\n".join(formatted_steps)
        return ANSI(final_str)
    
    final_str = color_text(f"{prefix}{message}", color)
    return ANSI(final_str)
# ### END FILE: ai_assistant/utils/display_utils.py ###

# ### START FILE: tests/__init__.py ###
# This file marks tests as a package.

# ### END FILE: tests/__init__.py ###

# ### START FILE: tests/test_action_executor.py ###
import unittest
from unittest import mock
from unittest.mock import patch, AsyncMock
import asyncio
import os
import sys
import uuid
import datetime
from dataclasses import dataclass, field

try:
    from ai_assistant.execution.action_executor import ActionExecutor
    from ai_assistant.core.reflection import ReflectionLogEntry, global_reflection_log as core_global_reflection_log
    from ai_assistant.planning.execution import ExecutionAgent
    from ai_assistant.code_services.service import CodeService # Added for mocking
except ImportError: # pragma: no cover
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
    if project_root not in sys.path:
        sys.path.insert(0, project_root)
    from ai_assistant.execution.action_executor import ActionExecutor
    from ai_assistant.core.reflection import ReflectionLogEntry, global_reflection_log as core_global_reflection_log
    from ai_assistant.planning.execution import ExecutionAgent
    from ai_assistant.code_services.service import CodeService


@dataclass
class MockReflectionLogEntryForTest:
    entry_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    goal_description: str = "Mock Goal"
    plan: Optional[List[Dict[str, Any]]] = None
    execution_results: Optional[List[Any]] = None
    status: str = "UNKNOWN"
    notes: Optional[str] = ""
    timestamp: datetime.datetime = field(default_factory=lambda: datetime.datetime.now(datetime.timezone.utc))
    error_type: Optional[str] = None
    error_message: Optional[str] = None
    traceback_snippet: Optional[str] = None
    is_self_modification_attempt: bool = False
    source_suggestion_id: Optional[str] = None
    modification_type: Optional[str] = None
    modification_details: Optional[Dict[str, Any]] = None
    post_modification_test_passed: Optional[bool] = None
    post_modification_test_details: Optional[Dict[str, Any]] = None
    commit_info: Optional[Dict[str, Any]] = None

class TestActionExecutor(unittest.TestCase):

    def setUp(self):
        self.executor = ActionExecutor()
        self.original_log_entries = list(core_global_reflection_log.log_entries)
        core_global_reflection_log.log_entries = []

    def tearDown(self):
        core_global_reflection_log.log_entries = self.original_log_entries

    @patch('ai_assistant.execution.action_executor.self_modification.edit_function_source_code')
    @patch('ai_assistant.execution.action_executor.global_reflection_log.log_execution')
    @patch.object(ActionExecutor, '_run_post_modification_test', new_callable=AsyncMock)
    def test_execute_action_propose_tool_modification_success_with_test_pass(self, mock_run_post_mod_test, mock_log_execution, mock_edit_code):
        mock_edit_code.return_value = "Successfully modified function 'test_func' in module 'test_module'."
        mock_run_post_mod_test.return_value = (True, "Post-modification test passed successfully.")

        action_details = {
            "module_path": "test_module.py", "function_name": "test_func", "tool_name": "test_tool",
            "suggested_code_change": "def test_func(): pass # new code",
            "original_reflection_entry_id": "entry_id_for_test_pass"
        }
        proposed_action = {"source_insight_id": "insight1", "action_type": "PROPOSE_TOOL_MODIFICATION", "details": action_details}

        result = asyncio.run(self.executor.execute_action(proposed_action))

        self.assertTrue(result)
        mock_edit_code.assert_called_once()
        mock_run_post_mod_test.assert_called_once()
        final_log_call_args = mock_log_execution.call_args_list[-1].kwargs
        self.assertTrue(final_log_call_args.get('overall_success'))
        self.assertTrue(final_log_call_args.get('post_modification_test_passed'))
        self.assertEqual(final_log_call_args.get('modification_details', {}).get('source_of_code'), "Insight")


    @patch('ai_assistant.execution.action_executor.global_reflection_log.log_execution')
    @patch('ai_assistant.execution.action_executor.self_modification.edit_function_source_code')
    @patch.object(ActionExecutor, '_run_post_modification_test', new_callable=AsyncMock)
    def test_tool_mod_via_codeservice_and_test_pass(self, mock_run_post_mod_test, mock_edit_code, mock_log_execution):
        # Mock the CodeService's modify_code method on the executor's instance
        self.executor.code_service = mock.AsyncMock(spec=CodeService)
        self.executor.code_service.modify_code.return_value = {
            "status": "SUCCESS_CODE_GENERATED",
            "modified_code_string": "def new_llm_func(): return 'fixed_by_codeservice'"
        }
        mock_edit_code.return_value = "Successfully modified function 'old_func'."
        mock_run_post_mod_test.return_value = (True, "Post-CodeService-mod test passed.")

        action_details = {
            "module_path": "test_module.py", "function_name": "old_func", "tool_name": "test_tool",
            "suggested_change_description": "Needs a fix via CodeService.",
            # NO suggested_code_change, to trigger the CodeService path
            "original_reflection_entry_id": "dummy_ref_id_cs"
        }
        proposed_action = {"source_insight_id": "insight_cs_llm", "action_type": "PROPOSE_TOOL_MODIFICATION", "details": action_details}

        result = asyncio.run(self.executor.execute_action(proposed_action))

        self.assertTrue(result)
        self.executor.code_service.modify_code.assert_called_once_with(
            context="SELF_FIX_TOOL",
            modification_instruction="Needs a fix via CodeService.",
            module_path="test_module.py",
            function_name="old_func",
            existing_code=None # ActionExecutor calls CodeService with existing_code=None
        )
        mock_edit_code.assert_called_once_with(module_path="test_module.py", function_name="old_func", new_code_string="def new_llm_func(): return 'fixed_by_codeservice'")
        mock_run_post_mod_test.assert_called_once()

        code_service_log_call = next(call for call in mock_log_execution.call_args_list if call.kwargs.get('status_override') == "CODE_SERVICE_GEN_SUCCESS")
        self.assertIsNotNone(code_service_log_call)
        self.assertTrue(code_service_log_call.kwargs.get('overall_success'))

        final_log_call_args = mock_log_execution.call_args_list[-1].kwargs
        self.assertTrue(final_log_call_args.get('overall_success'))
        self.assertEqual(final_log_call_args.get('modification_details', {}).get('source_of_code'), "CodeService_LLM")
        self.assertTrue(final_log_call_args.get('post_modification_test_passed'))

    @patch('ai_assistant.execution.action_executor.global_reflection_log.log_execution')
    def test_tool_mod_via_codeservice_fails_to_generate(self, mock_log_execution):
        self.executor.code_service = mock.AsyncMock(spec=CodeService)
        self.executor.code_service.modify_code.return_value = {
            "status": "ERROR_LLM_NO_SUGGESTION",
            "modified_code_string": None,
            "error": "LLM said no."
        }
        action_details = {
            "module_path": "test_module.py", "function_name": "old_func", "tool_name": "test_tool",
            "suggested_change_description": "Needs a fix via CodeService, but CS will fail.",
            "original_reflection_entry_id": "dummy_ref_id_cs_fail"
        }
        proposed_action = {"source_insight_id": "insight_cs_llm_fail", "action_type": "PROPOSE_TOOL_MODIFICATION", "details": action_details}

        result = asyncio.run(self.executor.execute_action(proposed_action))

        self.assertFalse(result)
        self.executor.code_service.modify_code.assert_called_once()

        code_service_fail_log_call = next(call for call in mock_log_execution.call_args_list if call.kwargs.get('status_override') == "CODE_SERVICE_GEN_FAILED")
        self.assertIsNotNone(code_service_fail_log_call)
        self.assertFalse(code_service_fail_log_call.kwargs.get('overall_success'))


    @patch('ai_assistant.execution.action_executor.global_reflection_log.log_execution')
    @patch('ai_assistant.execution.action_executor.self_modification.get_backup_function_source_code')
    @patch('ai_assistant.execution.action_executor.self_modification.edit_function_source_code')
    @patch.object(ActionExecutor, '_run_post_modification_test', new_callable=AsyncMock)
    def test_tool_mod_test_fails_and_reversion_succeeds(self, mock_run_post_mod_test, mock_edit_code,
                                                       mock_get_backup, mock_log_execution):
        mock_edit_code.side_effect = [
            "Successfully modified function 'test_func'.",
            "Successfully reverted function 'test_func'."
        ]
        mock_run_post_mod_test.return_value = (False, "Post-mod test failed critically.")
        mock_get_backup.return_value = "def test_func(): pass # Original backup code"
        action_details = {
            "module_path": "test_module.py", "function_name": "test_func", "tool_name": "test_tool",
            "suggested_code_change": "def test_func(): pass # new potentially buggy code",
            "original_reflection_entry_id": "dummy_orig_ref_id_for_revert_test"
        }
        proposed_action = {"source_insight_id": "insight_revert", "action_type": "PROPOSE_TOOL_MODIFICATION", "details": action_details}
        result = asyncio.run(self.executor.execute_action(proposed_action))
        self.assertFalse(result)
        self.assertEqual(mock_edit_code.call_count, 2)
        final_log_call_args = mock_log_execution.call_args_list[-1].kwargs
        self.assertTrue(final_log_call_args.get('modification_details', {}).get('reversion_attempted'))
        self.assertTrue(final_log_call_args.get('modification_details', {}).get('reversion_successful'))

    def test_find_original_reflection_entry(self):
        mock_entry_id_to_find = str(uuid.uuid4())
        mock_original_entry = MockReflectionLogEntryForTest(entry_id=mock_entry_id_to_find)
        core_global_reflection_log.log_entries = [mock_original_entry]
        found_entry = self.executor._find_original_reflection_entry(mock_entry_id_to_find)
        self.assertIsNotNone(found_entry)
        if found_entry: self.assertEqual(found_entry.entry_id, mock_entry_id_to_find)
        self.assertIsNone(self.executor._find_original_reflection_entry("non_existent_id"))

    @patch('ai_assistant.execution.action_executor.self_modification.edit_function_source_code')
    @patch('ai_assistant.execution.action_executor.global_reflection_log.log_execution')
    @patch.object(ActionExecutor, '_run_post_modification_test', new_callable=AsyncMock)
    def test_execute_action_propose_tool_modification_edit_fails(self, mock_run_post_mod_test, mock_log_execution, mock_edit_code):
        mock_edit_code.return_value = "Error: Failed to modify function."
        action_details = {
            "module_path": "test_module.py", "function_name": "test_func", "tool_name": "test_tool",
            "suggested_code_change": "def test_func(): pass # new code",
            "original_reflection_entry_id": "entry_id_for_edit_fail"
        }
        proposed_action = {"source_insight_id": "insight_edit_fail", "action_type": "PROPOSE_TOOL_MODIFICATION", "details": action_details}
        result = asyncio.run(self.executor.execute_action(proposed_action))
        self.assertFalse(result)
        mock_run_post_mod_test.assert_not_called()
        final_log_call_args = mock_log_execution.call_args_list[-1].kwargs
        self.assertIsNone(final_log_call_args.get('modification_details', {}).get('reversion_attempted'))

if __name__ == '__main__': # pragma: no cover
    unittest.main()

# ### END FILE: tests/test_action_executor.py ###

# ### START FILE: tests/test_autonomous_reflection.py ###
import unittest
from unittest.mock import patch, MagicMock, call
import json

# Assuming the module structure allows this import path
from ai_assistant.core.autonomous_reflection import (
    _invoke_suggestion_scoring_llm,
    run_self_reflection_cycle,
    select_suggestion_for_autonomous_action,
    get_reflection_log_summary_for_analysis, # Added for potential use in run_self_reflection_cycle tests
    _invoke_pattern_identification_llm # Added for potential use in run_self_reflection_cycle tests
)
import datetime # Added for ReflectionLogEntry timestamp

# Assuming the module structure allows this import path
from ai_assistant.core.reflection import ReflectionLogEntry, global_reflection_log # Added
from ai_assistant.llm_interface.ollama_client import invoke_ollama_model # Added for mocking

# If DEFAULT_OLLAMA_MODEL is a global constant in autonomous_reflection.py that needs to be defined for tests:
# from ai_assistant.core.autonomous_reflection import DEFAULT_OLLAMA_MODEL # Or define a mock one here

# Mock constants if they are not easily importable or for test stability
DEFAULT_OLLAMA_MODEL_FOR_TEST = "test_model"

# Merging new tests into a more general class or creating a new one.
# For simplicity, let's rename existing classes if their scope expands significantly
# or add a new class. Given the tasks, let's add to a new class for clarity.

class TestAutonomousReflectionEnhancements(unittest.TestCase):

    def setUp(self):
        # Common setup for new tests, if any
        self.sample_suggestion_for_modify = {
            "suggestion_id": "SUG_MODIFY_001",
            "suggestion_text": "Modify 'sample_tool_function' to improve efficiency.",
            "action_type": "MODIFY_TOOL_CODE",
            "action_details": {
                "module_path": "ai_assistant.dummy_modules.dummy_tool_module",
                "function_name": "sample_tool_function",
                "suggested_code_change": "def sample_tool_function(param1):\n    return param1 * 2",
                "original_code_snippet": "def sample_tool_function(param1):\n    return param1",
                "suggested_change_description": "Refactored for 2x performance."
            },
            "impact_score": 4, "risk_score": 1, "effort_score": 2, # Priority: 4-1-1 = 2
            "review_looks_good": True,
            "qualitative_review": "Looks good to go.",
            "reviewer_confidence": 0.9
        }

    @patch('ai_assistant.core.reflection.global_reflection_log.log_execution')
    @patch('ai_assistant.learning.evolution.apply_code_modification')
    def test_select_suggestion_logs_self_modification_details_on_success(self, mock_apply_code, mock_log_exec):
        mock_apply_return = {
            "overall_status": True, "overall_message": "All good, code modified and committed.",
            "edit_outcome": {"status": True, "message": "Edited successfully.", "backup_path": "path/to/dummy_tool_module.py.bak"},
            "test_outcome": {"passed": True, "stdout": "All tests passed.", "stderr": "", "notes": "Tests ran successfully."},
            "revert_outcome": None,
            "commit_outcome": {"status": True, "commit_message_generated": "AI Autocommit: Modified sample_tool_function...", "error_message": None}
        }
        mock_apply_code.return_value = mock_apply_return

        select_suggestion_for_autonomous_action([self.sample_suggestion_for_modify])
        
        mock_log_exec.assert_called_once()
        call_kwargs = mock_log_exec.call_args[1]

        self.assertTrue(call_kwargs['is_self_modification_attempt'])
        self.assertEqual(call_kwargs['source_suggestion_id'], self.sample_suggestion_for_modify['suggestion_id'])
        self.assertEqual(call_kwargs['modification_type'], "MODIFY_TOOL_CODE")
        self.assertEqual(call_kwargs['post_modification_test_passed'], True)
        self.assertEqual(call_kwargs['post_modification_test_details'], mock_apply_return['test_outcome'])
        self.assertEqual(call_kwargs['commit_info'], mock_apply_return['commit_outcome'])
        self.assertTrue(call_kwargs['overall_success']) # Based on mock_apply_return['overall_status']
        self.assertIn("Self-modification attempt for suggestion SUG_MODIFY_001", call_kwargs['goal_description'])
        self.assertEqual(call_kwargs['notes'], mock_apply_return['overall_message'])
        self.assertEqual(call_kwargs['modification_details']['module'], self.sample_suggestion_for_modify['action_details']['module_path'])

    @patch('ai_assistant.core.reflection.global_reflection_log.log_execution')
    @patch('ai_assistant.learning.evolution.apply_code_modification')
    def test_select_suggestion_logs_self_modification_details_on_test_failure(self, mock_apply_code, mock_log_exec):
        mock_apply_return_test_fail = {
            "overall_status": False, "overall_message": "Tests failed, reverted.",
            "edit_outcome": {"status": True, "message": "Edited successfully.", "backup_path": "path/to/dummy_tool_module.py.bak"},
            "test_outcome": {"passed": False, "stdout": "", "stderr": "AssertionError: 1 != 2", "notes": "Test failed."},
            "revert_outcome": {"status": True, "message": "Reverted successfully."},
            "commit_outcome": None 
        }
        mock_apply_code.return_value = mock_apply_return_test_fail

        select_suggestion_for_autonomous_action([self.sample_suggestion_for_modify])
        
        mock_log_exec.assert_called_once()
        call_kwargs = mock_log_exec.call_args[1]

        self.assertTrue(call_kwargs['is_self_modification_attempt'])
        self.assertEqual(call_kwargs['source_suggestion_id'], self.sample_suggestion_for_modify['suggestion_id'])
        self.assertEqual(call_kwargs['post_modification_test_passed'], False)
        self.assertEqual(call_kwargs['post_modification_test_details'], mock_apply_return_test_fail['test_outcome'])
        self.assertIsNone(call_kwargs['commit_info']) # No commit if tests fail
        self.assertFalse(call_kwargs['overall_success'])
        self.assertEqual(call_kwargs['notes'], mock_apply_return_test_fail['overall_message'])

    @patch('ai_assistant.core.reflection.global_reflection_log.get_entries')
    def test_get_summary_includes_self_modification(self, mock_get_entries):
        now = datetime.datetime.now(datetime.timezone.utc)
        mock_self_mod_entry = ReflectionLogEntry(
            goal_description="Self-mod attempt: SUG00X",
            plan=[], execution_results=[], status="SUCCESS", timestamp=now,
            is_self_modification_attempt=True,
            source_suggestion_id="SUG00X",
            modification_type="MODIFY_TOOL_CODE",
            modification_details={"module": "a.b.c", "function": "test_func"},
            post_modification_test_passed=True,
            post_modification_test_details={"passed": True, "notes": "All good in the hood.", "stdout": "OK"},
            commit_info={"status": True, "message": "Committed: AI fix for SUG00X"}
        )
        mock_normal_entry = ReflectionLogEntry(
            goal_description="Normal goal", plan=[], execution_results=[], status="FAILURE", 
            error_type="TypeError", error_message="Something bad", timestamp=now
        )
        mock_get_entries.return_value = [mock_normal_entry, mock_self_mod_entry]

        summary = get_reflection_log_summary_for_analysis(max_entries=2, min_entries_for_analysis=1)

        self.assertIn("--- SELF-MODIFICATION ATTEMPT ---", summary)
        self.assertIn("Source Suggestion ID: SUG00X", summary)
        self.assertIn("Test Outcome: PASSED", summary) # Based on ReflectionLogEntry formatting
        self.assertIn("Test Notes: All good in the hood.", summary)
        self.assertIn("Commit Status: Committed (Msg: Committed: AI fix for SUG00X)", summary)
        # Ensure normal entry details are also present
        self.assertIn("Goal: Normal goal", summary)
        self.assertIn("Error: TypeError - Something bad", summary)

    @patch('ai_assistant.core.autonomous_reflection.invoke_ollama_model')
    def test_suggestion_generation_prompt_for_modify_tool_code(self, mock_invoke_ollama):
        # This test focuses on the prompt content for _invoke_suggestion_generation_llm
        mock_invoke_ollama.return_value = '{"improvement_suggestions": []}' # Minimal valid response
        
        sample_patterns = [{"pattern_type": "FREQUENTLY_FAILING_TOOL", "tool_name": "test_tool"}]
        sample_tools = {"test_tool": "A tool that often fails."}
        
        # We call the internal _invoke_suggestion_generation_llm directly for this test
        # In a real scenario, run_self_reflection_cycle would call this after other steps.
        from ai_assistant.core.autonomous_reflection import _invoke_suggestion_generation_llm
        _invoke_suggestion_generation_llm(
            identified_patterns_json_list_str=json.dumps(sample_patterns),
            available_tools_json_str=json.dumps(sample_tools),
            llm_model_name=DEFAULT_OLLAMA_MODEL_FOR_TEST
        )

        mock_invoke_ollama.assert_called_once()
        prompt_arg = mock_invoke_ollama.call_args[0][0]

        # Check for key phrases from the updated MODIFY_TOOL_CODE example in the prompt
        self.assertIn('"module_path": "path.to.your.module"', prompt_arg)
        self.assertIn('"function_name": "function_to_modify"', prompt_arg)
        self.assertIn('"suggested_code_change": "def function_to_modify(param1, param2):', prompt_arg)
        self.assertIn("New, complete function code here", prompt_arg)
        self.assertIn('"original_code_snippet": "(Optional) Few lines of the original code for context', prompt_arg)
        self.assertIn('"suggested_change_description": "Detailed textual description of what was changed and why, suitable for a commit message body."', prompt_arg)
        self.assertIn("For MODIFY_TOOL_CODE, 'module_path', 'function_name', and 'suggested_code_change' (the new complete function source code) are mandatory.", prompt_arg)


class TestInvokeSuggestionScoringLLM(unittest.TestCase):

    @patch('ai_assistant.core.autonomous_reflection.invoke_ollama_model')
    def test_successful_scoring(self, mock_invoke_ollama):
        mock_response = '{ "impact_score": 4, "risk_score": 2, "effort_score": 3 }'
        mock_invoke_ollama.return_value = mock_response
        
        sample_suggestion = {
            "suggestion_id": "SUG_001",
            "suggestion_text": "Test suggestion",
            "action_type": "MODIFY_TOOL_CODE",
            "action_details": {"tool_name": "test_tool"}
        }
        
        expected_scores = {"impact_score": 4, "risk_score": 2, "effort_score": 3}
        
        result = _invoke_suggestion_scoring_llm(sample_suggestion, llm_model_name=DEFAULT_OLLAMA_MODEL_FOR_TEST)
        
        self.assertEqual(result, expected_scores)
        mock_invoke_ollama.assert_called_once()
        # You could add more assertions here to check the prompt contents if needed, by inspecting mock_invoke_ollama.call_args

    @patch('ai_assistant.core.autonomous_reflection.invoke_ollama_model')
    @patch('builtins.print')
    def test_llm_returns_invalid_json(self, mock_print, mock_invoke_ollama):
        mock_invoke_ollama.return_value = "This is not JSON"
        
        sample_suggestion = {"suggestion_text": "Test", "action_type": "ANY"}
        result = _invoke_suggestion_scoring_llm(sample_suggestion, llm_model_name=DEFAULT_OLLAMA_MODEL_FOR_TEST)
        
        self.assertIsNone(result)
        mock_print.assert_any_call("Error decoding JSON from suggestion scoring LLM: Expecting value: line 1 column 1 (char 0). Response: This is not JSON")

    @patch('ai_assistant.core.autonomous_reflection.invoke_ollama_model')
    @patch('builtins.print')
    def test_llm_returns_json_with_missing_keys(self, mock_print, mock_invoke_ollama):
        mock_invoke_ollama.return_value = '{ "impact_score": 4, "risk_score": 2 }' # Missing "effort_score"
        
        sample_suggestion = {"suggestion_text": "Test", "action_type": "ANY"}
        result = _invoke_suggestion_scoring_llm(sample_suggestion, llm_model_name=DEFAULT_OLLAMA_MODEL_FOR_TEST)
        
        self.assertIsNone(result)
        mock_print.assert_any_call("Warning: LLM response for suggestion scoring missing key 'effort_score'. Response: { \"impact_score\": 4, \"risk_score\": 2 }")

    @patch('ai_assistant.core.autonomous_reflection.invoke_ollama_model')
    @patch('builtins.print')
    def test_llm_returns_json_with_non_integer_scores(self, mock_print, mock_invoke_ollama):
        mock_invoke_ollama.return_value = '{ "impact_score": "high", "risk_score": 2, "effort_score": 3 }'
        
        sample_suggestion = {"suggestion_text": "Test", "action_type": "ANY"}
        result = _invoke_suggestion_scoring_llm(sample_suggestion, llm_model_name=DEFAULT_OLLAMA_MODEL_FOR_TEST)
        
        self.assertIsNone(result)
        mock_print.assert_any_call("Warning: LLM response for suggestion scoring key 'impact_score' is not an integer. Value: high. Response: { \"impact_score\": \"high\", \"risk_score\": 2, \"effort_score\": 3 }")

    @patch('ai_assistant.core.autonomous_reflection.invoke_ollama_model')
    def test_handling_action_details_present_and_absent(self, mock_invoke_ollama):
        # Test with action_details
        mock_invoke_ollama.return_value = '{ "impact_score": 1, "risk_score": 1, "effort_score": 1 }'
        suggestion_with_details = {
            "suggestion_text": "Test with details", 
            "action_type": "MODIFY_TOOL_CODE",
            "action_details": {"tool_name": "some_tool", "change": "critical"}
        }
        result_with_details = _invoke_suggestion_scoring_llm(suggestion_with_details, llm_model_name=DEFAULT_OLLAMA_MODEL_FOR_TEST)
        self.assertIsNotNone(result_with_details)
        
        # Check if prompt formatting for action_details was as expected (stringified JSON)
        args_with_details, _ = mock_invoke_ollama.call_args
        prompt_with_details = args_with_details[0]
        self.assertIn('"action_details": {"tool_name": "some_tool", "change": "critical"}', prompt_with_details.replace("\\", "")) # Handle potential escapes

        mock_invoke_ollama.reset_mock() # Reset for the next call

        # Test without action_details (should default to "{}")
        mock_invoke_ollama.return_value = '{ "impact_score": 2, "risk_score": 2, "effort_score": 2 }'
        suggestion_without_details = {
            "suggestion_text": "Test without details", 
            "action_type": "MANUAL_REVIEW_NEEDED"
            # "action_details": None is implied
        }
        result_without_details = _invoke_suggestion_scoring_llm(suggestion_without_details, llm_model_name=DEFAULT_OLLAMA_MODEL_FOR_TEST)
        self.assertIsNotNone(result_without_details)
        
        args_without_details, _ = mock_invoke_ollama.call_args
        prompt_without_details = args_without_details[0]
        self.assertIn('"action_details_json_str": "{}"', prompt_without_details.replace(" ", "").replace("\\n", "")) # Check for empty JSON object in prompt


class TestRunSelfReflectionCycleScoring(unittest.TestCase):

    @patch('ai_assistant.core.autonomous_reflection.get_reflection_log_summary_for_analysis')
    @patch('ai_assistant.core.autonomous_reflection._invoke_pattern_identification_llm')
    @patch('ai_assistant.core.autonomous_reflection._invoke_suggestion_generation_llm')
    @patch('ai_assistant.core.autonomous_reflection._invoke_suggestion_scoring_llm')
    def test_successful_scoring_for_all_suggestions(
        self, 
        mock_score_suggestion, 
        mock_generate_suggestions, 
        mock_identify_patterns, 
        mock_get_log_summary
    ):
        # Setup: Mock previous steps in the cycle to return valid data
        mock_get_log_summary.return_value = "Some log summary"
        mock_identify_patterns.return_value = {"identified_patterns": [{"pattern_type": "Test Pattern"}]}
        
        sample_suggestions_generated = [
            {"suggestion_id": "SUG_001", "suggestion_text": "Suggestion 1", "action_type": "TYPE_A"},
            {"suggestion_id": "SUG_002", "suggestion_text": "Suggestion 2", "action_type": "TYPE_B"},
        ]
        mock_generate_suggestions.return_value = {"improvement_suggestions": sample_suggestions_generated}
        
        # Mock scoring to return different valid scores
        mock_score_suggestion.side_effect = [
            {"impact_score": 5, "risk_score": 1, "effort_score": 2},
            {"impact_score": 4, "risk_score": 2, "effort_score": 3},
        ]
        
        result = run_self_reflection_cycle(available_tools={"tool1": "desc"}, llm_model_name=DEFAULT_OLLAMA_MODEL_FOR_TEST)
        
        self.assertIsNotNone(result)
        self.assertEqual(len(result), 2)
        
        self.assertEqual(result[0]["suggestion_id"], "SUG_001")
        self.assertEqual(result[0]["impact_score"], 5)
        self.assertEqual(result[0]["risk_score"], 1)
        self.assertEqual(result[0]["effort_score"], 2)
        
        self.assertEqual(result[1]["suggestion_id"], "SUG_002")
        self.assertEqual(result[1]["impact_score"], 4)
        self.assertEqual(result[1]["risk_score"], 2)
        self.assertEqual(result[1]["effort_score"], 3)
        
        self.assertEqual(mock_score_suggestion.call_count, 2)
        mock_score_suggestion.assert_any_call(sample_suggestions_generated[0], llm_model_name=DEFAULT_OLLAMA_MODEL_FOR_TEST)
        mock_score_suggestion.assert_any_call(sample_suggestions_generated[1], llm_model_name=DEFAULT_OLLAMA_MODEL_FOR_TEST)

    @patch('ai_assistant.core.autonomous_reflection.get_reflection_log_summary_for_analysis')
    @patch('ai_assistant.core.autonomous_reflection._invoke_pattern_identification_llm')
    @patch('ai_assistant.core.autonomous_reflection._invoke_suggestion_generation_llm')
    @patch('ai_assistant.core.autonomous_reflection._invoke_suggestion_scoring_llm')
    @patch('builtins.print') # To suppress or check print warnings
    def test_scoring_fails_for_one_suggestion(
        self, 
        mock_print,
        mock_score_suggestion, 
        mock_generate_suggestions, 
        mock_identify_patterns, 
        mock_get_log_summary
    ):
        mock_get_log_summary.return_value = "Some log summary"
        mock_identify_patterns.return_value = {"identified_patterns": [{"pattern_type": "Test Pattern"}]}
        
        sample_suggestions_generated = [
            {"suggestion_id": "SUG_001", "suggestion_text": "Suggestion 1", "action_type": "TYPE_A"},
            {"suggestion_id": "SUG_002", "suggestion_text": "Suggestion 2", "action_type": "TYPE_B"},
        ]
        mock_generate_suggestions.return_value = {"improvement_suggestions": sample_suggestions_generated}
        
        # Mock scoring: success for first, None (failure) for second
        mock_score_suggestion.side_effect = [
            {"impact_score": 5, "risk_score": 1, "effort_score": 2},
            None, 
        ]
        
        result = run_self_reflection_cycle(available_tools={"tool1": "desc"}, llm_model_name=DEFAULT_OLLAMA_MODEL_FOR_TEST)
        
        self.assertIsNotNone(result)
        self.assertEqual(len(result), 2)
        
        self.assertEqual(result[0]["suggestion_id"], "SUG_001")
        self.assertEqual(result[0]["impact_score"], 5)
        self.assertEqual(result[0]["risk_score"], 1)
        self.assertEqual(result[0]["effort_score"], 2)
        
        self.assertEqual(result[1]["suggestion_id"], "SUG_002")
        self.assertEqual(result[1]["impact_score"], -1) # Default error score
        self.assertEqual(result[1]["risk_score"], -1)  # Default error score
        self.assertEqual(result[1]["effort_score"], -1) # Default error score
        
        mock_print.assert_any_call("Warning: Failed to score suggestion ID: SUG_002. Assigning default error scores (-1).")
        self.assertEqual(mock_score_suggestion.call_count, 2)

    @patch('ai_assistant.core.autonomous_reflection.get_reflection_log_summary_for_analysis')
    @patch('ai_assistant.core.autonomous_reflection._invoke_pattern_identification_llm')
    @patch('ai_assistant.core.autonomous_reflection._invoke_suggestion_generation_llm')
    def test_no_suggestions_generated(
        self, 
        mock_generate_suggestions, 
        mock_identify_patterns, 
        mock_get_log_summary
    ):
        mock_get_log_summary.return_value = "Some log summary"
        mock_identify_patterns.return_value = {"identified_patterns": [{"pattern_type": "Test Pattern"}]}
        mock_generate_suggestions.return_value = {"improvement_suggestions": []} # No suggestions
        
        result = run_self_reflection_cycle(available_tools={"tool1": "desc"}, llm_model_name=DEFAULT_OLLAMA_MODEL_FOR_TEST)
        self.assertEqual(result, []) # Should return an empty list

    @patch('ai_assistant.core.autonomous_reflection.get_reflection_log_summary_for_analysis')
    @patch('ai_assistant.core.autonomous_reflection._invoke_pattern_identification_llm')
    @patch('ai_assistant.core.autonomous_reflection._invoke_suggestion_generation_llm')
    def test_suggestion_generation_returns_none(
        self, 
        mock_generate_suggestions, 
        mock_identify_patterns, 
        mock_get_log_summary
    ):
        mock_get_log_summary.return_value = "Some log summary"
        mock_identify_patterns.return_value = {"identified_patterns": [{"pattern_type": "Test Pattern"}]}
        mock_generate_suggestions.return_value = None # LLM call failed for suggestions
        
        result = run_self_reflection_cycle(available_tools={"tool1": "desc"}, llm_model_name=DEFAULT_OLLAMA_MODEL_FOR_TEST)
        self.assertIsNone(result)


class TestSelectSuggestionForAutonomousAction(unittest.TestCase):

    def _create_sample_suggestion(self, id, action_type, action_details, impact, risk, effort):
        return {
            "suggestion_id": id,
            "suggestion_text": f"Text for {id}",
            "action_type": action_type,
            "action_details": action_details,
            "impact_score": impact,
            "risk_score": risk,
            "effort_score": effort,
            # "_priority_score" will be calculated by the function if scores are valid
        }

    def test_basic_selection_with_scoring(self):
        suggestions = [
            self._create_sample_suggestion("S1", "UPDATE_TOOL_DESCRIPTION", {"tool_name": "t1", "new_description": "d1"}, impact=3, risk=1, effort=1), # Priority: 3-1-0.5 = 1.5
            self._create_sample_suggestion("S2", "CREATE_NEW_TOOL", {"tool_description_prompt": "p2"}, impact=5, risk=1, effort=2), # Priority: 5-1-1 = 3
            self._create_sample_suggestion("S3", "UPDATE_TOOL_DESCRIPTION", {"tool_name": "t3", "new_description": "d3"}, impact=4, risk=2, effort=2), # Priority: 4-2-1 = 1
        ]
        selected = select_suggestion_for_autonomous_action(suggestions)
        self.assertIsNotNone(selected)
        self.assertEqual(selected["suggestion_id"], "S2")

    def test_higher_scored_suggestion_is_invalid_action_details(self):
        suggestions = [
            self._create_sample_suggestion("S1_invalid_details", "CREATE_NEW_TOOL", {"tool_description_prompt": ""}, impact=5, risk=1, effort=1), # High priority (3.5), but invalid (empty prompt)
            self._create_sample_suggestion("S2_valid", "UPDATE_TOOL_DESCRIPTION", {"tool_name": "t2", "new_description": "d2"}, impact=3, risk=1, effort=1), # Lower priority (1.5) but valid
        ]
        selected = select_suggestion_for_autonomous_action(suggestions)
        self.assertIsNotNone(selected)
        self.assertEqual(selected["suggestion_id"], "S2_valid")

    def test_filtering_by_action_type(self):
        suggestions = [
            self._create_sample_suggestion("S1_unsupported_type", "MODIFY_TOOL_CODE", {"tool_name": "t1", "suggested_change_description": "c1"}, impact=5, risk=1, effort=1), # High priority (3.5), but unsupported type
            self._create_sample_suggestion("S2_supported_type", "CREATE_NEW_TOOL", {"tool_description_prompt": "p2"}, impact=3, risk=1, effort=1), # Lower priority (1.5) but supported type
        ]
        # Default supported: ["UPDATE_TOOL_DESCRIPTION", "CREATE_NEW_TOOL"]
        selected = select_suggestion_for_autonomous_action(suggestions)
        self.assertIsNotNone(selected)
        self.assertEqual(selected["suggestion_id"], "S2_supported_type")
        
        # Test with explicit supported types
        selected_custom = select_suggestion_for_autonomous_action(suggestions, supported_action_types=["MODIFY_TOOL_CODE"])
        self.assertIsNotNone(selected_custom)
        self.assertEqual(selected_custom["suggestion_id"], "S1_unsupported_type")


    def test_filtering_out_failed_scores(self):
        suggestions = [
            self._create_sample_suggestion("S1_failed_score", "CREATE_NEW_TOOL", {"tool_description_prompt": "p1"}, impact=5, risk=-1, effort=1), # High "raw" impact, but risk is -1
            self._create_sample_suggestion("S2_valid_scores", "UPDATE_TOOL_DESCRIPTION", {"tool_name": "t2", "new_description": "d2"}, impact=3, risk=1, effort=1), # Valid scores, priority 1.5
        ]
        selected = select_suggestion_for_autonomous_action(suggestions)
        self.assertIsNotNone(selected)
        self.assertEqual(selected["suggestion_id"], "S2_valid_scores")

    def test_empty_suggestion_list(self):
        selected = select_suggestion_for_autonomous_action([])
        self.assertIsNone(selected)

    def test_no_suitable_suggestion_found_all_invalid(self):
        suggestions = [
            self._create_sample_suggestion("S1_unsupported", "MODIFY_TOOL_CODE", {"tool_name": "t1", "suggested_change_description": "c1"}, impact=5, risk=1, effort=1),
            self._create_sample_suggestion("S2_failed_score", "CREATE_NEW_TOOL", {"tool_description_prompt": "p2"}, impact=5, risk=1, effort=-1),
            self._create_sample_suggestion("S3_invalid_details", "UPDATE_TOOL_DESCRIPTION", {"tool_name": "t3"}, impact=4, risk=1, effort=1), # Missing new_description
        ]
        selected = select_suggestion_for_autonomous_action(suggestions)
        self.assertIsNone(selected)
        
    def test_priority_calculation_and_sorting(self):
        # Effort has 0.5 multiplier, lower is better for risk and effort
        # Priority = Impact - Risk - (Effort * 0.5)
        suggestions = [
            self._create_sample_suggestion("S_LowImpact_LowRisk_LowEffort", "CREATE_NEW_TOOL", {"tool_description_prompt": "p1"}, impact=2, risk=1, effort=1), # P = 2 - 1 - 0.5 = 0.5
            self._create_sample_suggestion("S_HighImpact_HighRisk_HighEffort", "CREATE_NEW_TOOL", {"tool_description_prompt": "p2"}, impact=5, risk=3, effort=4),# P = 5 - 3 - 2 = 0
            self._create_sample_suggestion("S_MidImpact_LowRisk_MidEffort", "CREATE_NEW_TOOL", {"tool_description_prompt": "p3"}, impact=4, risk=1, effort=2), # P = 4 - 1 - 1 = 2
            self._create_sample_suggestion("S_HighImpact_MidRisk_LowEffort", "CREATE_NEW_TOOL", {"tool_description_prompt": "p4"}, impact=5, risk=2, effort=1), # P = 5 - 2 - 0.5 = 2.5 (Highest)
        ]
        # Expected order: S_HighImpact_MidRisk_LowEffort (2.5), S_MidImpact_LowRisk_MidEffort (2), S_LowImpact_LowRisk_LowEffort (0.5), S_HighImpact_HighRisk_HighEffort (0)
        selected = select_suggestion_for_autonomous_action(suggestions)
        self.assertIsNotNone(selected)
        self.assertEqual(selected["suggestion_id"], "S_HighImpact_MidRisk_LowEffort")

    def test_selection_amongst_equally_prioritized_valid_suggestions(self):
        # If multiple suggestions have the same highest priority score and are valid,
        # the current implementation will pick the one that appears first in the *sorted* list.
        # The sort is stable, so if they had same priority, their original relative order (after filtering) would be maintained.
        # This test ensures one is picked.
        suggestions = [
             self._create_sample_suggestion("S1_equal_priority", "CREATE_NEW_TOOL", {"tool_description_prompt": "prompt1"}, impact=4, risk=1, effort=2), # P = 4 - 1 - 1 = 2
             self._create_sample_suggestion("S2_equal_priority", "UPDATE_TOOL_DESCRIPTION", {"tool_name":"t1", "new_description": "desc1"}, impact=4, risk=1, effort=2), # P = 4 - 1 - 1 = 2
        ]
        selected = select_suggestion_for_autonomous_action(suggestions)
        self.assertIsNotNone(selected)
        # The exact one depends on Python's list sort stability if scores are identical.
        # Both are valid, so one of them should be chosen.
        self.assertIn(selected["suggestion_id"], ["S1_equal_priority", "S2_equal_priority"])


if __name__ == '__main__':
    unittest.main()

# ### END FILE: tests/test_autonomous_reflection.py ###

# ### START FILE: tests/test_cli.py ###
import unittest
from unittest import mock
import asyncio
import os # For path manipulation if needed in tests
import sys
import json # For json.dumps in test data if needed, and for CodeService metadata

# Add project root to sys.path to allow importing ai_assistant modules
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if project_root not in sys.path: # pragma: no cover
    sys.path.insert(0, project_root)

from ai_assistant.communication import cli # Module to test
# from ai_assistant.code_services.service import CodeService # Will be mocked via cli.CodeService
# from ai_assistant.core.fs_utils import write_to_file # Will be mocked via cli.write_to_file
# from ai_assistant.tools import tool_system # For _perform_tool_registration, will be mocked via cli._perform_tool_registration
# from ai_assistant.core.reflection import global_reflection_log # Will be mocked via cli.global_reflection_log


class TestCliToolGenerationFlow(unittest.TestCase):

    @mock.patch('ai_assistant.communication.cli.global_reflection_log.log_execution') # Mock logging
    @mock.patch('ai_assistant.communication.cli._perform_tool_registration') # Mock tool registration
    @mock.patch('ai_assistant.communication.cli.write_to_file') # Mock file saving (fs_utils)
    @mock.patch('builtins.input') # Mock user input
    @mock.patch('ai_assistant.communication.cli.CodeService') # Mock CodeService class
    @mock.patch('ai_assistant.communication.cli.tool_system_instance.execute_tool', new_callable=mock.AsyncMock) # Mock code review tool
    async def test_handle_code_generation_triggers_scaffold_success(
        self, mock_execute_review_tool, MockCodeService, mock_input,
        mock_cli_write_to_file, mock_perform_registration, mock_log_execution
    ):
        # --- Setup Mocks ---

        # 1. CodeService().generate_code (called twice)
        mock_cs_instance = MockCodeService.return_value

        new_tool_metadata = {"suggested_function_name": "my_new_tool_func", "suggested_tool_name": "myNewTool", "suggested_description": "A new tool."}
        new_tool_code = "def my_new_tool_func():\n    pass"

        test_scaffold_code = "import unittest\nclass TestMyNewTool(unittest.TestCase): pass"
        expected_test_target_path = os.path.join("tests", "custom_tools", "test_my_new_tool_func.py")

        # Configure side_effect for sequential calls to generate_code
        generate_code_results = [
            {
                "status": "SUCCESS_CODE_GENERATED", "code_string": new_tool_code,
                "metadata": new_tool_metadata, "logs": [], "error": None
            },
            {
                "status": "SUCCESS_CODE_GENERATED", "code_string": test_scaffold_code,
                "metadata": None, "logs": [], "error": None, "saved_to_path": expected_test_target_path
            }
        ]

        # Use a callable for side_effect to inspect arguments if necessary, or just return futures
        async def generate_code_side_effect(*args, **kwargs):
            if kwargs.get('context') == "NEW_TOOL":
                return generate_code_results[0]
            elif kwargs.get('context') == "GENERATE_UNIT_TEST_SCAFFOLD":
                return generate_code_results[1]
            # Fallback for any other calls, though not expected in this test
            return {"status": "ERROR_UNEXPECTED_CONTEXT_IN_MOCK"} # pragma: no cover

        mock_cs_instance.generate_code = mock.AsyncMock(side_effect=generate_code_side_effect)


        # 2. User input (mock_input)
        mock_input.side_effect = ['y'] # Confirm use of suggested metadata

        # 3. fs_utils.write_to_file (mock_cli_write_to_file) - for saving the *tool* code
        mock_cli_write_to_file.return_value = True # Tool saving success

        # 4. _perform_tool_registration (mock_perform_registration)
        mock_perform_registration.return_value = (True, "Tool registered.") # Registration success

        # 5. Mock for code review tool (if called) - assuming it's short enough to skip review
        # If code review is triggered (len(cleaned_code.splitlines()) > 3), this needs to be set.
        # new_tool_code is short, so review is skipped. If testing review, make new_tool_code longer.
        mock_execute_review_tool.return_value = {"status": "approved", "comments": "Looks good."}


        # --- Call the function under test ---
        test_description = "a brand new awesome tool"
        await cli._handle_code_generation_and_registration(test_description)

        # --- Assertions ---

        # Check calls to CodeService().generate_code
        self.assertEqual(mock_cs_instance.generate_code.call_count, 2)

        # Call 1 (NEW_TOOL)
        mock_cs_instance.generate_code.assert_any_call(
            context="NEW_TOOL",
            prompt_or_description=test_description,
            target_path=None
        )

        # Call 2 (GENERATE_UNIT_TEST_SCAFFOLD)
        expected_module_path_for_hint = "ai_assistant.custom_tools.my_new_tool_func"
        mock_cs_instance.generate_code.assert_any_call(
            context="GENERATE_UNIT_TEST_SCAFFOLD",
            prompt_or_description=new_tool_code,
            additional_context={"module_name_hint": expected_module_path_for_hint},
            target_path=expected_test_target_path
        )

        # User input for metadata confirmation was called
        mock_input.assert_called_with(mock.ANY)

        # write_to_file called for the new tool's code by CLI
        expected_tool_filepath = os.path.join("ai_assistant", "custom_tools", "my_new_tool_func.py")
        mock_cli_write_to_file.assert_called_once_with(expected_tool_filepath, new_tool_code)

        # _perform_tool_registration called
        mock_perform_registration.assert_called_once_with(
            expected_module_path_for_hint,
            new_tool_metadata["suggested_function_name"],
            new_tool_metadata["suggested_tool_name"],
            new_tool_metadata["suggested_description"]
        )

        # Verify reflection logging (optional, but good for completeness)
        # Check for scaffold success log
        scaffold_log_found = False
        for call_args in mock_log_execution.call_args_list:
            if call_args.kwargs.get("status_override") == "SCAFFOLD_GEN_SAVE_SUCCESS":
                scaffold_log_found = True
                self.assertIn(expected_test_target_path, call_args.kwargs.get("execution_results")[0])
                break
        self.assertTrue(scaffold_log_found, "Scaffold success log not found.")


if __name__ == '__main__': # pragma: no cover
    unittest.main()

# ### END FILE: tests/test_cli.py ###

# ### START FILE: tests/test_code_service.py ###
import unittest
from unittest import mock
from unittest.mock import patch, AsyncMock
import asyncio
import os
import sys
import uuid
import datetime
import json # Added for test data
from dataclasses import dataclass, field

try:
    from ai_assistant.code_services.service import CodeService
    from ai_assistant.core import self_modification
except ImportError: # pragma: no cover
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
    if project_root not in sys.path:
        sys.path.insert(0, project_root)
    from ai_assistant.code_services.service import CodeService
    from ai_assistant.core import self_modification


class TestCodeService(unittest.TestCase):

    def setUp(self):
        self.mock_llm_provider = mock.AsyncMock()
        self.mock_self_mod_service = mock.Mock()

        self.code_service = CodeService(
            llm_provider=self.mock_llm_provider,
            self_modification_service=self.mock_self_mod_service
        )
        # Store a version of code_service with None providers for specific tests
        self.code_service_no_llm = CodeService(llm_provider=None, self_modification_service=self.mock_self_mod_service)
        self.code_service_no_self_mod = CodeService(llm_provider=self.mock_llm_provider, self_modification_service=None)


    # --- Tests for generate_code (NEW_TOOL context) ---
    async def test_generate_code_new_tool_success_no_save(self): # RENAMED, target_path=None
        expected_metadata = {"suggested_function_name": "add_numbers", "suggested_tool_name": "addNumbers", "suggested_description": "Adds two numbers."}
        metadata_json_str = json.dumps(expected_metadata)
        expected_code_content = "def add_numbers(a: int, b: int) -> int:\n    return a + b"
        llm_output = f"# METADATA: {metadata_json_str}\n{expected_code_content}"
        self.mock_llm_provider.invoke_ollama_model_async.return_value = llm_output

        result = await self.code_service.generate_code(
            context="NEW_TOOL",
            prompt_or_description="A tool to add two numbers.",
            target_path=None # Explicitly no save
        )

        self.assertEqual(result["status"], "SUCCESS_CODE_GENERATED")
        self.assertEqual(result["code_string"], expected_code_content)
        self.assertEqual(result["metadata"], expected_metadata)
        self.assertIsNone(result["error"])
        self.assertIsNone(result.get("saved_to_path")) # Verify no save path
        self.mock_llm_provider.invoke_ollama_model_async.assert_called_once()

    @mock.patch('ai_assistant.code_services.service.write_to_file')
    async def test_generate_code_new_tool_success_and_save(self, mock_write_to_file):
        expected_metadata = {"suggested_function_name": "add_numbers", "suggested_tool_name": "addNumbers", "suggested_description": "Adds two numbers."}
        metadata_json_str = json.dumps(expected_metadata)
        expected_code_content = "def add_numbers(a: int, b: int) -> int:\n    return a + b"
        llm_output = f"# METADATA: {metadata_json_str}\n{expected_code_content}"

        self.mock_llm_provider.invoke_ollama_model_async.return_value = llm_output
        mock_write_to_file.return_value = True

        test_target_path = "generated_tools/new_tool.py"
        result = await self.code_service.generate_code(
            context="NEW_TOOL",
            prompt_or_description="A tool to add two numbers.",
            target_path=test_target_path
        )

        self.assertEqual(result["status"], "SUCCESS_CODE_GENERATED")
        self.assertEqual(result["code_string"], expected_code_content)
        self.assertEqual(result["metadata"], expected_metadata)
        self.assertEqual(result["saved_to_path"], test_target_path)
        self.assertIsNone(result["error"])
        self.mock_llm_provider.invoke_ollama_model_async.assert_called_once()
        mock_write_to_file.assert_called_once_with(test_target_path, expected_code_content)

    @mock.patch('ai_assistant.code_services.service.write_to_file')
    async def test_generate_code_new_tool_save_fails(self, mock_write_to_file):
        expected_metadata = {"suggested_function_name": "add_numbers", "suggested_tool_name": "addNumbers", "suggested_description": "Adds two numbers."}
        metadata_json_str = json.dumps(expected_metadata)
        expected_code_content = "def add_numbers(a: int, b: int) -> int:\n    return a + b"
        llm_output = f"# METADATA: {metadata_json_str}\n{expected_code_content}"

        self.mock_llm_provider.invoke_ollama_model_async.return_value = llm_output
        mock_write_to_file.return_value = False

        test_target_path = "generated_tools/new_tool_fails_save.py"
        result = await self.code_service.generate_code(
            context="NEW_TOOL",
            prompt_or_description="A tool to add two numbers, save fails.",
            target_path=test_target_path
        )

        self.assertEqual(result["status"], "ERROR_SAVING_CODE")
        self.assertEqual(result["code_string"], expected_code_content)
        self.assertEqual(result["metadata"], expected_metadata)
        self.assertIsNone(result["saved_to_path"])
        self.assertIsNotNone(result["error"])
        self.assertIn("failed to save", result["error"])
        mock_write_to_file.assert_called_once_with(test_target_path, expected_code_content)

    async def test_generate_code_new_tool_llm_no_code(self):
        self.mock_llm_provider.invoke_ollama_model_async.return_value = "" # Empty response

        result = await self.code_service.generate_code(
            context="NEW_TOOL",
            prompt_or_description="A tool."
        )
        self.assertEqual(result["status"], "ERROR_LLM_NO_CODE")
        self.assertIsNone(result["code_string"])
        self.assertIsNotNone(result["error"])

    async def test_generate_code_new_tool_missing_metadata_line(self):
        llm_output = "def my_func(): pass" # No # METADATA: line
        self.mock_llm_provider.invoke_ollama_model_async.return_value = llm_output

        result = await self.code_service.generate_code(
            context="NEW_TOOL",
            prompt_or_description="A tool."
        )
        self.assertEqual(result["status"], "ERROR_METADATA_PARSING")
        self.assertEqual(result["code_string"], "def my_func(): pass")
        self.assertIsNone(result["metadata"])
        self.assertIsNotNone(result["error"])

    async def test_generate_code_new_tool_malformed_metadata_json(self):
        llm_output = "# METADATA: {this_is_not_json: }\ndef my_func(): pass"
        self.mock_llm_provider.invoke_ollama_model_async.return_value = llm_output

        result = await self.code_service.generate_code(
            context="NEW_TOOL",
            prompt_or_description="A tool."
        )
        self.assertEqual(result["status"], "ERROR_METADATA_PARSING")
        self.assertEqual(result["code_string"].strip(), "def my_func(): pass")
        self.assertIsNone(result["metadata"])
        self.assertIsNotNone(result["error"])

    async def test_generate_code_new_tool_metadata_ok_no_code_block(self):
        expected_metadata = {"suggested_function_name": "add_numbers", "suggested_tool_name": "addNumbers", "suggested_description": "Adds two numbers."}
        metadata_json_str = json.dumps(expected_metadata)
        llm_output = f"# METADATA: {metadata_json_str}\n   # Only comments, no actual code"
        self.mock_llm_provider.invoke_ollama_model_async.return_value = llm_output

        result = await self.code_service.generate_code(
            context="NEW_TOOL",
            prompt_or_description="A tool."
        )
        self.assertEqual(result["status"], "ERROR_CODE_EMPTY_POST_METADATA")
        self.assertEqual(result["code_string"], "# Only comments, no actual code")
        self.assertEqual(result["metadata"], expected_metadata)
        self.assertIsNotNone(result["error"])

    async def test_generate_code_unsupported_context_for_generate(self):
        # This test doesn't involve LLM provider, so can use any CS instance
        result = await self.code_service.generate_code(
            context="SELF_FIX_TOOL",
            prompt_or_description="A test tool."
        )
        self.assertEqual(result["status"], "ERROR_UNSUPPORTED_CONTEXT")
        self.assertIsNone(result["code_string"])

    async def test_generate_code_llm_provider_missing(self):
        result = await self.code_service_no_llm.generate_code(
            context="NEW_TOOL",
            prompt_or_description="A tool."
        )
        self.assertEqual(result["status"], "ERROR_LLM_PROVIDER_MISSING")
        self.assertIsNone(result["code_string"])
        self.assertIn("LLM provider not configured", result["error"])

    # --- Tests for modify_code (focused on SELF_FIX_TOOL context) ---
    async def test_modify_code_self_fix_tool_success(self):
        self.mock_self_mod_service.get_function_source_code.return_value = "def old_func(a): return a"
        self.mock_llm_provider.invoke_ollama_model_async.return_value = "def old_func(a): return a + 1 # Fixed by LLM"

        result = await self.code_service.modify_code(
            context="SELF_FIX_TOOL",
            existing_code=None,
            modification_instruction="Fix the bug in old_func.",
            module_path="dummy.module",
            function_name="old_func"
        )

        self.assertEqual(result["status"], "SUCCESS_CODE_GENERATED")
        self.assertEqual(result["modified_code_string"], "def old_func(a): return a + 1 # Fixed by LLM")
        self.mock_self_mod_service.get_function_source_code.assert_called_once_with("dummy.module", "old_func")
        self.mock_llm_provider.invoke_ollama_model_async.assert_called_once()

    async def test_modify_code_self_fix_tool_no_original_code(self):
        self.mock_self_mod_service.get_function_source_code.return_value = None

        result = await self.code_service.modify_code(
            context="SELF_FIX_TOOL",
            existing_code=None,
            modification_instruction="Fix the bug.",
            module_path="dummy.module",
            function_name="some_func"
        )
        self.assertEqual(result["status"], "ERROR_NO_ORIGINAL_CODE")
        self.assertIsNone(result["modified_code_string"])
        self.mock_self_mod_service.get_function_source_code.assert_called_once_with("dummy.module", "some_func")

    async def test_modify_code_self_fix_tool_llm_no_suggestion(self):
        # Provide existing_code directly, so get_function_source_code is not called
        self.mock_llm_provider.invoke_ollama_model_async.return_value = "// NO_CODE_SUGGESTION_POSSIBLE"

        result = await self.code_service.modify_code(
            context="SELF_FIX_TOOL",
            existing_code="def old_func(a): return a",
            modification_instruction="Fix it.",
            module_path="dummy.module", # Still required for context
            function_name="old_func"    # Still required for context
        )
        self.assertEqual(result["status"], "ERROR_LLM_NO_SUGGESTION")
        self.assertIsNone(result["modified_code_string"])
        self.mock_self_mod_service.get_function_source_code.assert_not_called() # Because existing_code was provided
        self.mock_llm_provider.invoke_ollama_model_async.assert_called_once()

    async def test_modify_code_self_fix_tool_llm_empty_response(self):
        self.mock_llm_provider.invoke_ollama_model_async.return_value = "   "

        result = await self.code_service.modify_code(
            context="SELF_FIX_TOOL",
            existing_code="def old_func(a): return a", # Provide existing_code
            modification_instruction="Fix it.",
            module_path="dummy.module",
            function_name="old_func"
        )
        self.assertEqual(result["status"], "ERROR_LLM_NO_SUGGESTION")
        self.assertIsNone(result["modified_code_string"])
        self.mock_self_mod_service.get_function_source_code.assert_not_called()

    async def test_modify_code_unsupported_context(self):
        result = await self.code_service.modify_code(
            context="UNKNOWN_CONTEXT",
            modification_instruction="Do something.",
            existing_code="code"
        )
        self.assertEqual(result["status"], "ERROR_UNSUPPORTED_CONTEXT")

    async def test_modify_code_missing_details_for_self_fix(self):
        result = await self.code_service.modify_code(
            context="SELF_FIX_TOOL",
            modification_instruction="Fix it.",
            existing_code="code",
            module_path=None,
            function_name="some_func"
        )
        self.assertEqual(result["status"], "ERROR_MISSING_DETAILS")

    async def test_modify_code_llm_provider_missing(self):
        # For this test, self_modification_service might be called first if existing_code is None
        self.mock_self_mod_service.get_function_source_code.return_value = "def old_func(): pass"
        result = await self.code_service_no_llm.modify_code(
            context="SELF_FIX_TOOL",
            modification_instruction="Fix it.",
            existing_code=None, # Force attempt to use self_mod_service then llm_provider
            module_path="dummy.module",
            function_name="old_func"
        )
        self.assertEqual(result["status"], "ERROR_LLM_PROVIDER_MISSING")
        self.assertIsNone(result["modified_code_string"])
        self.assertIn("LLM provider not configured", result["error"])
        # Ensure self_mod_service was called as it's configured for code_service_no_llm
        self.mock_self_mod_service.get_function_source_code.assert_called_with("dummy.module", "old_func")


    async def test_modify_code_self_mod_service_missing(self):
        result = await self.code_service_no_self_mod.modify_code(
            context="SELF_FIX_TOOL",
            modification_instruction="Fix it.",
            existing_code=None, # This will trigger the need for self_modification_service
            module_path="dummy.module",
            function_name="some_func"
        )
        self.assertEqual(result["status"], "ERROR_SELF_MOD_SERVICE_MISSING")
        self.assertIsNone(result["modified_code_string"])
        self.assertIn("Self modification service not configured", result["error"])

    # --- Tests for generate_code (GENERATE_UNIT_TEST_SCAFFOLD context) ---
    async def test_generate_code_unit_test_scaffold_success_no_save(self): # RENAMED
        sample_code_to_test = "def my_func(x): return x*2"
        expected_scaffold = "import unittest\nfrom your_module_to_test import my_func\n\nclass TestMyFunc(unittest.TestCase):\n    def test_my_func_basic(self):\n        self.fail(\"Test not yet implemented\")"
        self.mock_llm_provider.invoke_ollama_model_async.return_value = expected_scaffold

        result = await self.code_service.generate_code(
            context="GENERATE_UNIT_TEST_SCAFFOLD",
            prompt_or_description=sample_code_to_test,
            additional_context={"module_name_hint": "your_module_to_test"},
            target_path=None # Explicitly no save
        )

        self.assertEqual(result["status"], "SUCCESS_CODE_GENERATED")
        self.assertEqual(result["code_string"], expected_scaffold)
        self.assertIsNone(result["metadata"])
        self.assertIsNone(result["error"])
        self.assertIsNone(result.get("saved_to_path"))
        self.mock_llm_provider.invoke_ollama_model_async.assert_called_once()
        args, kwargs = self.mock_llm_provider.invoke_ollama_model_async.call_args
        self.assertIn(sample_code_to_test, args[0])
        self.assertIn("module_name_hint='your_module_to_test'", args[0])

    @mock.patch('ai_assistant.code_services.service.write_to_file')
    async def test_generate_code_unit_test_scaffold_success_and_save(self, mock_write_to_file):
        sample_code_to_test = "def my_func(x): return x*2"
        expected_scaffold = "import unittest\nfrom your_module_to_test import my_func\n\nclass TestMyFunc(unittest.TestCase):\n    def test_my_func_basic(self):\n        self.fail(\"Test not yet implemented\")"
        self.mock_llm_provider.invoke_ollama_model_async.return_value = expected_scaffold
        mock_write_to_file.return_value = True

        test_target_path = "tests/test_my_func_scaffold.py"
        result = await self.code_service.generate_code(
            context="GENERATE_UNIT_TEST_SCAFFOLD",
            prompt_or_description=sample_code_to_test,
            additional_context={"module_name_hint": "your_module_to_test"},
            target_path=test_target_path
        )

        self.assertEqual(result["status"], "SUCCESS_CODE_GENERATED")
        self.assertEqual(result["code_string"], expected_scaffold)
        self.assertEqual(result["saved_to_path"], test_target_path)
        self.assertIsNone(result["error"])
        mock_write_to_file.assert_called_once_with(test_target_path, expected_scaffold)

    @mock.patch('ai_assistant.code_services.service.write_to_file')
    async def test_generate_code_unit_test_scaffold_save_fails(self, mock_write_to_file):
        sample_code_to_test = "def my_func(x): return x*2"
        expected_scaffold = "import unittest\nfrom your_module_to_test import my_func\n\nclass TestMyFunc(unittest.TestCase):\n    def test_my_func_basic(self):\n        self.fail(\"Test not yet implemented\")"
        self.mock_llm_provider.invoke_ollama_model_async.return_value = expected_scaffold
        mock_write_to_file.return_value = False

        test_target_path = "tests/test_my_func_scaffold_fails_save.py"
        result = await self.code_service.generate_code(
            context="GENERATE_UNIT_TEST_SCAFFOLD",
            prompt_or_description=sample_code_to_test,
            additional_context={"module_name_hint": "your_module_to_test"},
            target_path=test_target_path
        )

        self.assertEqual(result["status"], "ERROR_SAVING_CODE")
        self.assertEqual(result["code_string"], expected_scaffold)
        self.assertIsNone(result["saved_to_path"])
        self.assertIsNotNone(result["error"])
        self.assertIn("failed to save", result["error"])
        mock_write_to_file.assert_called_once_with(test_target_path, expected_scaffold)

    async def test_generate_code_unit_test_scaffold_llm_no_code(self):
        sample_code_to_test = "def my_func(x): return x*2"
        self.mock_llm_provider.invoke_ollama_model_async.return_value = "" # Empty response

        result = await self.code_service.generate_code(
            context="GENERATE_UNIT_TEST_SCAFFOLD",
            prompt_or_description=sample_code_to_test,
            additional_context={"module_name_hint": "your_module_to_test"}
        )
        self.assertEqual(result["status"], "ERROR_LLM_NO_CODE")
        self.assertIsNone(result["code_string"])
        self.assertIsNotNone(result["error"])

    async def test_generate_code_unit_test_scaffold_llm_returns_none(self):
        sample_code_to_test = "def my_func(x): return x*2"
        self.mock_llm_provider.invoke_ollama_model_async.return_value = None # None response

        result = await self.code_service.generate_code(
            context="GENERATE_UNIT_TEST_SCAFFOLD",
            prompt_or_description=sample_code_to_test,
            additional_context={"module_name_hint": "your_module_to_test"}
        )
        self.assertEqual(result["status"], "ERROR_LLM_NO_CODE")
        self.assertIsNone(result["code_string"])
        self.assertIsNotNone(result["error"])

    async def test_generate_code_unit_test_scaffold_cleaning_applied(self):
        sample_code_to_test = "def my_func(x): return x*2"
        raw_llm_output = "```python\ndef test_scaffold(): pass\n```"
        expected_cleaned_output = "def test_scaffold(): pass"
        self.mock_llm_provider.invoke_ollama_model_async.return_value = raw_llm_output

        result = await self.code_service.generate_code(
            context="GENERATE_UNIT_TEST_SCAFFOLD",
            prompt_or_description=sample_code_to_test
            # Not providing module_name_hint here to ensure defaults work
        )
        self.assertEqual(result["status"], "SUCCESS_CODE_GENERATED")
        self.assertEqual(result["code_string"], expected_cleaned_output)
        # Check that the default module_name_hint was used in the prompt
        args, kwargs = self.mock_llm_provider.invoke_ollama_model_async.call_args
        self.assertIn("module_name_hint='your_module_to_test'", args[0])

    # --- Tests for generate_code (EXPERIMENTAL_HIERARCHICAL_OUTLINE context) ---
    async def test_generate_code_hierarchical_outline_success(self):
        # Use self.mock_llm_provider from setUp
        expected_outline_dict = {"module_name": "test_module.py", "components": [{"type": "function", "name": "main"}]}
        llm_json_output = json.dumps(expected_outline_dict)
        self.mock_llm_provider.invoke_ollama_model_async.return_value = llm_json_output

        result = await self.code_service.generate_code(
            context="EXPERIMENTAL_HIERARCHICAL_OUTLINE",
            prompt_or_description="A simple test module."
        )

        self.assertEqual(result["status"], "SUCCESS_OUTLINE_GENERATED")
        self.assertEqual(result["parsed_outline"], expected_outline_dict)
        self.assertEqual(result["outline_str"], llm_json_output)
        self.assertIsNone(result["code_string"])
        self.assertIsNone(result["error"])
        self.mock_llm_provider.invoke_ollama_model_async.assert_called_once()

    async def test_generate_code_hierarchical_outline_llm_empty(self):
        self.mock_llm_provider.invoke_ollama_model_async.return_value = ""

        result = await self.code_service.generate_code(
            context="EXPERIMENTAL_HIERARCHICAL_OUTLINE",
            prompt_or_description="A simple test module."
        )
        self.assertEqual(result["status"], "ERROR_LLM_NO_OUTLINE")
        self.assertIsNone(result["parsed_outline"])

    async def test_generate_code_hierarchical_outline_bad_json(self):
        self.mock_llm_provider.invoke_ollama_model_async.return_value = "{'bad_json': not_quoted}" # Malformed JSON

        result = await self.code_service.generate_code(
            context="EXPERIMENTAL_HIERARCHICAL_OUTLINE",
            prompt_or_description="A simple test module."
        )
        self.assertEqual(result["status"], "ERROR_OUTLINE_PARSING")
        self.assertIsNone(result["parsed_outline"])
        self.assertIsNotNone(result["error"])

    # --- Tests for _generate_detail_for_component ---
    async def test_generate_detail_for_component_success_function(self):
        component_def = {
            "type": "function", "name": "my_util_func", "signature": "(path: str) -> bool",
            "description": "A utility function.", "body_placeholder": "Return True if path exists."
        }
        full_outline = {
            "module_name": "my_utils.py", "description": "Utility module.",
            "imports": ["os"]
        }
        # LLM is expected to return the full function definition as per current prompt design
        full_expected_code = "def my_util_func(path: str) -> bool:\n    return os.path.exists(path)"

        self.mock_llm_provider.invoke_ollama_model_async.return_value = full_expected_code

        result_code = await self.code_service._generate_detail_for_component(
            component_definition=component_def,
            full_outline=full_outline,
            llm_config=None
        )

        self.assertEqual(result_code, full_expected_code)
        self.mock_llm_provider.invoke_ollama_model_async.assert_called_once()
        prompt_arg = self.mock_llm_provider.invoke_ollama_model_async.call_args[0][0]
        self.assertIn("my_util_func", prompt_arg)
        self.assertIn("Return True if path exists", prompt_arg)
        self.assertIn("import os", prompt_arg)

    async def test_generate_detail_for_component_success_method(self):
        component_def = {
            "type": "method", "name": "process", "signature": "(self, data: dict) -> None",
            "description": "Processes data.", "body_placeholder": "self.some_attr = data.get('key')"
        }
        full_outline = {
            "module_name": "my_class_module.py",
            "components": [{
                "type": "class", "name": "MyProcessor",
                "attributes": [{"name": "some_attr", "type": "Optional[Any]"}],
                "description": "A data processor class.",
                "methods": [component_def]
            }],
            "imports": []
        }
        expected_code = "def process(self, data: dict) -> None:\n    self.some_attr = data.get('key')"
        self.mock_llm_provider.invoke_ollama_model_async.return_value = expected_code

        result_code = await self.code_service._generate_detail_for_component(
            component_definition=component_def,
            full_outline=full_outline,
            llm_config=None
        )
        self.assertEqual(result_code, expected_code)
        self.mock_llm_provider.invoke_ollama_model_async.assert_called_once()
        prompt_arg = self.mock_llm_provider.invoke_ollama_model_async.call_args[0][0]
        self.assertIn("class 'MyProcessor'", prompt_arg)
        self.assertIn("self.some_attr = data.get('key')", prompt_arg)

    async def test_generate_detail_for_component_llm_returns_none(self):
        component_def = {"type": "function", "name": "test_func", "signature": "()", "description": "", "body_placeholder": ""}
        full_outline = {"imports": []}
        self.mock_llm_provider.invoke_ollama_model_async.return_value = None

        result_code = await self.code_service._generate_detail_for_component(component_def, full_outline, None)
        self.assertIsNone(result_code)

    async def test_generate_detail_for_component_llm_returns_error_marker(self):
        component_def = {"type": "function", "name": "test_func", "signature": "()", "description": "", "body_placeholder": ""}
        full_outline = {"imports": []}
        self.mock_llm_provider.invoke_ollama_model_async.return_value = "# IMPLEMENTATION_ERROR: Too complex."

        result_code = await self.code_service._generate_detail_for_component(component_def, full_outline, None)
        self.assertIsNone(result_code)

    async def test_generate_detail_for_component_llm_returns_short_code(self):
        component_def = {"type": "function", "name": "test_func", "signature": "()", "description": "", "body_placeholder": ""}
        full_outline = {"imports": []}
        self.mock_llm_provider.invoke_ollama_model_async.return_value = "pass" # Too short

        result_code = await self.code_service._generate_detail_for_component(component_def, full_outline, None)
        self.assertIsNone(result_code)

    async def test_generate_detail_for_component_code_cleaning(self):
        component_def = {"type": "function", "name": "test_func", "signature": "()", "description": "", "body_placeholder": ""}
        full_outline = {"imports": []}
        raw_code = "```python\ndef test_func():\n    # Escaped newline test \\n    pass\n```"
        # The implementation now replaces "\\n" with "\n" and then all "\n" with actual newlines.
        # So, "\\n" -> "\n" (literal newline char)
        expected_cleaned_code = "def test_func():\n    # Escaped newline test \n    pass"

        self.mock_llm_provider.invoke_ollama_model_async.return_value = raw_code

        result_code = await self.code_service._generate_detail_for_component(component_def, full_outline, None)
        self.assertEqual(result_code, expected_cleaned_code)

    # --- Tests for _assemble_components ---
    def test_assemble_components_only_functions(self):
        outline = {
            "imports": ["os", "sys"],
            "components": [
                {"type": "function", "name": "func1", "signature": "()", "description": "d1", "body_placeholder": "p1"},
                {"type": "function", "name": "func2", "signature": "(x: int)", "description": "d2", "body_placeholder": "p2"}
            ],
            "main_execution_block": "if __name__ == '__main__':\n    func1()"
        }
        details = {
            "func1": "def func1():\n    print('hello')",
            "func2": "def func2(x: int):\n    print(f'x is {x}')"
        }
        result_code = self.code_service._assemble_components(outline, details)

        expected_code = """import os
import sys

def func1():
    print('hello')

def func2(x: int):
    print(f'x is {x}')


if __name__ == '__main__':
    func1()"""
        self.assertEqual(result_code.strip(), expected_code.strip())

    def test_assemble_components_class_with_methods(self):
        outline = {
            "imports": ["math"],
            "components": [{
                "type": "class", "name": "MyCalc", "description": "A calculator.",
                "attributes": [{"name": "pi", "type": "float", "description": "Value of PI"}],
                "methods": [
                    {"type": "method", "name": "__init__", "signature": "(self, val: float)", "description": "ctor", "body_placeholder": "self.val = val"},
                    {"type": "method", "name": "add", "signature": "(self, x: float) -> float", "description": "adds", "body_placeholder": "return self.val + x"}
                ]
            }]
        }
        details = {
            "MyCalc.__init__": "def __init__(self, val: float):\n    self.val = val",
            "MyCalc.add": "def add(self, x: float) -> float:\n    return self.val + x"
        }
        result_code = self.code_service._assemble_components(outline, details)

        expected_code = """import math

class MyCalc:
    """A calculator."""

    # Defined attributes (from outline):
    # pi: float # Value of PI

    def __init__(self, val: float):
        self.val = val

    def add(self, x: float) -> float:
        return self.val + x

"""
        self.assertEqual(result_code.strip(), expected_code.strip())


    def test_assemble_components_missing_detail_uses_placeholder(self):
        outline = {
            "components": [{"type": "function", "name": "func1", "signature": "()", "description": "Test func", "body_placeholder": "pass"}]
        }
        details = {} # func1 detail missing
        result_code = self.code_service._assemble_components(outline, details)

        self.assertIn("# Function 'func1' was planned but not generated.", result_code)
        self.assertIn("def func1():", result_code)
        self.assertIn("Original placeholder: pass", result_code)

    def test_assemble_components_module_docstring(self):
        outline = {"module_docstring": "This is a test module."}
        details = {}
        result_code = self.code_service._assemble_components(outline, details)
        self.assertIn('"""This is a test module."""', result_code)

    # --- Tests for generate_code (EXPERIMENTAL_HIERARCHICAL_FULL_TOOL context) ---
    async def test_generate_code_hierarchical_full_tool_success(self):
        mock_outline = {
            "module_name": "test_tool.py",
            "components": [
                {"type": "function", "name": "func_one", "signature": "()", "description": "d1", "body_placeholder": "p1"},
                {"type": "class", "name": "MyClass", "methods": [
                    {"type": "method", "name": "method_a", "signature": "(self)", "description": "d2", "body_placeholder": "p2"}
                ]}
            ]
        }
        outline_gen_success_return = {
            "status": "SUCCESS_OUTLINE_GENERATED", "parsed_outline": mock_outline,
            "outline_str": json.dumps(mock_outline), "logs": [], "error": None
        }

        detail_for_func_one = "def func_one():\n    pass # func_one_impl"
        detail_for_method_a = "def method_a(self):\n    pass # method_a_impl"

        async def mock_recursive_generate_code(*args, **kwargs):
            if kwargs.get('context') == "EXPERIMENTAL_HIERARCHICAL_OUTLINE":
                return outline_gen_success_return
            return {"status": "ERROR_UNEXPECTED_RECURSIVE_CALL"} # pragma: no cover

        async def mock_detail_gen(*args, **kwargs):
            component_def = args[0]
            if component_def["name"] == "func_one":
                return detail_for_func_one
            elif component_def["name"] == "method_a":
                return detail_for_method_a
            return None # pragma: no cover

        with mock.patch.object(self.code_service, 'generate_code', side_effect=mock_recursive_generate_code) as mock_outline_call, \
             mock.patch.object(self.code_service, '_generate_detail_for_component', side_effect=mock_detail_gen) as mock_detail_call:

            result = await self.code_service.generate_code(
                context="EXPERIMENTAL_HIERARCHICAL_FULL_TOOL",
                prompt_or_description="A complex tool."
            )

            self.assertEqual(result["status"], "SUCCESS_HIERARCHICAL_DETAILS_GENERATED")
            self.assertEqual(result["parsed_outline"], mock_outline)
            self.assertIsNone(result["code_string"])

            expected_component_details = {
                "func_one": detail_for_func_one,
                "MyClass.method_a": detail_for_method_a
            }
            self.assertEqual(result["component_details"], expected_component_details)

            mock_outline_call.assert_called_once_with(
                context="EXPERIMENTAL_HIERARCHICAL_OUTLINE",
                prompt_or_description="A complex tool.",
                language="python",
                llm_config=None,
                additional_context=None
            )

            self.assertEqual(mock_detail_call.call_count, 2)
            # Ensure component definitions are passed correctly
            # Note: Direct comparison of dicts passed to mocks can be tricky if order or extra keys differ.
            # It's often better to assert specific key-value pairs or types if full dict match is problematic.
            # For this test, we assume the component_def dictionaries are passed as is from mock_outline.
            mock_detail_call.assert_any_call(mock_outline["components"][0], mock_outline, None) # func_one
            mock_detail_call.assert_any_call(mock_outline["components"][1]["methods"][0], mock_outline, None) # MyClass.method_a


    async def test_generate_code_hierarchical_full_tool_outline_fails(self):
        outline_gen_failure_return = {
            "status": "ERROR_OUTLINE_PARSING", "parsed_outline": None,
            "outline_str": "{bad json", "logs": ["Failed parsing"], "error": "JSON error"
        }

        async def mock_recursive_generate_code_fail_outline(*args, **kwargs):
            if kwargs.get('context') == "EXPERIMENTAL_HIERARCHICAL_OUTLINE":
                return outline_gen_failure_return
            return {"status": "ERROR_UNEXPECTED_RECURSIVE_CALL"} # pragma: no cover

        with mock.patch.object(self.code_service, 'generate_code', side_effect=mock_recursive_generate_code_fail_outline) as mock_outline_call:
            result = await self.code_service.generate_code(
                context="EXPERIMENTAL_HIERARCHICAL_FULL_TOOL",
                prompt_or_description="A complex tool."
            )
            self.assertEqual(result["status"], "ERROR_OUTLINE_PARSING")
            self.assertIsNone(result["component_details"])
            mock_outline_call.assert_called_once()


    async def test_generate_code_hierarchical_full_tool_one_detail_fails(self):
        mock_outline = { # Simplified outline for this test
            "module_name": "test_tool.py",
            "components": [
                {"type": "function", "name": "func_one", "signature": "()", "description": "d1", "body_placeholder": "p1"},
                {"type": "function", "name": "func_two", "signature": "()", "description": "d2", "body_placeholder": "p2"}
            ]
        }
        outline_gen_success_return = {"status": "SUCCESS_OUTLINE_GENERATED", "parsed_outline": mock_outline, "logs": []}

        detail_for_func_one = "def func_one(): pass"

        async def mock_recursive_generate_code_for_partial(*args, **kwargs):
            if kwargs.get('context') == "EXPERIMENTAL_HIERARCHICAL_OUTLINE":
                return outline_gen_success_return
            return {"status": "ERROR_UNEXPECTED_RECURSIVE_CALL"} # pragma: no cover

        async def mock_detail_gen_partial_fail(*args, **kwargs):
            component_def = args[0]
            if component_def["name"] == "func_one":
                return detail_for_func_one
            elif component_def["name"] == "func_two":
                return None
            return None # pragma: no cover

        with mock.patch.object(self.code_service, 'generate_code', side_effect=mock_recursive_generate_code_for_partial), \
             mock.patch.object(self.code_service, '_generate_detail_for_component', side_effect=mock_detail_gen_partial_fail) as mock_detail_call:

            result = await self.code_service.generate_code(
                context="EXPERIMENTAL_HIERARCHICAL_FULL_TOOL",
                prompt_or_description="Tool with two funcs."
            )

            self.assertEqual(result["status"], "PARTIAL_HIERARCHICAL_DETAILS_GENERATED")
            self.assertEqual(result["parsed_outline"], mock_outline)
            expected_component_details = {
                "func_one": detail_for_func_one,
                "func_two": None
            }
            self.assertEqual(result["component_details"], expected_component_details)
            self.assertIsNotNone(result["error"])
            self.assertEqual(mock_detail_call.call_count, 2)

    # --- Tests for generate_code (HIERARCHICAL_GEN_COMPLETE_TOOL context) ---
    async def test_generate_code_hierarchical_complete_tool_success_no_save(self): # RENAMED
        mock_outline = {"module_name": "tool.py", "imports": ["os"], "components": [{"type": "function", "name": "my_func"}]}
        mock_component_details = {"my_func": "def my_func():\n    print('done')"}
        # This expected code should be what _assemble_components produces based on the mocks above
        expected_assembled_code = self.code_service._assemble_components(mock_outline, mock_component_details)


        orchestration_success_return = {
            "status": "SUCCESS_HIERARCHICAL_DETAILS_GENERATED",
            "parsed_outline": mock_outline,
            "component_details": mock_component_details,
            "logs": [], "error": None
        }

        async def mock_recursive_generate_code(*args, **kwargs):
            if kwargs.get('context') == "EXPERIMENTAL_HIERARCHICAL_FULL_TOOL":
                return orchestration_success_return
            raise AssertionError(f"Unexpected recursive call to generate_code with context: {kwargs.get('context')}") # pragma: no cover

        # For this test, we let _assemble_components run, so no need to mock it.
        with mock.patch.object(self.code_service, 'generate_code', side_effect=mock_recursive_generate_code) as mock_inner_gen_code_call:
            result = await self.code_service.generate_code(
                context="HIERARCHICAL_GEN_COMPLETE_TOOL",
                prompt_or_description="A complex tool requiring assembly.",
                target_path=None # Explicitly no save
            )

            self.assertEqual(result["status"], "SUCCESS_HIERARCHICAL_ASSEMBLED")
            self.assertEqual(result["code_string"].strip(), expected_assembled_code.strip()) # Compare stripped
            self.assertEqual(result["parsed_outline"], mock_outline)
            self.assertEqual(result["component_details"], mock_component_details)
            self.assertIsNone(result.get("saved_to_path"))

            mock_inner_gen_code_call.assert_called_once_with(
                context="EXPERIMENTAL_HIERARCHICAL_FULL_TOOL",
                prompt_or_description="A complex tool requiring assembly.",
                language="python",
                llm_config=None,
                additional_context=None
            )

    @mock.patch('ai_assistant.code_services.service.write_to_file')
    async def test_generate_code_hierarchical_complete_tool_success_and_save(self, mock_write_to_file):
        mock_outline = {"module_name": "tool.py", "imports": ["os"], "components": [{"type": "function", "name": "my_func"}]}
        mock_component_details = {"my_func": "def my_func():\n    print('done')"}
        # Let _assemble_components run to get the code that would be saved
        expected_assembled_code = self.code_service._assemble_components(mock_outline, mock_component_details)
        mock_write_to_file.return_value = True

        orchestration_success_return = {
            "status": "SUCCESS_HIERARCHICAL_DETAILS_GENERATED",
            "parsed_outline": mock_outline,
            "component_details": mock_component_details,
            "logs": [], "error": None
        }

        async def mock_recursive_generate_code(*args, **kwargs):
            if kwargs.get('context') == "EXPERIMENTAL_HIERARCHICAL_FULL_TOOL":
                return orchestration_success_return
            raise AssertionError(f"Unexpected recursive call to generate_code with context: {kwargs.get('context')}") # pragma: no cover

        with mock.patch.object(self.code_service, 'generate_code', side_effect=mock_recursive_generate_code) as mock_inner_gen_code_call:
            test_target_path = "output/hierarchical_tool.py"
            result = await self.code_service.generate_code(
                context="HIERARCHICAL_GEN_COMPLETE_TOOL",
                prompt_or_description="A complex tool requiring assembly.",
                target_path=test_target_path
            )

            self.assertEqual(result["status"], "SUCCESS_HIERARCHICAL_ASSEMBLED")
            self.assertEqual(result["code_string"].strip(), expected_assembled_code.strip())
            self.assertEqual(result["saved_to_path"], test_target_path)
            mock_write_to_file.assert_called_once_with(test_target_path, expected_assembled_code)

    @mock.patch('ai_assistant.code_services.service.write_to_file')
    async def test_generate_code_hierarchical_complete_tool_save_fails(self, mock_write_to_file):
        mock_outline = {"module_name": "tool.py", "imports": ["os"], "components": [{"type": "function", "name": "my_func"}]}
        mock_component_details = {"my_func": "def my_func():\n    print('done')"}
        expected_assembled_code = self.code_service._assemble_components(mock_outline, mock_component_details) # Let it run
        mock_write_to_file.return_value = False # Simulate save failure

        orchestration_success_return = {
            "status": "SUCCESS_HIERARCHICAL_DETAILS_GENERATED",
            "parsed_outline": mock_outline,
            "component_details": mock_component_details,
            "logs": [], "error": None
        }

        async def mock_recursive_generate_code(*args, **kwargs):
            if kwargs.get('context') == "EXPERIMENTAL_HIERARCHICAL_FULL_TOOL":
                return orchestration_success_return
            raise AssertionError(f"Unexpected recursive call to generate_code with context: {kwargs.get('context')}") # pragma: no cover

        with mock.patch.object(self.code_service, 'generate_code', side_effect=mock_recursive_generate_code) as mock_inner_gen_code_call:
            test_target_path = "output/hierarchical_tool_fail_save.py"
            result = await self.code_service.generate_code(
                context="HIERARCHICAL_GEN_COMPLETE_TOOL",
                prompt_or_description="A complex tool, save fails.",
                target_path=test_target_path
            )

            self.assertEqual(result["status"], "ERROR_SAVING_ASSEMBLED_CODE")
            self.assertEqual(result["code_string"].strip(), expected_assembled_code.strip()) # Code still generated
            self.assertIsNone(result["saved_to_path"])
            self.assertIsNotNone(result["error"])
            self.assertIn("failed to save", result["error"])
            mock_write_to_file.assert_called_once_with(test_target_path, expected_assembled_code)

    async def test_generate_code_hierarchical_complete_tool_orchestration_fails(self):
        orchestration_failure_return = {
            "status": "ERROR_OUTLINE_PARSING", "parsed_outline": None,
            "component_details": None, "logs": ["Failed parsing"], "error": "JSON error"
        }

        async def mock_recursive_generate_code_fail(*args, **kwargs):
            if kwargs.get('context') == "EXPERIMENTAL_HIERARCHICAL_FULL_TOOL":
                return orchestration_failure_return
            raise AssertionError("Unexpected recursive call") # pragma: no cover

        with mock.patch.object(self.code_service, 'generate_code', side_effect=mock_recursive_generate_code_fail) as mock_inner_gen_code_call, \
             mock.patch.object(self.code_service, '_assemble_components') as mock_assemble:

            result = await self.code_service.generate_code(
                context="HIERARCHICAL_GEN_COMPLETE_TOOL",
                prompt_or_description="A complex tool."
            )

            self.assertEqual(result["status"], "ERROR_OUTLINE_PARSING")
            self.assertIsNone(result["code_string"])
            mock_inner_gen_code_call.assert_called_once()
            mock_assemble.assert_not_called()

    async def test_generate_code_hierarchical_complete_tool_assembly_fails(self):
        mock_outline = {"components": []}
        mock_component_details = {}
        orchestration_success_return = {
            "status": "SUCCESS_HIERARCHICAL_DETAILS_GENERATED",
            "parsed_outline": mock_outline,
            "component_details": mock_component_details,
            "logs": [], "error": None # Ensure error is None for this path
        }

        async def mock_recursive_generate_code_for_assembly_fail(*args, **kwargs):
            if kwargs.get('context') == "EXPERIMENTAL_HIERARCHICAL_FULL_TOOL":
                return orchestration_success_return
            raise AssertionError("Unexpected recursive call") # pragma: no cover

        with mock.patch.object(self.code_service, 'generate_code', side_effect=mock_recursive_generate_code_for_assembly_fail), \
             mock.patch.object(self.code_service, '_assemble_components', side_effect=Exception("Assembly crashed!")) as mock_assemble:

            result = await self.code_service.generate_code(
                context="HIERARCHICAL_GEN_COMPLETE_TOOL",
                prompt_or_description="A complex tool."
            )

            self.assertEqual(result["status"], "ERROR_ASSEMBLY_FAILED")
            self.assertIsNone(result["code_string"])
            self.assertIn("Assembly crashed!", result.get("error", ""))
            mock_assemble.assert_called_once()


if __name__ == '__main__': # pragma: no cover
    # This custom runner will execute sync tests via standard unittest mechanisms
    # and then gather and run async tests using an asyncio event loop.
    suite_sync = unittest.TestSuite()
    sync_tests_found = False
    async_test_methods_names = []

    for name in dir(TestCodeService):
        if name.startswith("test_"):
            method = getattr(TestCodeService, name)
            if asyncio.iscoroutinefunction(method):
                async_test_methods_names.append(name)
            else:
                suite_sync.addTest(TestCodeService(name))
                sync_tests_found = True

    if sync_tests_found:
        print("Running synchronous tests...")
        runner_sync = unittest.TextTestRunner()
        runner_sync.run(suite_sync)

    if async_test_methods_names:
        print("\nRunning asynchronous tests...")
        loop = asyncio.get_event_loop_policy().new_event_loop()
        asyncio.set_event_loop(loop)

        async def run_specific_async_tests():
            test_instance = TestCodeService()
            # Call setUp for the instance
            if hasattr(test_instance, 'setUp'):
                 test_instance.setUp()

            tasks = []
            for name in async_test_methods_names:
                method_to_run = getattr(test_instance, name)
                # Check if the method is already bound or needs binding
                # For instance methods, getattr directly gives a bound method
                if asyncio.iscoroutinefunction(method_to_run):
                     tasks.append(method_to_run())
                else: # Should not happen with this filtering
                     print(f"Warning: {name} is not a coroutine function, skipping.")

            await asyncio.gather(*tasks)

            # Call tearDown for the instance
            if hasattr(test_instance, 'tearDown'):
                test_instance.tearDown()

        try:
            loop.run_until_complete(run_specific_async_tests())
        finally:
            loop.close()
            asyncio.set_event_loop(None)

    if not sync_tests_found and not async_test_methods_names:
        print("No tests found.")

# ### END FILE: tests/test_code_service.py ###

# ### START FILE: tests/test_code_synthesis_service.py ###
import unittest
from unittest import mock
import asyncio
import os
import sys
import uuid
import datetime
from dataclasses import field

try:
    from ai_assistant.code_synthesis import CodeSynthesisService, CodeTaskRequest, CodeTaskType, CodeTaskStatus, CodeTaskResult
except ImportError: # pragma: no cover
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
    if project_root not in sys.path:
        sys.path.insert(0, project_root)
    from ai_assistant.code_synthesis import CodeSynthesisService, CodeTaskRequest, CodeTaskType, CodeTaskStatus, CodeTaskResult

class TestCodeSynthesisService(unittest.IsolatedAsyncioTestCase): # Use IsolatedAsyncioTestCase for async tests

    def setUp(self):
        self.service = CodeSynthesisService()

    @mock.patch('ai_assistant.code_synthesis.service.self_modification.get_function_source_code')
    @mock.patch('ai_assistant.code_synthesis.service.invoke_ollama_model_async', new_callable=mock.AsyncMock)
    async def test_handle_existing_tool_self_fix_llm_success(self, mock_invoke_llm, mock_get_source):
        mock_get_source.return_value = "def old_func(a): return a"
        mock_invoke_llm.return_value = "def old_func(a): return a + 1 # Fixed by LLM"

        request_data = {
            "module_path": "dummy.module",
            "function_name": "old_func",
            "problem_description": "Needs a fix"
        }
        request = CodeTaskRequest(
            task_type=CodeTaskType.EXISTING_TOOL_SELF_FIX_LLM,
            context_data=request_data
        )

        result = await self.service._handle_existing_tool_self_fix_llm(request)

        self.assertEqual(result.status, CodeTaskStatus.SUCCESS)
        self.assertIsNotNone(result.generated_code)
        self.assertIn("# Fixed by LLM", result.generated_code if result.generated_code else "")
        mock_get_source.assert_called_once_with("dummy.module", "old_func")
        mock_invoke_llm.assert_called_once()
        self.assertEqual(result.request_id, request.request_id) # Corrected assertion

    @mock.patch('ai_assistant.code_synthesis.service.self_modification.get_function_source_code')
    @mock.patch('ai_assistant.code_synthesis.service.invoke_ollama_model_async', new_callable=mock.AsyncMock)
    async def test_handle_existing_tool_self_fix_llm_no_code_from_llm(self, mock_invoke_llm, mock_get_source):
        mock_get_source.return_value = "def old_func(a): return a"
        mock_invoke_llm.return_value = "// NO_CODE_SUGGESTION_POSSIBLE"

        request = CodeTaskRequest(task_type=CodeTaskType.EXISTING_TOOL_SELF_FIX_LLM, context_data={
            "module_path": "dummy.module", "function_name": "old_func", "problem_description": "Needs a fix"
        })
        result = await self.service._handle_existing_tool_self_fix_llm(request)

        self.assertEqual(result.status, CodeTaskStatus.FAILURE_LLM_GENERATION)
        self.assertIsNone(result.generated_code)

    @mock.patch('ai_assistant.code_synthesis.service.self_modification.get_function_source_code')
    async def test_handle_existing_tool_self_fix_llm_no_original_code(self, mock_get_source):
        mock_get_source.return_value = None

        request = CodeTaskRequest(task_type=CodeTaskType.EXISTING_TOOL_SELF_FIX_LLM, context_data={
            "module_path": "dummy.module", "function_name": "old_func", "problem_description": "Needs a fix"
        })
        result = await self.service._handle_existing_tool_self_fix_llm(request)

        self.assertEqual(result.status, CodeTaskStatus.FAILURE_PRECONDITION)
        self.assertIsNotNone(result.error_message)

    async def test_submit_task_unsupported_type(self):
        # Create a dummy enum member for test by directly assigning an int value outside the Enum definition
        # This is a bit hacky for testing but avoids modifying the original Enum for a test case.
        class MockUnsupportedTaskType(Enum):
             BOGUS_TASK = 999
             # Add existing valid values to satisfy isinstance checks if any occur before dispatch
             NEW_TOOL_CREATION_LLM = CodeTaskType.NEW_TOOL_CREATION_LLM.value
             EXISTING_TOOL_SELF_FIX_LLM = CodeTaskType.EXISTING_TOOL_SELF_FIX_LLM.value
             EXISTING_TOOL_SELF_FIX_AST = CodeTaskType.EXISTING_TOOL_SELF_FIX_AST.value

        request = CodeTaskRequest(task_type=MockUnsupportedTaskType.BOGUS_TASK, context_data={}) # type: ignore
        result = await self.service.submit_task(request)
        self.assertEqual(result.status, CodeTaskStatus.FAILURE_UNSUPPORTED_TASK)

    async def test_handle_new_tool_creation_llm_placeholder(self):
        request = CodeTaskRequest(task_type=CodeTaskType.NEW_TOOL_CREATION_LLM, context_data={"description": "test"})
        result = await self.service._handle_new_tool_creation_llm(request) # Test private method directly
        self.assertEqual(result.status, CodeTaskStatus.FAILURE_UNSUPPORTED_TASK)
        self.assertIn("not fully implemented", result.error_message or "")

if __name__ == '__main__': # pragma: no cover
    unittest.main()

# ### END FILE: tests/test_code_synthesis_service.py ###

# ### START FILE: tests/test_learning.py ###
import unittest
from unittest import mock # Ensure mock is imported
import asyncio
import os
import datetime
import tempfile
import json
import uuid # Added for generating entry_ids
from typing import List, Dict, Any, Optional
from dataclasses import field # Ensure field is imported for dataclasses

# Attempt to import from the ai_assistant package.
try:
    from ai_assistant.learning.learning import LearningAgent, ActionableInsight, InsightType
    from ai_assistant.core.reflection import ReflectionLogEntry
    from ai_assistant.execution.action_executor import ActionExecutor # Needed for patching target
except ImportError: # pragma: no cover
    import sys
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
    if project_root not in sys.path:
        sys.path.insert(0, project_root)
    from ai_assistant.learning.learning import LearningAgent, ActionableInsight, InsightType
    from ai_assistant.core.reflection import ReflectionLogEntry
    from ai_assistant.execution.action_executor import ActionExecutor


class TestActionableInsight(unittest.TestCase):

    def test_insight_creation_default_id(self):
        timestamp_now_iso = datetime.datetime.now(datetime.timezone.utc).isoformat()
        insight = ActionableInsight(
            type=InsightType.KNOWLEDGE_GAP_IDENTIFIED,
            description="Test description",
            source_reflection_entry_ids=["entry1"],
            creation_timestamp=timestamp_now_iso
        )
        self.assertIsNotNone(insight.insight_id)
        self.assertTrue(InsightType.KNOWLEDGE_GAP_IDENTIFIED.name in insight.insight_id)
        self.assertEqual(insight.status, "NEW")
        self.assertEqual(insight.priority, 5)

    def test_insight_creation_with_id(self):
        insight = ActionableInsight(
            insight_id="custom_id_123",
            type=InsightType.TOOL_BUG_SUSPECTED,
            description="Tool bug",
            source_reflection_entry_ids=["entry2"]
        )
        self.assertEqual(insight.insight_id, "custom_id_123")


class TestLearningAgent(unittest.TestCase):

    def setUp(self):
        self.temp_insights_file = tempfile.NamedTemporaryFile(delete=False, mode='w+', suffix='.json')
        self.temp_insights_filepath = self.temp_insights_file.name
        self.temp_insights_file.close()
        # self.agent is no longer created here to allow per-test mocking of ActionExecutor

    def tearDown(self):
        if os.path.exists(self.temp_insights_filepath):
            os.remove(self.temp_insights_filepath)

    def _create_mock_reflection_entry(
        self,
        goal: str,
        status: str,
        entry_id: Optional[str] = None, # Added entry_id for explicit setting if needed
        error_type: Optional[str] = None,
        error_message: Optional[str] = None,
        plan: Optional[List[Dict[str, Any]]] = None,
        results: Optional[List[Any]] = None,
        notes: Optional[str] = None
    ) -> ReflectionLogEntry:
        # ReflectionLogEntry's default_factory for entry_id will handle it if None
        return ReflectionLogEntry(
            entry_id=entry_id if entry_id else str(uuid.uuid4()), # Ensure it has an ID
            goal_description=goal,
            status=status,
            plan=plan if plan is not None else [],
            execution_results=results if results is not None else [],
            error_type=error_type,
            error_message=error_message,
            notes=notes,
            timestamp=datetime.datetime.now(datetime.timezone.utc)
        )

    def test_agent_initialization_empty_file(self):
        # Patch ActionExecutor during LearningAgent instantiation for this test
        with mock.patch('ai_assistant.learning.learning.ActionExecutor') as MockedActionExecutor:
            agent = LearningAgent(insights_filepath=self.temp_insights_filepath)
            self.assertEqual(len(agent.insights), 0)
            MockedActionExecutor.assert_called_once()


    def test_agent_initialization_with_existing_insights(self):
        timestamp_now = datetime.datetime.now(datetime.timezone.utc)
        insights_data = [
            ActionableInsight(
                insight_id="id1", type=InsightType.KNOWLEDGE_GAP_IDENTIFIED,
                description="desc1", source_reflection_entry_ids=["src1"],
                creation_timestamp=timestamp_now.isoformat()
            ).__dict__,
            ActionableInsight(
                insight_id="id2", type=InsightType.TOOL_BUG_SUSPECTED,
                description="desc2", source_reflection_entry_ids=["src2"],
                related_tool_name="tool_A",
                creation_timestamp=(timestamp_now + datetime.timedelta(seconds=1)).isoformat()
            ).__dict__
        ]
        insights_data[0]['type'] = InsightType.KNOWLEDGE_GAP_IDENTIFIED.name
        insights_data[1]['type'] = InsightType.TOOL_BUG_SUSPECTED.name

        with open(self.temp_insights_filepath, 'w') as f:
            json.dump(insights_data, f)

        with mock.patch('ai_assistant.learning.learning.ActionExecutor'): # Mock ActionExecutor
            agent = LearningAgent(insights_filepath=self.temp_insights_filepath)
            self.assertEqual(len(agent.insights), 2)
            self.assertEqual(agent.insights[0].insight_id, "id1")
            self.assertEqual(agent.insights[1].type, InsightType.TOOL_BUG_SUSPECTED)

    def test_process_reflection_entry_generates_insight(self):
        with mock.patch('ai_assistant.learning.learning.ActionExecutor'): # Mock ActionExecutor
            agent = LearningAgent(insights_filepath=self.temp_insights_filepath)

        failed_plan = [{"tool_name": "broken_tool", "args": ["a"], "kwargs": {}}]
        mock_entry_failure = self._create_mock_reflection_entry(
            goal="test failure", status="FAILURE", error_type="TestError",
            error_message="Something broke", plan=failed_plan, results=[Exception("TestError")]
        )
        insight = agent.process_reflection_entry(mock_entry_failure)

        self.assertIsNotNone(insight)
        self.assertEqual(len(agent.insights), 1)
        if insight:
            self.assertEqual(insight.type, InsightType.TOOL_BUG_SUSPECTED)
            self.assertEqual(insight.related_tool_name, "broken_tool")
            self.assertEqual(insight.source_reflection_entry_ids[0], mock_entry_failure.entry_id) # Verify entry_id
            self.assertEqual(insight.metadata.get("original_reflection_entry_ref_id"), mock_entry_failure.entry_id) # Verify metadata

            self.assertTrue(os.path.exists(self.temp_insights_filepath))
            with open(self.temp_insights_filepath, 'r') as f:
                saved_data = json.load(f)
                self.assertEqual(len(saved_data), 1)
                self.assertEqual(saved_data[0]['insight_id'], insight.insight_id)

    async def test_review_and_propose_next_action_selects_highest_priority(self):
        # Instantiate agent here to allow easier mocking of its action_executor
        agent = LearningAgent(insights_filepath=self.temp_insights_filepath)
        agent.action_executor = mock.AsyncMock() # Replace with an AsyncMock instance

        ts_now_iso = datetime.datetime.now(datetime.timezone.utc).isoformat()

        # Insight for KNOWLEDGE_GAP_IDENTIFIED (priority 1)
        knowledge_insight_id = str(uuid.uuid4())
        high_priority_insight = ActionableInsight(
            insight_id="high_prio_insight",
            type=InsightType.KNOWLEDGE_GAP_IDENTIFIED,
            description="Urgent knowledge needed",
            source_reflection_entry_ids=[knowledge_insight_id],
            knowledge_to_learn="Learn X immediately",
            priority=1, status="NEW", creation_timestamp=ts_now_iso,
            metadata={'original_reflection_entry_ref_id': knowledge_insight_id}
        )

        # Insight for TOOL_BUG_SUSPECTED (priority 3)
        tool_mod_insight_id = str(uuid.uuid4())
        tool_mod_insight = ActionableInsight(
            insight_id="tool_mod_insight",
            type=InsightType.TOOL_BUG_SUSPECTED,
            description="Tool needs fix",
            source_reflection_entry_ids=[tool_mod_insight_id],
            related_tool_name="test_tool", priority=3, status="NEW",
            creation_timestamp=(datetime.datetime.fromisoformat(ts_now_iso) + datetime.timedelta(seconds=1)).isoformat(),
            metadata={
                'original_reflection_entry_ref_id': tool_mod_insight_id,
                'module_path': 'dummy.module',
                'function_name': 'dummy_func'
            }
        )
        agent.insights.extend([tool_mod_insight, high_priority_insight]) # Add higher prio last to test sorting
        agent._save_insights()

        # Test 1: Highest priority (knowledge_insight), execution success
        agent.action_executor.execute_action.return_value = True
        action_result_tuple_1 = await agent.review_and_propose_next_action()

        self.assertIsNotNone(action_result_tuple_1)
        if action_result_tuple_1:
            proposed_action_1, exec_success_1 = action_result_tuple_1
            self.assertTrue(exec_success_1)
            self.assertEqual(proposed_action_1["source_insight_id"], high_priority_insight.insight_id)
            self.assertEqual(proposed_action_1["action_type"], "ADD_LEARNED_FACT")
            found_insight_1 = next(i for i in agent.insights if i.insight_id == high_priority_insight.insight_id)
            self.assertEqual(found_insight_1.status, "ACTION_SUCCESSFUL")
        agent.action_executor.execute_action.assert_called_once()
        agent.action_executor.execute_action.reset_mock() # Reset for next call

        # Test 2: Next priority (tool_mod_insight), execution failure
        agent.action_executor.execute_action.return_value = False
        action_result_tuple_2 = await agent.review_and_propose_next_action()

        self.assertIsNotNone(action_result_tuple_2)
        if action_result_tuple_2:
            proposed_action_2, exec_success_2 = action_result_tuple_2
            self.assertFalse(exec_success_2)
            self.assertEqual(proposed_action_2["source_insight_id"], tool_mod_insight.insight_id)
            self.assertEqual(proposed_action_2["action_type"], "PROPOSE_TOOL_MODIFICATION")
            found_insight_2 = next(i for i in agent.insights if i.insight_id == tool_mod_insight.insight_id)
            self.assertEqual(found_insight_2.status, "ACTION_FAILED")
        agent.action_executor.execute_action.assert_called_once()
        agent.action_executor.execute_action.reset_mock()

        # Test 3: No more "NEW" insights
        action_result_tuple_3 = await agent.review_and_propose_next_action()
        self.assertIsNone(action_result_tuple_3)
        agent.action_executor.execute_action.assert_not_called() # Should not be called if no insights

if __name__ == '__main__': # pragma: no cover
    unittest.main()

# ### END FILE: tests/test_learning.py ###

# ### START FILE: tests/test_placeholder.py ###
def test_always_passes():
    assert True

# ### END FILE: tests/test_placeholder.py ###

# ### START FILE: tests/test_planning.py ###
import unittest
from unittest.mock import patch
import json # To construct mock LLM responses
from ai_assistant.planning.planning import PlannerAgent

class TestPlannerAgentLLMSearch(unittest.TestCase):

    def setUp(self):
        """Set up test fixtures, if any."""
        self.planner = PlannerAgent()
        self.available_tools = {
            "search_duckduckgo": "Searches the internet using DuckDuckGo. Args: query (str). Returns JSON string.",
            "process_search_results": "Processes JSON search results. Args: search_query (str), search_results_json (str). Optional kwargs: processing_instruction (str: 'answer_query' (default), 'summarize_results', 'extract_entities', 'custom_instruction:<your_request>'). Returns natural language text.",
            "no_op_tool": "Does nothing, useful for default plans or when no other tool is suitable."
            # Example of another tool the LLM might consider for other tasks:
            # "calculate_math": "Calculates simple math expressions. Args: expression (str). Returns number."
        }

    @patch('ai_assistant.planning.planning.invoke_ollama_model')
    def test_plan_search_answer_query(self, mock_invoke_llm):
        """Test planning a search with default 'answer_query' processing."""
        goal = "What is the current weather in London?"
        
        llm_response_json = json.dumps([
            {"tool_name": "search_duckduckgo", "args": ["current weather in London"], "kwargs": {}},
            {"tool_name": "process_search_results", "args": ["current weather in London", "[[step_1_output]]"], "kwargs": {}}
        ])
        mock_invoke_llm.return_value = llm_response_json

        plan = self.planner.create_plan_with_llm(goal, self.available_tools)

        self.assertIsInstance(plan, list)
        self.assertEqual(len(plan), 2)
        
        self.assertEqual(plan[0]['tool_name'], "search_duckduckgo")
        self.assertEqual(plan[0]['args'], ("current weather in London",))
        self.assertEqual(plan[0]['kwargs'], {})

        self.assertEqual(plan[1]['tool_name'], "process_search_results")
        self.assertEqual(plan[1]['args'], ("current weather in London", "[[step_1_output]]"))
        self.assertEqual(plan[1]['kwargs'], {}) # Default processing_instruction is answer_query

    @patch('ai_assistant.planning.planning.invoke_ollama_model')
    def test_plan_search_summarize_results(self, mock_invoke_llm):
        """Test planning a search with 'summarize_results' processing."""
        goal = "Summarize the latest advancements in quantum computing."
        
        llm_response_json = json.dumps([
            {"tool_name": "search_duckduckgo", "args": ["latest advancements in quantum computing"], "kwargs": {}},
            {"tool_name": "process_search_results", "args": ["latest advancements in quantum computing", "[[step_1_output]]"], "kwargs": {"processing_instruction": "summarize_results"}}
        ])
        mock_invoke_llm.return_value = llm_response_json

        plan = self.planner.create_plan_with_llm(goal, self.available_tools)

        self.assertIsInstance(plan, list)
        self.assertEqual(len(plan), 2)
        self.assertEqual(plan[1]['tool_name'], "process_search_results")
        self.assertEqual(plan[1]['args'], ("latest advancements in quantum computing", "[[step_1_output]]"))
        self.assertEqual(plan[1]['kwargs'], {"processing_instruction": "summarize_results"})

    @patch('ai_assistant.planning.planning.invoke_ollama_model')
    def test_plan_search_extract_entities(self, mock_invoke_llm):
        """Test planning a search with 'extract_entities' processing."""
        goal = "Extract key people mentioned in articles about the G7 summit."
        
        llm_response_json = json.dumps([
            {"tool_name": "search_duckduckgo", "args": ["key people in articles about G7 summit"], "kwargs": {}},
            {"tool_name": "process_search_results", "args": ["key people in articles about G7 summit", "[[step_1_output]]"], "kwargs": {"processing_instruction": "extract_entities"}}
        ])
        mock_invoke_llm.return_value = llm_response_json

        plan = self.planner.create_plan_with_llm(goal, self.available_tools)

        self.assertIsInstance(plan, list)
        self.assertEqual(len(plan), 2)
        self.assertEqual(plan[1]['tool_name'], "process_search_results")
        self.assertEqual(plan[1]['args'], ("key people in articles about G7 summit", "[[step_1_output]]"))
        self.assertEqual(plan[1]['kwargs'], {"processing_instruction": "extract_entities"})

    @patch('ai_assistant.planning.planning.invoke_ollama_model')
    def test_plan_search_custom_instruction(self, mock_invoke_llm):
        """Test planning a search with a custom processing instruction."""
        goal = "Find out the main arguments against nuclear power from web results."
        
        llm_response_json = json.dumps([
            {"tool_name": "search_duckduckgo", "args": ["main arguments against nuclear power"], "kwargs": {}},
            {"tool_name": "process_search_results", "args": ["main arguments against nuclear power", "[[step_1_output]]"], "kwargs": {"processing_instruction": "custom_instruction:Extract the main arguments against nuclear power"}}
        ])
        mock_invoke_llm.return_value = llm_response_json

        plan = self.planner.create_plan_with_llm(goal, self.available_tools)

        self.assertIsInstance(plan, list)
        self.assertEqual(len(plan), 2)
        self.assertEqual(plan[1]['tool_name'], "process_search_results")
        self.assertEqual(plan[1]['args'], ("main arguments against nuclear power", "[[step_1_output]]"))
        self.assertEqual(plan[1]['kwargs'], {"processing_instruction": "custom_instruction:Extract the main arguments against nuclear power"})

    @patch('ai_assistant.planning.planning.invoke_ollama_model')
    def test_plan_no_search_for_simple_math(self, mock_invoke_llm):
        """Test that simple math queries do not trigger a web search."""
        goal = "What is 5 plus 5?"
        
        # LLM might decide no tool is appropriate or use a general no_op/placeholder
        llm_response_json = json.dumps([
            {"tool_name": "no_op_tool", "args": [], "kwargs": {"note":"LLM decided no specific tool needed or cannot answer."}}
        ])
        mock_invoke_llm.return_value = llm_response_json

        plan = self.planner.create_plan_with_llm(goal, self.available_tools)

        self.assertIsInstance(plan, list)
        if plan: # Plan might be empty if LLM returns [] and no_op_tool is not forced
            for step in plan:
                self.assertNotEqual(step['tool_name'], "search_duckduckgo", "Search tool should not be used for simple math.")
            # Check if the no_op_tool was used as per mock
            if len(plan) == 1:
                 self.assertEqual(plan[0]['tool_name'], "no_op_tool")


    @patch('ai_assistant.planning.planning.invoke_ollama_model')
    def test_plan_no_search_for_creative_task(self, mock_invoke_llm):
        """Test that creative tasks do not trigger a web search."""
        goal = "Write a short story about a dragon."

        llm_response_json = json.dumps([
            {"tool_name": "no_op_tool", "args": [], "kwargs": {"note":"LLM decided no specific tool needed or cannot answer with available tools."}}
        ])
        mock_invoke_llm.return_value = llm_response_json

        plan = self.planner.create_plan_with_llm(goal, self.available_tools)

        self.assertIsInstance(plan, list)
        if plan:
            for step in plan:
                self.assertNotEqual(step['tool_name'], "search_duckduckgo", "Search tool should not be used for creative tasks.")
            if len(plan) == 1:
                 self.assertEqual(plan[0]['tool_name'], "no_op_tool")

if __name__ == '__main__':
    unittest.main()

# ### END FILE: tests/test_planning.py ###

# ### START FILE: tests/test_reflection.py ###
import unittest
import datetime
import json
from ai_assistant.core.reflection import ReflectionLogEntry, ReflectionLog

class TestReflectionLogEntry(unittest.TestCase):

    def test_initialization_with_self_modification_fields(self):
        now = datetime.datetime.now(datetime.timezone.utc)
        entry = ReflectionLogEntry(
            goal_description="Test goal",
            plan=[{"tool_name": "dummy_tool"}],
            execution_results=["dummy_result"],
            status="SUCCESS",
            timestamp=now,
            is_self_modification_attempt=True,
            source_suggestion_id="SUG001",
            modification_type="MODIFY_TOOL_CODE",
            modification_details={"module": "test.py", "function_name": "do_stuff"},
            post_modification_test_passed=True,
            post_modification_test_details={"passed": True, "notes": "All good", "stdout": "OK"},
            commit_info={"message": "Committed successfully", "status": True}
        )
        self.assertTrue(entry.is_self_modification_attempt)
        self.assertEqual(entry.source_suggestion_id, "SUG001")
        self.assertEqual(entry.modification_type, "MODIFY_TOOL_CODE")
        self.assertEqual(entry.modification_details, {"module": "test.py", "function_name": "do_stuff"})
        self.assertTrue(entry.post_modification_test_passed)
        self.assertEqual(entry.post_modification_test_details, {"passed": True, "notes": "All good", "stdout": "OK"})
        self.assertEqual(entry.commit_info, {"message": "Committed successfully", "status": True})
        self.assertEqual(entry.timestamp, now)

    def test_serialization_deserialization_self_modification(self):
        now = datetime.datetime.now(datetime.timezone.utc)
        entry_orig = ReflectionLogEntry(
            goal_description="Test serialization",
            plan=[],
            execution_results=[],
            status="FAILURE",
            notes="Self-mod test",
            timestamp=now,
            is_self_modification_attempt=True,
            source_suggestion_id="SUG002",
            modification_type="UPDATE_TOOL_DESCRIPTION",
            modification_details={"tool_name": "tool_x", "new_description": "Better desc"},
            post_modification_test_passed=False,
            post_modification_test_details={"passed": False, "notes": "Test failed", "stderr": "Error!"},
            commit_info={"message": "Commit attempt failed", "status": False, "error": "Git error"}
        )

        serialized_dict = entry_orig.to_serializable_dict()

        # Assert new fields are in the dict
        self.assertTrue(serialized_dict["is_self_modification_attempt"])
        self.assertEqual(serialized_dict["source_suggestion_id"], "SUG002")
        self.assertEqual(serialized_dict["modification_type"], "UPDATE_TOOL_DESCRIPTION")
        self.assertEqual(serialized_dict["modification_details"], {"tool_name": "tool_x", "new_description": "Better desc"})
        self.assertFalse(serialized_dict["post_modification_test_passed"])
        self.assertEqual(serialized_dict["post_modification_test_details"], {"passed": False, "notes": "Test failed", "stderr": "Error!"})
        self.assertEqual(serialized_dict["commit_info"], {"message": "Commit attempt failed", "status": False, "error": "Git error"})
        self.assertEqual(serialized_dict["timestamp"], now.isoformat())


        entry_new = ReflectionLogEntry.from_serializable_dict(serialized_dict)

        # Assert new entry has all fields correctly loaded
        self.assertTrue(entry_new.is_self_modification_attempt)
        self.assertEqual(entry_new.source_suggestion_id, "SUG002")
        self.assertEqual(entry_new.modification_type, "UPDATE_TOOL_DESCRIPTION")
        self.assertEqual(entry_new.modification_details, {"tool_name": "tool_x", "new_description": "Better desc"})
        self.assertFalse(entry_new.post_modification_test_passed)
        self.assertEqual(entry_new.post_modification_test_details, {"passed": False, "notes": "Test failed", "stderr": "Error!"})
        self.assertEqual(entry_new.commit_info, {"message": "Commit attempt failed", "status": False, "error": "Git error"})
        self.assertEqual(entry_new.timestamp, now) # Timestamps are compared directly
        self.assertEqual(entry_new.notes, "Self-mod test")

    def test_deserialization_backward_compatibility(self):
        now_iso = datetime.datetime.now(datetime.timezone.utc).isoformat()
        old_entry_data = {
            "goal_description": "Old goal",
            "plan": [{"tool_name": "old_tool"}],
            "execution_results": ["old_result"],
            "status": "SUCCESS",
            "notes": "Old notes",
            "timestamp": now_iso,
            "error_type": None,
            "error_message": None,
            "traceback_snippet": None
            # New self-modification fields are absent
        }

        entry = ReflectionLogEntry.from_serializable_dict(old_entry_data)

        self.assertFalse(entry.is_self_modification_attempt)
        self.assertIsNone(entry.source_suggestion_id)
        self.assertIsNone(entry.modification_type)
        self.assertIsNone(entry.modification_details)
        self.assertIsNone(entry.post_modification_test_passed)
        self.assertIsNone(entry.post_modification_test_details)
        self.assertIsNone(entry.commit_info)
        self.assertEqual(entry.goal_description, "Old goal")

    def test_to_formatted_string_self_modification(self):
        entry_mod = ReflectionLogEntry(
            goal_description="Test formatting self-mod",
            plan=[], execution_results=[], status="SUCCESS",
            is_self_modification_attempt=True,
            source_suggestion_id="SUG003",
            modification_type="MODIFY_TOOL_CODE",
            modification_details={"module": "a.b.c", "function": "xyz"},
            post_modification_test_passed=True,
            post_modification_test_details={"passed": True, "notes": "Tests look great!"},
            commit_info={"message": "Code committed", "status": True}
        )
        formatted_str_mod = entry_mod.to_formatted_string()
        self.assertIn("--- Self-Modification Attempt Details ---", formatted_str_mod)
        self.assertIn("Source Suggestion ID: SUG003", formatted_str_mod)
        self.assertIn("Modification Type: MODIFY_TOOL_CODE", formatted_str_mod)
        self.assertIn(json.dumps(entry_mod.modification_details, indent=2), formatted_str_mod)
        self.assertIn("Test Outcome: True", formatted_str_mod) # Note: was PASSED, now True/False
        self.assertIn("Test Details: Tests look great!", formatted_str_mod)
        self.assertIn("Commit Info: Code committed", formatted_str_mod)


        entry_no_mod = ReflectionLogEntry(
            goal_description="Test formatting no self-mod",
            plan=[], execution_results=[], status="SUCCESS",
            is_self_modification_attempt=False
        )
        formatted_str_no_mod = entry_no_mod.to_formatted_string()
        self.assertNotIn("--- Self-Modification Attempt Details ---", formatted_str_no_mod)


class TestReflectionLog(unittest.TestCase):

    def setUp(self):
        # For these tests, we'll use an in-memory ReflectionLog
        # by not providing a filepath or mocking persistence functions.
        self.reflection_log = ReflectionLog(filepath=":memory:") # Use a special value or mock load/save

    def test_log_execution_with_self_modification_params(self):
        goal = "Test self-mod logging in ReflectionLog"
        plan_data = [{"tool_name": "self_mod_tool"}]
        exec_results = [{"outcome": "details from apply_code_modification"}]
        mod_details = {"module": "core.py", "change": "refactor"}
        test_details = {"passed": False, "notes": "Unit test failed post-mod"}
        commit_details = {"message": "Attempted refactor, tests failed", "status": False}

        self.reflection_log.log_execution(
            goal_description=goal,
            plan=plan_data,
            execution_results=exec_results,
            overall_success=False, # The self-mod attempt itself might "succeed" but the outcome (test) failed
            notes="Logging a self-modification attempt.",
            is_self_modification_attempt=True,
            source_suggestion_id="SUG004",
            modification_type="MODIFY_TOOL_CODE",
            modification_details=mod_details,
            post_modification_test_passed=False,
            post_modification_test_details=test_details,
            commit_info=commit_details
        )

        self.assertEqual(len(self.reflection_log.log_entries), 1)
        last_entry = self.reflection_log.log_entries[-1]

        self.assertIsInstance(last_entry, ReflectionLogEntry)
        self.assertEqual(last_entry.goal_description, goal)
        self.assertEqual(last_entry.notes, "Logging a self-modification attempt.")
        self.assertTrue(last_entry.is_self_modification_attempt)
        self.assertEqual(last_entry.source_suggestion_id, "SUG004")
        self.assertEqual(last_entry.modification_type, "MODIFY_TOOL_CODE")
        self.assertEqual(last_entry.modification_details, mod_details)
        self.assertFalse(last_entry.post_modification_test_passed)
        self.assertEqual(last_entry.post_modification_test_details, test_details)
        self.assertEqual(last_entry.commit_info, commit_details)
        # Check a few other standard fields
        self.assertEqual(last_entry.plan, plan_data)
        self.assertEqual(last_entry.execution_results, exec_results)
        # Status should be determined by log_execution logic based on overall_success and other factors.
        # If overall_success is False, status should reflect failure.
        self.assertIn(last_entry.status, ["FAILURE", "PARTIAL_SUCCESS"])


if __name__ == '__main__':
    unittest.main()

# ### END FILE: tests/test_reflection.py ###

